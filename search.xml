<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="posts/0/"/>
      <url>posts/0/</url>
      
        <content type="html"><![CDATA[<p>尚硅谷大数据技术之企业真题-深圳校区<br />(作者：尚硅谷大数据研发部)</p><p>版本：V1.0.0</p><h1 id="1-腾讯"><a class="markdownIt-Anchor" href="#1-腾讯"></a> 1.     腾讯</h1><h2 id="学长-1"><a class="markdownIt-Anchor" href="#学长-1"></a> 学长 1</h2><p>1.java  的多态，继承<br />2.hashmap，数据结构，堆，栈特点，递归如何变非递归<br />3.hbase 的 rowkey 设计原则<br />4.hbase 的二级索引<br />5.flink 如何保证精准一次<br />6.flink 乱序处理机制<br />7.flink 背压机制<br />8.flink 如何处理数据倾斜<br />9.presto 相关原理</p><h2 id="学长-2"><a class="markdownIt-Anchor" href="#学长-2"></a> 学长 2</h2><p>编程题 1：<br />给一个日期 20200202，观察这个日期，同时满足两个特征：左右对称；除数字 0 外，只有一个非零数字。</p><p>编程题 2：编码实现以下两项：<br />1.  输入任意日期，判断是否满足以上条件，测试用例：20200202，21211212<br />2.  输出自 19700101 至今所有符合条件的日期<br />        组件题：<br />1.       hive 数据倾斜是什么？怎么解决？<br />2.       count distinct 和 groupby 区别？为什么 count distinct 会把数据发到一个 reducer？<br />3.       mapjoin 怎么实现？多大算小表？小表在内存是什么数据结构？<br />4.       MRshuffle 及优化？<br />5.       redis 和 hbase 各自的优缺点？<br />6.       hbase 为什么快？<br />7.       hbase 热点产生的原因？<br />8.       hbase 性能降低的时候 sparkstreaming 怎么解决反压的问题？<br />9.       flink 相比 sparkstreaming 优点？<br />10.    flink 挂了之后 cp 的整个加载流程？</p><h1 id="2-安信仪表自动化有限公司"><a class="markdownIt-Anchor" href="#2-安信仪表自动化有限公司"></a> 2.  安信仪表自动化有限公司</h1><p>笔试题目：<br />1.       hive 里面 shuffle 优化<br />2.       sqoop 参数，全量导出、增量导出需要哪些参数<br />3.       hive 数据倾斜原因<br />4.       hive 数据倾斜怎么解决<br />5.        悲观锁、乐观锁区别？实现？<br />6.       spark streaming 消息语义，kafka 两种连接模式<br />7.        一道简单的 java 题 a<ins>与</ins>a<br />8.        设计模式有哪些，怎么应用？<br />面试：<br />1.        数仓结构<br />2.       hive 调优<br />3.       spark streaming  精准一次消费<br />4.       flink 水印机制<br />5.       flume 拦截器<br />6.       kafka 精准一次消费<br />7.       hive -n 含义（hive \N  我可能听错了）<br />8.       sqoop 导数据会不会</p><h1 id="3-风变编程"><a class="markdownIt-Anchor" href="#3-风变编程"></a> 3.     风变编程</h1><p>1.       sqoop 里面的参数有哪些？<br />2.        维度建模：问我一些里面的表结构，什么组件？<br />3.        用 load 命令到会不会太慢，有没有什么优化？<br />4.       sql 优化，in 和 exist 有什么区别？<br />5.       10 亿条数据用 1M 内存怎么处理，如果用 redis 去重怎么去优化（布隆过滤器）？<br />6.        宽表里面的索引稀疏怎么解决？<br />7.        实时数仓的目的是什么，给你们解决了什么问题？</p><p>总结：每个人估计问的问题都不一样，主要是问你介绍过的，然后问得有点深，有很多概念都没听说过。所以同学们介绍自己熟悉一点的东西，如果 flink 什么不熟，千万别强行装逼。对面有点强。架构哪些如果可以，适当调整一下。</p><h1 id="4-领星网络"><a class="markdownIt-Anchor" href="#4-领星网络"></a> 4.     领星网络</h1><p>1.       Flink  的背压处理方式，什么原因引起的？<br />2.       Flink 出现的问题，怎么解决的？重启策略？<br />3.       Flink 数据倾斜的处理方式，怎么处理的，哪些算子引起的数据倾斜？<br />4.       Flink 集群的数量，资源调度是怎么做的？<br />5.       Flink 的提交参数，怎么确定下来的？<br />6.       redis 和 Mysql 的数据一致性保证是怎么做的<br />7.       hashmap 数据架构<br />8.        通信协议，用的什么–涉及到后端的东西</p><h1 id="5-平安寿险"><a class="markdownIt-Anchor" href="#5-平安寿险"></a> 5.     平安寿险</h1><h2 id="学长-1-2"><a class="markdownIt-Anchor" href="#学长-1-2"></a> 学长 1</h2><p>1.        介绍一下离线数据：每层的设计原则、都做了什么、为什么这么做<br />2.        说一下你都在离线数仓中都做了什么<br />3.        说一下你负责的 ads 层的指标的具体实现过程、怎么来的<br />4.        有没有遇到数据倾斜的问题<br />5.        介绍一下实时数仓是怎么做的<br />6.    介绍一下 flink 都做过哪些指标<br />7.   flink 中有遇到过什么 bug<br />8.    介绍一下 watermark<br />9.   flink 是怎么保证状态一致性的<br />10.  说一下 canal 的原理<br />11.  介绍一下 impala 的架构原理（没想到会问这个问题）<br />12.  说一下 es 的倒排索引<br />13.  说一下 hbase 的 rowkey 设计原则</p><h2 id="学长-2-2"><a class="markdownIt-Anchor" href="#学长-2-2"></a> 学长 2</h2><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954104562-9c805b8d-af09-4a1b-b448-26ede519b7f5.png#height=395&amp;width=324" alt="" /></p><h2 id="学长-3"><a class="markdownIt-Anchor" href="#学长-3"></a> 学长 3</h2><p>1.  一开始问 java 方面，jvm -&gt; juc -&gt; hashmap 的数据结构  -&gt;  做过的 java 项目  -&gt;  你觉得最难的 java 项目的实现。<br />   2. flink 做过哪些指标，具体怎么一个实现过程，从读取流到数据计算到存储，用了哪些算子<br />   3. flink 的背压机制，如何调优<br />   4. flink 的 cep 规则。  两个不同的条件能不能匹配到一起<br />   5. flink sql 的原理<br />   6. flink 有没有遇到 oom。怎么判断是哪个算子导致的<br />   7. flink 一个流表和一个很大的流表如何 join<br />   8. spark-streaming  做过哪些需求。怎么保证状态一致性。<br />   9.  使用过 Hbase，写过读写的代码吗，代码是怎么样的？</p><h2 id="学长-4"><a class="markdownIt-Anchor" href="#学长-4"></a> 学长 4</h2><p>1.先介绍一下离线数仓搭建的情况,  和你的主要职责 2.然后问了以下数仓搭建过程中遇到的问题 3.问了维度表如何确定,事实表如何确定,  然后问了维度域/主题域,这个确实不清楚<br />4.hive sql 的调优 5.指标是业务直接给到你的,  还是业务给组长然后给你的,  有哪些指标</p><h2 id="学长-5"><a class="markdownIt-Anchor" href="#学长-5"></a> 学长 5</h2><p>1.flink 和 sparkstreaming 区别<br />2.flink 做了哪些指标<br />3.flink 统计一个月的 uv 怎么算？自然月呢  ？<br />4.watermark<br />5.cep 用过吗？怎么实现？ 6.离线数仓怎么做？<br />7.flume 工作原理，flume 多长时间输出？</p><h2 id="学长-6"><a class="markdownIt-Anchor" href="#学长-6"></a> 学长 6</h2><p>1  自我介绍<br />2  离线数仓各个层都做了什么？<br />3 Hive 的调优？</p><h2 id="学长-7"><a class="markdownIt-Anchor" href="#学长-7"></a> 学长 7</h2><p>1.       flink 做过哪些指标，具体怎么一个实现过程，从读取流到数据计算到存储，用了哪些算子<br />2.       flink 的背压机制，如何调优<br />3.       flink 的 cep 规则。  两个不同的条件能不能匹配到一起<br />4.       flink sql 的原理<br />5.       flink 有没有遇到 oom。怎么判断是哪个算子导致的<br />6.       flink 的状态后端<br />7.       flink 一个流表和一个很大的流表如何 join<br />8.       barriar 原理<br />9.       spark-streaming  做过哪些需求。怎么保证状态一致性。<br />10.     使用过 Hbase，写过读写的代码吗，代码是怎么样的？</p><h2 id="学长-8"><a class="markdownIt-Anchor" href="#学长-8"></a> 学长 8</h2><p>1.       hive 的优化<br />2.       mapjoin 的条件<br />3.       spark 的代码优化<br />4.        伴生类伴生对象<br />5.       Nil，NULL,NONE,NOTHING 的区别<br />6.        有没有写过 mr 程序，什么场景<br />7.       mr 程序输出数据的格式是什么<br />8.       flink 的 checkpoint 机制<br />9.       foreachRDD，foreach，foreachpartition 有什么区别<br />10.    spark 中产生 shuffle 的算子，回答了之后就会问区别<br />11.    repartition 和 coalesce 的区别</p><h2 id="学长-9"><a class="markdownIt-Anchor" href="#学长-9"></a> 学长 9</h2><p>1.        讲下你最熟悉的项目<br />2.        我说我公司是大概 5、6 个维度，面试官：“你们的维度是怎么确定的？”<br />3.        你们的标签（我问了就是指标）是怎么确定的呢？<br />4.        某天用户量 50 万，第二天是 20 万，什么原因导致这样？<br />5.        你们怎么保证你们指标的结果是可靠的？</p><h1 id="6-荣耀"><a class="markdownIt-Anchor" href="#6-荣耀"></a> 6.  荣耀</h1><h2 id="学长-1-3"><a class="markdownIt-Anchor" href="#学长-1-3"></a> 学长 1</h2><ol><li>ods 层变化表怎么处理：dwd 层进行<br />  2. Hive 有没有删除过表？比如维度建错了<br />  3.  降维怎么降？原因？维度表发生变化怎么办？缓慢变化维？<br />      缓慢变化维：就是拉链表<br />  4.  事实维度表？<br />      两张事实表和关联同一张事实表，我理解的其实就是构成星座模型<br />  5.  采集的调优</li></ol><h2 id="学长-2-3"><a class="markdownIt-Anchor" href="#学长-2-3"></a> 学长 2</h2><p>1、hive 调优本质（千万别调内存，在有限的内存里完成数据倾斜的处理才是你的本事）<br />2、事实表的制定原理，和如何划分，再举例说明（举例订单业务的事实表，划分为单头和单身，对应我们的订单表和订单详情表，再细化这样划分的原因，主要是粒度和度量的不同）<br />3、前端业务数据硬删除对大数据的影响，如何处理</p><h1 id="7-极光"><a class="markdownIt-Anchor" href="#7-极光"></a> 7.  极光</h1><p>1.  内部表与外部表的区别？<br />   2.  内部表删掉有什么恢复的手段？<br />      1.  删除表底层实际上将存储表的数据文件移动到.Trash/Current，只是删除了元数据信息，如果删除了不及时找回，./Trash/Current 目录下文件会定时清理，那就真的丢失了；<br />      2. Hadoop 回收站 trash 默认是关闭的，修改 conf/core-site.xml，添加 trash.interval，时间单位是分钟，设置为一天；<br />      3.   执行：<code>sh hadoop fs -lsr Trash/Current文件目录</code><br />   3. order by  和  cluster by 、sort by  的区别？<br />      1. order by 全局排序，sort by  是分区排序<br />      2. distribute by  是分区，结合 sort by  使用<br />      3. cluster by：如果 distribute by  和 sort by  字段相同，那么就可以用 cluster by  代替<br />   4. hive 的调优？<br />      1. mapjoin 默认开启<br />      2.  列式存储<br />      3.  分区表和分桶表<br />      4.  合理设置 map 数和 reduce 数<br />      5.  小文件问题<br />         1. combineHiveInputFormat<br />         2. merge，merge-only<br />         3. reduce 端开启 merge<br />         4. JVM 重用：set mapreduce.job.jvm.numtasks=10<br />         5.  采用压缩<br />   5.  去重，除了 group by  和 distinct  还有什么？<br />      1. rank()开窗，排序，过滤 rk = 1 的<br />   6.  维护 user id  全量和 app  版本的（保证最新），另外一个是增量的数据？<br />      1.  拉链表<br />   7. Hive 的存储格式，数据治理，建一张内表的时候字段名错了，有历史数据，怎么解决？如果存储方式是 textFile，怎么解决？修改字段名是否可以？<br />      1.  执行 ALTER TABLE  名字  change  原字段名   当前字段名   类型，查询的修改的字段，全部为 null；<br />      2.  修复办法：<br />         1.  创建临时表 temp，和修改之前的表结构相同，然后用 hdfs 命令将当前表下的目录拷贝到临时表下<br />         2.  修复分区：msck repair table temp<br />         3.  清空当前的表，再用 insert overwrite 从临时表 temp 中导入当前表<br />      3.  避免办法：ALTER TABLE  名字  change  原字段名   当前字段名   类型  cascade；<br />   8.  修改字段名，关联的时候会出现什么情况？<br />   9.  落地格式是什么？hdfs -》ods 层   怎么导入的？<br />      1. textFile；load data inpath  路径  （overwrite）into table  表名<br />   10. RDD、DataFrame、DataSet 的区别？RDD 存储的是什么？<br />       1. RDD<br />          1.  优点：编译时安全，编译时就能检查出错误，通过类名点的方式操作数据<br />          2.  缺点：序列化和反序列化性能开销，GC 性能开销，频繁的创建和销毁对象，势必会增加 GC<br />       2.  联系：<br />          1.  三者都是 spark 的分布式弹性数据集；<br />          2.  三者都是惰性的，懒加载<br />          3.  都有 partition 的概念<br />          4.  三者都会自动缓存运算，即使数据量大，也不担心 OOM<br />       3.  区别：<br />          1. DataFrame 引入了 schema 和 off-heap，schema：存储数据的结构，每一行的数据结构都相同，spark 通过 schema 就能读懂数据，因此，通信和 IO 时只需要序列化和反序列化数据，结构就省略了<br />          2. DataSet：结合了 DataFrame 和 RDD 的优点，带来新的概念 Encoder，序列化数据时，Encoder 产生字节码与 off-heap 进行交互，能够达到按需访问数据的效果，而不是反序列化整个对象。<br />   11. Application 有几个 job 怎么确定？<br />       1. yarn web  界面  8088 查看有多少 job 在跑<br />   12. redis  在哪个算子里？map  和 mapPartitons  的区别？<br />       1. mapPartitions<br />   13.  往 redis 里写之前，有做本地去重吗？<br />       1.  没有，不确定。。。。。<br />   14. checkpoint 的过程？<br />   15. operator 上游有两条流，barrier 的情况？<br />   16. flink 的背压机制？sink 处理不过来了，operator 怎么感知到的？<br />       1.<br />   17. HBase 的 Rowkey 的设计原则，散列的方式，多列簇和单列簇的区别？<br />       1.  每一个 family 都会分配一个 memstore，多列簇会消耗更多的内存<br />       2. flush 和 compaction 是以 region 为单位，多列簇：有一个 family 达到 flush 条件，该 region 所有的 family 所属的 memstore 都会 flush 一次，导致少量数据的 memstore 也会 flush，会生成很多小文件，增加 compaction 的几率，从而降低系统的整体吞吐量<br />       3. hfile 是以 family 为单位的，多个 family，数据被分散到了更多的 hfile 中，减小了 split 发生的机率。<br />          1.  更少的 split 会导致该 region 的体积比较大，由于 balance 是以 region 的数目而不是大小为单位进行，因此可能导致 balance 失效；<br />          2.  更少的 split 会让系统提供更加稳定的在线服务，有利于实时的读写<br />   18.  每个单元格具体存储哪些东西？HBase 的删除，怎么做到删除？regionserver 挂掉了集群会有哪些动作呢？<br />       1. RowKey，ColumnFamily，Column Qualifier，TimeStamp，Type，Value<br />       2.  删除：delete 操作并不会马上删除数据，只会将对应的数据打上删除标记（tombstone），只有在合并数据时，数据才会被删除<br />          1.  删除 Cell：delete ‘表名’,‘rowkey’,‘列簇:列限定符’；deleteall ‘表名’,‘rowkey’,‘列簇:列限定符’<br />          2.  删除整行数据：deleteall ‘表名’,‘rowkey’<br />       3. regionserver 挂掉，Master 会将宕机的 regionserver 上的 region 分配到其他的 regionserver 上<br />   19. redis 的数据结构？<br />   20.  布隆过滤器，可以精准确定是否存在？数据量特别大，怎么能更好的判断它存在？<br />       1.  增大 bitmap 的大小<br />       2.  多设置几个 hashfunction<br />   21.  数据量特别大，在 set 里去重怎么优化？<br />   22. java 字符串拼接，线程池类的哪几种方式、多线程，hashmap？<br />   23. hive 中多少个 map 和多少个 reduce 怎么确定的？<br />       1. map 数= split 数，<br />       2.  reduce 数<br />          1.  自己设定：set mapred.reduce.tasks;<br />          2.  默认自定估算：数据量  /  每个 reduce 处理的数量（1G）<br />          3.  调整估算结果：per.reducer 的大小默认 1G，可以调整<br />          4.  每个任务最多有 reducers.max 参数是 999</p><h1 id="8-oppo"><a class="markdownIt-Anchor" href="#8-oppo"></a> 8. OPPO</h1><p>1.  为什么考虑离职？<br />      1.  寻求大点平台，数据量大，<br />      2.  上海气候不是很适应<br />   2.  公司的主要业务，怎么和项目关联起来的？<br />      1.  产品经理那边负责业务，我们只负责对产品经理提的需求，完成业务指标<br />      2.  应该就是一个广告链接，然后跳转到各大平台的商品详情页<br />   3.  有什么业务价值？<br />      1.  主要是汇总各大电商平台的优惠券的商品链接，做推送；<br />   4.  离线平台的团队架构？<br />      1. 10 个人，离线和实时 5 个人，其余的负责前端和后台还有运维<br />   5.  采集平台的架构？<br />   6.  技术框架选型是基于什么样的考虑？<br />      1.  基于一个版本的兼容性，技术框架搭建，可能最新的版本可能有些功能不兼容，导致出错<br />      2.  各版本型号：<br />         1. hadoop：2.x -&gt; 3.x<br />         2. spark：1.x -&gt; 2.x<br />   7.  上下游部门怎么对接？<br />      1.  有产品经理提出方案，我们部门和产品经理共同商议业务指标的确定，然后由我们具体实现<br />   8.  数仓的设计怎么设计的，有没有设计？<br />   9.  数仓的建模的思路？<br />   10.  维度的改变？怎么改变？（缓慢变化维、拉链表）<br />       1.  重写维度值：新维度覆盖旧维度值，拉链表，适用于不需要保留历史维度变化的情况<br />       2.  插入新的行：拉链表（老表 lfet join  新表，查到改变的表然后 union all  新表），新维度不覆盖旧维度值，适用于保存维度变化的情况<br />       3.  插入新的列：既能够用变化前的属性值又能够用变化后的属性值，比如字段加 pre-region  和 current-region<br />   11.  最有挑战的问题，怎么解决的？挑战是在什么地方？<br />       1.  实时：dws 层构建所谓的宽表的时候，1.字段选择，容易漏掉；2.双流 join，因为网络延迟、网络不好导致乱序、滞后的现象没有考虑过<br />       2.  数仓：<br />   12.  为什么考虑要用 flink？做这个技术的转换，有什么目的？<br />       1.  真正的流处理；<br />       2.  时间机制：flink 有事件时间语义；有 watermark 机制，处理乱序问题<br />       3. CEP：可以很简单的实现复杂事件：比如恶意登录问题，超时支付监控<br />   13.  项目有哪些不够完善的，我讲了 flink 的项目<br />   14. java 开发有什么了解的？<br />   15.  为什么非科班出身，他们要招你<br />   16.  数据倾斜？</p><h1 id="9-跨越速运"><a class="markdownIt-Anchor" href="#9-跨越速运"></a> 9.  跨越速运</h1><h2 id="学长-1-4"><a class="markdownIt-Anchor" href="#学长-1-4"></a> 学长 1</h2><p>1.Flink 的 JobManager 和 TaskManager 能不能失败重启？机制是什么？ 2.拉链表如果有一天没有拉成功，导致漏拉了一天(比如 11 号漏了 12 号的，直接拉 13 号)，怎么处理？怎么避免？<br />3.Flink 有没有出现过窗口无法触发计算的情况？<br />4.Flink 的 Checkpoint，如果部分算子已经完成本次 ck，宕机以后是从哪开始？保存好的算子开始还是从 source 端重新开始？ 5.为什么维度表要存在 HBase 里不存在 MySQL 里，HBase 的 rowkey 设计有什么实际应用场景？<br />6.Spark 的下游分区如何知道从哪个上游分区读取数据？<br />7.Hive 的 groupBy 和窗口的 partitionBy 有什么区别？从数据倾斜的角度讲<br />8.Flink 各个窗口的区别？从源码的角度讲<br />9.Flink 的前端页面可以看到 Job 运行过程中的那些信息？</p><h1 id="10-法本内部岗"><a class="markdownIt-Anchor" href="#10-法本内部岗"></a> 10.  法本内部岗</h1><ol><li>架构师面的我，主要问的是 spark 这一块的知识点，就是业务上的东西</li><li>ssc 的数据清洗？</li><li>数据清洗？算子清洗？</li><li>Spark 连接 hive  怎么个连接法？</li><li>数据来源不一致，比如分别从支付宝和微信上来的数据，但是数据合适不一致，怎么解决？</li><li>数据倾斜这一块详细的优化，比如我的 ssc 是动态的，怎么进行调整？</li></ol><h1 id="11-小鹅通"><a class="markdownIt-Anchor" href="#11-小鹅通"></a> 11.  小鹅通</h1><h2 id="学长-1-5"><a class="markdownIt-Anchor" href="#学长-1-5"></a> 学长 1</h2><p>1.        为什么使用双层 flume？<br />2.        数仓如何搭建的？<br />3.        从前往后梳理了一遍<br />4.        主要负责那块的业务？<br />5.       Flink 给了一个场景，7 亿客户的数据每个页面，实时统计 uv 增长。</p><h2 id="学长-2-4"><a class="markdownIt-Anchor" href="#学长-2-4"></a> 学长 2</h2><p>1.        一开始先自我介绍<br />2.        然后问数仓，数仓怎么建模，维度表有哪些，事实表有哪些，具体到表，字段<br />3.        然后实时问，7 亿用户，怎么算 uv，set oom，redis 不想用，想听 bitmap  布隆过滤器<br />4.        问 hive 有没有数据倾斜，hql 怎么解决数据倾斜，不能用 mr，只能用 hql</p><h1 id="12-山西证券"><a class="markdownIt-Anchor" href="#12-山西证券"></a> 12.  山西证券</h1><p>1、对离线数仓的全过程感兴趣，要熟练 sql 的函数，必问<br />2、问对机器学习的了解，盲猜他们本身机器学习的程度不深<br />3、问做实时数仓的意义，公司为什么做实时<br />4、问实时数仓为什么要做分层</p><h1 id="13-深圳新房科技"><a class="markdownIt-Anchor" href="#13-深圳新房科技"></a> 13.  深圳新房科技</h1><p>1.        讲下你对大数据的理解<br />2.        最熟练的技能告诉我，最主要的日常工作是做什么事情？编程有没有接触过？<br />3.        任务怎么提交到阿兹卡班，用什么语言开发<br />4.       context 和定时器是做什么的？<br />5.        说说最成功的一个项目<br />6.       ods 层用 textfile 和 lzo 有什么作用？还可以优化吗？</p><h1 id="14-赛维网络"><a class="markdownIt-Anchor" href="#14-赛维网络"></a> 14.  赛维网络</h1><p>1.        主要框架 HDFS HBASE SPARK KAFKA<br />2.        喜欢问相关优化，kafka 精准一次的保证<br />3.       spark 的优化<br />4.       hbase 相关，认为 hbase 瓶颈一般在查询，更新相对很快</p><h1 id="15-索信达"><a class="markdownIt-Anchor" href="#15-索信达"></a> 15.  索信达</h1><p>1、你认为 Flink 的核心是什么？<br />2、你们的数据流怎么走的？<br />3、Kafka 数据量<br />4、实时的指标做过哪些？<br />5、presto 和 impala 的用过没，区别？</p><h1 id="16-船奇"><a class="markdownIt-Anchor" href="#16-船奇"></a> 16.  船奇</h1><p>1.        分三个方面问：<br />1.java 基础 2.大数据 3.综合实例<br />2.       java 问你会什么<br />l  jvm<br />l  gc<br />l   数据结构<br />l   然后问多个字段怎么排序<br />3.        大数据问怎么保证一致性，flink 消费 kafka 写到 hbase，说前面 10 个写到 hbase 了，挂了，怎么保证数据不重不丢？<br />4.       3 台服务器，32 核，128g，100 个指标，50 个维度字段，一个字段 20 字节，问怎么实现，怎么优化？从时间复杂度和空间复杂度考虑</p><h1 id="17-搜电充电"><a class="markdownIt-Anchor" href="#17-搜电充电"></a> 17.  搜电充电</h1><p>1.       hdfs 高可用，分布式怎么配置，secondnamenode 怎么从 nn 拷贝数据的，想要能够画出来图<br />2.       java 和 scala 区别<br />3.       jvm 底层<br />4.        垃圾回收机制，新生代，老年代</p><h1 id="18-爱租机"><a class="markdownIt-Anchor" href="#18-爱租机"></a> 18.  爱租机</h1><p>1.       namenode 挂了，怎么不停止读写数据？<br />2.       flink 怎么感知 kafka 分区调整？</p><h1 id="19-歌力思"><a class="markdownIt-Anchor" href="#19-歌力思"></a> 19.  歌力思</h1><p>1.       hadoop 和 spark 区别？<br />2.       etl 和 elt 什么区别？</p><h1 id="20-民太安"><a class="markdownIt-Anchor" href="#20-民太安"></a> 20.  民太安</h1><h2 id="学长-1-6"><a class="markdownIt-Anchor" href="#学长-1-6"></a> 学长 1</h2><p>1.        讲一下之前遇到的项目<br />2.        自己工作的内容有哪些<br />3.        数层的结果是怎么展示的<br />4.        实时这一块有哪些做了哪些<br />5.       flink 与 spark 的区别<br />6.       spark 与 flink 窗口的区别<br />7.       flink 遇到了哪些问题<br />8.       flink watermark 的了解多少<br />9.       flink 窗口有哪些<br />10.     再 count 窗口的基础上，实现时间窗口，<br />11.    checkpoint 与 spark 的 checkpoint 有什么区别<br />12.    flink 再 yarn 里运行的过程，画一下，在讲一下<br />13.     怎么观察算子运行的状态，怎么看 flink  的数据倾斜<br />14.    flink 调优，怎么调<br />15.    flink 计算的结果存在哪儿了<br />16.    flink 往 hbase 里面存，有哪些需要注意的事情<br />17.     数据量有多大？<br />18.    hive 分区，分表，怎么用的<br />19.    hive 在实际工作中遇到过什么问题<br />20.    flink 常用到的算子，<br />21.    process 算子里面的 open 的运行机制是怎么样的<br />22.     你有没有什么想问的</p><h2 id="学长-2-5"><a class="markdownIt-Anchor" href="#学长-2-5"></a> 学长 2</h2><p>1.        数据量很大,2 千台车,1 台车 1000 万条数据,flink 开窗口,车辆出险预警信号,怎么实现从当前开始,将该窗口内的数据进行计算以及及时预警？<br />2.       Flink 的热点问题<br />3.       Flink 的 watermark 了解吗<br />4.       Flink 的分区<br />5.       Mapreduce 流程讲一讲  <br />6.       HBase 热点问题<br />7.       Flink 对接 kafka 分区时,有没有遇到什么问题<br />8.       MySql 客户端连接进行增删改查会不会</p><h1 id="21-顺丰"><a class="markdownIt-Anchor" href="#21-顺丰"></a> 21.  顺丰</h1><h2 id="学生-1"><a class="markdownIt-Anchor" href="#学生-1"></a> 学生 1</h2><ol><li>JVM 原理,GC 回收机制,hashMap 底层原理</li><li>hadoop 的组件,各自的作用,SecondNode 的作用</li><li>es 的倒排索引原理<br />4.  看过什么框架的源码,选取一个简单讲解一下</li><li>Flink 有哪些窗口,介绍一下区别?</li><li>flink 的提交方式?</li><li>Flink 的编程模型</li><li>Flink 的状态有了解吗?有哪些?</li><li>Kafka 读数据的原理(二分法的那个  )</li><li>scala 的尾递归知道吗?</li><li>FLink 数据倾斜优化？<br />思路:对 A 分区里面的   设备 id 进行 hash 分组   比如说取十个分区   将数据放在是个分区中区计算,计算后再把结果聚合)</li><li>kafka 实现精准一次性？</li><li>flink 是用什么监控的？</li><li>Linux 用了哪些命令？</li></ol><h1 id="22-微众银行"><a class="markdownIt-Anchor" href="#22-微众银行"></a> 22.  微众银行</h1><h2 id="学长-1-7"><a class="markdownIt-Anchor" href="#学长-1-7"></a> 学长 1</h2><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954105223-5ec50e32-d2e8-4b42-87e2-64b5f931c3d4.png#height=297&amp;width=415" alt="" /><br /><img src="https://cdn.nlark.com/yuque/0/2021/jpeg/12672933/1620954105600-d8148194-080a-4998-8805-ce51edaa7e24.jpeg#height=389&amp;width=313" alt="" /><br /><img src="https://cdn.nlark.com/yuque/0/2021/jpeg/12672933/1620954106017-64f184b8-922b-48e7-a14b-f445cc3c3d50.jpeg#height=257&amp;width=361" alt="" /></p><h1 id="23-瑞达飞行"><a class="markdownIt-Anchor" href="#23-瑞达飞行"></a> 23.  瑞达飞行</h1><p>1.  介绍一下自己，自己的大学经历和工作经历<br />2.  介绍一下自己的项目<br />3.  介绍一下自己的职业规划<br />4.  你门数仓做了哪些工作，你主要负责哪些，用到了哪些技 22220 术点 5. Flink 了解吗，介绍一下水位线和窗口 6. Es 的数据结构了解吗？ 7. Hbase 的数据结构了解吗？<br />8.  有了解过算法与数据结构吗？ 9. Flink 怎样保证精准一次？ 10. Kafka 怎样保证数据安全？<br />11.  总结：面试官应该是 Java 转的大数据，了解一点算法和数据结构 12.打包上线发布流程？ 13.问了框架选型，如果你要重新搭建一个框架该从哪些方面去考虑？ 14.你最擅长的一个项目你说一下。</p><h1 id="24-华士精成"><a class="markdownIt-Anchor" href="#24-华士精成"></a> 24.   华士精成</h1><p>1.  说下 MapReduce 底层的组成,有哪些类？<br />2.  说下大数据分布式架构的特点？<br />3.  你大数据框架是基于什么语言的?  4. Hive 里你写了哪些 java 代码? 5. Mysql 有几种引擎?<br />6.  一百篇文章,  你写个 MapReduce,  统计下每个单词的个数?<br />7.  你们大数据里面数据量很大的时候分表怎么分的?<br />8.  介绍下 kafka 的机制 9. 100G 数据,100 个分区,  两个消费者,  要正常消费这个数据你怎么优化(不能增加消费者数)</p><h1 id="25-平行线"><a class="markdownIt-Anchor" href="#25-平行线"></a> 25.   平行线</h1><p>产品经理提问:<br />1.  离职原因,  选择深圳原因,上家公司大数据组成,人员分工？<br />2.  业务的峰值时间段,  哪些指标是你做的,  你分析的指标帮助公司提高了哪些方面？<br />3.  简历上写的,  产品经理懂得,  都会叫你介绍下,  比如漏斗分析之类的<br />技术主管提问:<br />1.  介绍下你最近做过的项目,  你负责哪一块? 2. Flink 的优势, Flink 的组件, watermark 和 checkpoint 了不了解, savepoint 和它什么区别<br />3.  采集日志数据你的第一层 flume 装在哪?<br />4.  介绍离线数仓的搭建</p><h1 id="26-汉熵通信"><a class="markdownIt-Anchor" href="#26-汉熵通信"></a> 26.   汉熵通信</h1><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954106602-64d3ec89-fea5-4903-a5ed-af3316f28fa9.png#height=356&amp;width=332" alt="" /></p><h1 id="27-明源云"><a class="markdownIt-Anchor" href="#27-明源云"></a> 27.   明源云</h1><h2 id="学长-1-8"><a class="markdownIt-Anchor" href="#学长-1-8"></a> 学长 1</h2><p>1.        日志轮转有了解吗？<br />2.       taildirsource 短点续传原理，index 文件存了哪些东西？<br />3.       flume 到 hdfs 可以用什么代替？<br />4.       hive 抽样怎么确定数据是具有代表性的？<br />5.       MR  shuffle  流程？<br />6.       ES join 如何实现？<br />7.       kafka 在使用中有没有遇到什么问题，如何解决？<br />8.       flink 与 spark 区别？<br />9.       flink,spark 对于累计计算有哪些机制来做处理？</p><h2 id="学长-2-6"><a class="markdownIt-Anchor" href="#学长-2-6"></a> 学长 2</h2><ol><li>Sqoop 问题问了很多,导入到 hdfs 一致性问题,null 值的处理<br />       2.  假设 mysql 表 1 万条数据,导入到 hdfs 也是一亿条,但是有一条错误,对不上的,怎么快速找出这一条<br />       3.  两张表关联,筛选条件放在 wher 后面和 on 后面的区别,一定不一样吗,哪一个适合<br />       4.  为什么用 lzo 压缩(我答的可切片,不切片可以选 snappy)<br />       5. MySQL BTree 手画<br />       6. MySQL 索引的结构<br />       7. ES 的倒排索引<br />       8. ES 怎么实现 join 语句,<br />       9. Hive 里边 parquet,orc,text 区别,为什么不用 text,或者说用了 parquet 后续有什么问题<br />       10. Canal 导 mysql 时,有没有遇到什么问题,怎么解决的<br />       11. Hive 里边怎么保证今天的数据就到今天的对应文件</li></ol><h1 id="28-小牛漫画小明科技"><a class="markdownIt-Anchor" href="#28-小牛漫画小明科技"></a> 28.   小牛漫画&amp;小明科技</h1><p>1.问一些简单的算法,  写一下相关的伪代码或者实现过程(快排,  或者其他,  然后给一道简单的示例题目做,  写一下大致流程) 2.简单考察两道 sql 题目,  看看熟练程度,  讲一下过程 3.简单讲一下框架原理,  例如  zk 的选举   还有  yarn 的调度器 4.讲一下数仓搭建的流程,  实时的相关的搭建,  计算过程 5.讲一下为什么从 streaming 转 flink(按项目问),  然后问一下 spark/flink 的一些原理 6.项目中遇到什么样的难题,  怎么解决 7.经理会对一些 es, redis 等比较次重要的组件   还有一些基本的实际操作的点进行提问,  考察业务熟悉度  ,  然后根据实际项目的场景,  随意问一些问题,  问怎么处理<br />8.hadoop1.0 和 2.0 什么区别？</p><h1 id="29-深圳楼迅新房科技有限公司"><a class="markdownIt-Anchor" href="#29-深圳楼迅新房科技有限公司"></a> 29.   深圳楼迅（新房）科技有限公司</h1><p>1.说一下你最近的项目，你在项目中担任什么职务，负责什么（详细具体的技术实现） 2.数据是怎么采集的，这个问的比较具体（后面了解到他们是初级阶段，目前停留在数据采集与清洗阶段） 3.你最熟悉的语言是什么？问了一个 scala 异步同步的相关技术问题   4.数据怎么从 Hbase 到 Mysql （有这个需求吗？我们不是直接写入 Hbase 或 Mysql 吗）<br />5.  你给自己的定位是怎样的？<br />6.  你感觉你从这些项目中学到了哪些东西？<br />7.  你有什么想要了解的？</p><h1 id="30-平安智慧城市"><a class="markdownIt-Anchor" href="#30-平安智慧城市"></a> 30.  平安智慧城市</h1><p>1、hashmap<br />2、讲讲 MySQL、Oracle、PG 分别对应的 CDC 工具<br />3、canal 原理<br />4、Kafka 为什么能做到高并发</p><h1 id="31-平安证券"><a class="markdownIt-Anchor" href="#31-平安证券"></a> 31.  平安证券</h1><p>1 hive 有碰到过哪些关于数据倾斜的情况?  什么情况导致的?怎么解决的?<br />2  你们公司一天的数据量有多大?<br />3  除了二次聚合还有什么方式可以解决数据倾斜?<br />4 spark 和 flink 的区别?<br />5 hive 在你们集群上分配多少个节点?你们的集群配置怎么样?<br />6  让我说几个离线指标,说几个用到窗口函数的？</p><h1 id="32-华为"><a class="markdownIt-Anchor" href="#32-华为"></a> 32.  华为</h1><h2 id="学长-1-9"><a class="markdownIt-Anchor" href="#学长-1-9"></a> 学长 1</h2><p>1.介绍离线数仓分层<br />2.oracle 分区采用哪些分区<br />3.mysql 存储引擎<br />4.hive 数据倾斜优化<br />5.1.Linux 常用命令    脚本中&amp;！什么意思</p><h1 id="33-深圳西人马"><a class="markdownIt-Anchor" href="#33-深圳西人马"></a> 33.  深圳西人马</h1><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954107006-817b5bda-44d8-4b22-b725-9c1f2f72a401.png#height=326&amp;width=335" alt="" /></p><h1 id="34-货拉拉"><a class="markdownIt-Anchor" href="#34-货拉拉"></a> 34.  货拉拉</h1><h2 id="学长-1-10"><a class="markdownIt-Anchor" href="#学长-1-10"></a> 学长 1</h2><p>1.  你张几亿的订单表,  你们是怎么关联的?<br />2.  写了数据治理的内容,数据质量标准是怎么确定的? 3. ODS 层发现问题你们是怎么推动去解决的.  整个过程是怎么落实的？ 4. Atlas 原理有了解吗?  底层原理不了解你怎么知道这个血缘关系就是准确的呢?<br />5.  只用 flink 实现了那么点指标?</p><h1 id="35-乐刷"><a class="markdownIt-Anchor" href="#35-乐刷"></a> 35.  乐刷</h1><p>1 hive 分层,dwt 是什么原理<br />       2  压缩格式选 lzo,为什么,有啥特点<br />       3 kafka 分区,副本,咋选的,几个,为什么<br />       4  列式存储 parquet 有啥优点<br />       5  口述拉链表原理<br />       6  可视化用的什么</p><h1 id="36-平安证券"><a class="markdownIt-Anchor" href="#36-平安证券"></a> 36.  平安证券</h1><h2 id="学长-1-11"><a class="markdownIt-Anchor" href="#学长-1-11"></a> 学长 1</h2><p>1.自我介绍 2.在公司的主要工作内容（离线还是实时）（他们偏向离线） 3.介绍一下离线数仓项目 4.使用了那些函数解决了那些指标，比较看重窗口函数和自定义函数 5.使用过程中有没有碰到问题？  异常或者难的指标 6.数据量是多少？最大的表有多少条数据？感觉在核对数据量是否会碰到一些问题<br />7.Hive 的优化及数据倾斜的解决方法<br />8.Spark 和 Flink 的区别</p><h2 id="学长-2-7"><a class="markdownIt-Anchor" href="#学长-2-7"></a> 学长 2</h2><p>1.flink 和 sparkstreaming 比，为什么轻量 2.使用 flink 的最大的问题<br />3.cdc connector 了解情况<br />4.kafka 消费的一致性<br />5.  为什么使用 sqoop<br />6  离职原因和一些业务方面的</p><h1 id="37-山西证券"><a class="markdownIt-Anchor" href="#37-山西证券"></a> 37.  山西证券</h1><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954107923-a5d6a8a2-78c0-42d9-abaa-e5542ef95e9e.png#height=463&amp;width=367" alt="" /></p><h1 id="38-顺丰驿站"><a class="markdownIt-Anchor" href="#38-顺丰驿站"></a> 38.  顺丰驿站</h1><p>1、hive 内外部表、hive 的文件类型<br />2、Flink 的 exactly once<br />3、给了一个 Flink 的简单需求，统计每天收派件的数量<br />4、Flink 如何保证乱序数据的使用？<br />5、介绍一下 ES 的架构<br />6、Kafka 如何维护 offset 7.详细讲下雪花模型和星型模型</p><h1 id="39-赢时胜"><a class="markdownIt-Anchor" href="#39-赢时胜"></a> 39.  赢时胜</h1><h2 id="学长-1-12"><a class="markdownIt-Anchor" href="#学长-1-12"></a> 学长 1</h2><p>1.        怎么转行的<br />2.        为什么离职<br />3.        怎么这个时间点离职<br />4.       hive 的数据倾斜   场景+解决方案<br />5.       spark 的数据倾斜  oom+解决方案<br />6.       flink 踩过的坑</p><h2 id="学长-2-8"><a class="markdownIt-Anchor" href="#学长-2-8"></a> 学长 2</h2><p>1.       spark 默认并行度多少，怎么来的<br />2.        多线程安全问题，有遇到的具体场景<br />3.        加锁有哪几种方式<br />4.       spark 提交的 executor 核数多少，怎么来的<br />5.        为什么用到 hbase,hbase 和 ES 有啥关联场景<br />6.       hdfs 的数据怎么到 hive 表的</p><h2 id="学长-3-2"><a class="markdownIt-Anchor" href="#学长-3-2"></a> 学长 3</h2><p>1.常见加密算法，flume 数据如何加密；<br />2.hbase 的自动分区和 flush<br />3.java8 集合的流式处理新特性<br />4.sparkshuffle 和 mrshuffle 的区别 5.业务的了解 6.工作中上下游协调情况 7.刚问我离线数仓我准备输出就被叫停了</p><h1 id="40-北明软件"><a class="markdownIt-Anchor" href="#40-北明软件"></a> 40.  北明软件</h1><p>1.       sqoop 换行符<br />2.       sqoop 导入 hdfs 出现 oom 是什么原因<br />3.       kafka  数据从 Producer 到 broker 做了什么<br />4.       Hbase 什么时候真正的删除数据?<br />5.       Spark 的通讯流程?<br />6.       FLink 十几张表 join 怎么处理?<br />7.       FLink 定时器怎么用的?<br />8.       Flink 做了哪些指标?<br />9.       Flink 和 SparkStreming 的区别?</p><h1 id="41-平安科技"><a class="markdownIt-Anchor" href="#41-平安科技"></a> 41.  平安科技</h1><p>1、etl 数据清洗<br />2、数据重复的处理<br />3、版本迭代时间<br />4、java 的熟练度<br />5、滑动窗口的解释<br />6、hive 数据怎么导入到 mysql 中<br />7、hive 中遇到比较难的问题</p><h1 id="42-完美世界"><a class="markdownIt-Anchor" href="#42-完美世界"></a> 42.  完美世界</h1><p>1.        双层 Flume 把数据的处理时间改成业务时间了，如果我有历史数据现在采集了，我怎么知道哪些是现在采集的历史数据？<br />2.        双层 Flume 不能做到，是不是要再外接一个系统？</p><h1 id="43-oppo"><a class="markdownIt-Anchor" href="#43-oppo"></a> 43. OPPO</h1><p>1.        流怎么和一个百亿级的表 Join？<br />2.        小文件有哪些危害？不仅仅是寻址时间长<br />3.       Kafka 的特点，为什么适用于大数据计算？怎么实现高吞吐的</p><h1 id="44-小牛动漫"><a class="markdownIt-Anchor" href="#44-小牛动漫"></a> 44.  小牛动漫</h1><p>1.说一下数仓建模思路 2.给一个指标如留存率具体说说是怎么设计 hive 各层需要什么表来实现这一指标的计算 3.双流 join 的思路</p><h1 id="45-跨越速递"><a class="markdownIt-Anchor" href="#45-跨越速递"></a> 45.  跨越速递</h1><p>1.Flink 的 JobManager 和 TaskManager 能不能失败重启？机制是什么？ 2.拉链表如果有一天没有拉成功，导致漏拉了一天(比如 11 号漏了 12 号的，直接拉 13 号)，怎么处理？怎么避免？<br />3.Flink 有没有出现过窗口无法触发计算的情况？面试官说如果有两个分区，一个分区没数据，另一个分区一直有数据，就会出现这个问题，不是很懂<br />4.Flink 的 checkpoint，如果部分算子已经完成本次 ck，宕机以后是从哪开始？保存好的算子开始还是从 source 端重新开始？我记得鹏哥说过是从 source 重新开始，但面试官的样子好像表示我说的不对 5.为什么维度表要存在 HBase 里不存在 MySQL 里，HBase 的 rowkey 设计有什么实际应用场景？<br />6.Spark 的下游分区如何知道从哪个上游分区读取数据？<br />7.Hive 的 groupBy 和窗口的 partitionBy 有什么区别？从数据倾斜的角度讲<br />8.Flink 各个窗口的区别？从源码的角度讲<br />9.Flink 的前端页面可以看到 Job 运行过程中的那些信息？<br />10.Yarn 看没看过 stage 和 task  就是 spark on yarn 11.问了 presto 的缺点和优化 12.问了 hbase 和 phoenix 的原理，执行流程 13.问了 Hadoop 的 MR 流程</p><h1 id="46-平安外包"><a class="markdownIt-Anchor" href="#46-平安外包"></a> 46.  平安外包</h1><p>1.        介绍下做过的几个项目<br />2.        自己在项目中做了哪些工作<br />3.       flume 架构<br />4.       kafka 规模<br />5.       kafka 特点<br />6.       kafka 一致性如何实现<br />7.       hadoop 架构<br />8.       hbase 特点<br />9.       hbase 与 mysql 区别与联系<br />10.    hive 有哪些函数<br />11.    order by,sortby,distributeby,clusterby 区别<br />12.    hive 分区和分桶区别<br />13.    hive 优化<br />14.     小文件合并具体实现<br />15.    mr 流程<br />16.    mr 为什么慢及如何优化<br />17.    sparkstreaming 运行原理<br />18.    spark  核数参数设置<br />19.     对 java 和 scala 哪个熟悉<br />20.    java 中数组和集合区别</p><h1 id="47-akulaku"><a class="markdownIt-Anchor" href="#47-akulaku"></a> 47. Akulaku</h1><p>1、介绍 Flink 的精准一次性<br />2、Barrier 机制<br />3、Flink 任务的并行度是多少<br />4、Slot 与 TaskManager 之间的关系<br />5、详细写出来 TopN 的 Flink 实现。面试官非常专业，首先会询问你这个业务的场景是什么？然后一个算子一个算子进行讨论，甚至会问到算子里面传什么对象。Aggregate 算子的多参使用，这个面试官非常懂，非常懂<br />6、详细写出来双流 join，同时要求解释这个应用的场景，非常细致。还一起探讨了业务逻辑，非常细致。问这个问题他主要是想表示双流 join 的关键点是数据的丢失<br />7、Spark 和 Flink 的区别，我讲了时间语义，讲了窗口，讲了 Flink 的源码更清晰</p><h1 id="48-腾讯视频"><a class="markdownIt-Anchor" href="#48-腾讯视频"></a> 48.  腾讯视频</h1><p>1.        是视频面试，要手敲代码<br />2.       spark 的 standalone 模式有什么不足，相比于 yarn 模式<br />3.       spark 是怎么接收数据的？我说了个 receiver，他又问怎么设置 receiver，怎么创建 receiverStream（Receiver 对象）</p><h1 id="49-联易融"><a class="markdownIt-Anchor" href="#49-联易融"></a> 49.  联易融</h1><p>1.hdfs 文件读写流程<br />2.hbase 文件读写流程<br />3.spark 调度流程 4.常见问题的遇见与处理  （小文件、数据倾斜）<br />5.flink 比 spark 的优点<br />6.flink 的执行为什么比 spark 高效<br />7.kafka 高速读写原理<br />8.hbase regionServer 挂了怎么办？ 9.读过哪些源码？</p><h1 id="50-赢时胜"><a class="markdownIt-Anchor" href="#50-赢时胜"></a> 50.  赢时胜</h1><p>1.HBase 的 regionService 挂掉怎么办？元数据丢失怎么办？<br />这些场景下一旦 RegionServer 发生宕机，HBase 都会马上检测到这种宕机，并且在检测到宕机之后会将宕机 RegionServer 上的所有 Region 重新分配到集群中其他正常 RegionServer 上去，<br />再根据 HLog 进行丢失数据恢复，恢复完成之后就可以对外提供服务，整个过程都是自动完成的，并不需要人工介入。基本原理如下图所示：<br />Hbase 的 Hlog 失效先失效再删除<br />2.Hive 的解析过程？<br />        Client  用户接口（JDBC、ODBC） WEBUI<br />         元数据空间  MetaStore<br />         解析器： SQL 转为抽象语句树 AST  并分析优化<br />         编译器：编译器（Physical Plan）：将 AST 编译生成逻辑执行计划。<br />         优化器（Query Optimizer）：对逻辑执行计划进行优化。<br />         执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于 Hive 来说，就是 MR/Spark。 3.专门看待 Hive 的严格模式？<br />         严格模式限制了分区表全表扫描，order 不 limit，禁止笛卡尔集，不允许单动态分区（必须要有一行静态 partition(dt=xxx,hour)）<br />4.hashmap 的底层实现：<br />hashMap 的底层结构在 jdk1.7 中由数组+链表实现，在 jdk1.8 中由数组+链表+红黑树实现，以数组+链表的结构为例。<br />HashMap 只有 containsValue 和 containsKey 方法；HashTable 有 contains、containsKey 和 containsValue 三个方法，其中 contains 和 containsValue 方法功能相同。HashTable 不允许 null<br />         引申：什么是红黑树：<br />5.RDD 的血缘关系怎么保存的：<br />存储在一个 Seq[depency]里面；每一个新的 rdd 创建会传入当前的 seq[depencies],新的 rdd 进行一个加入新的 dependency 并获取结果； 6.如何理解 RDD 的 checkpoint 与怎么替代：</p><p>7.Synchronized 与 Lock 的区别：<br />Lock 比 Synchronized 更灵活；<br />S 自动上锁解锁，Lock 手动上锁解锁；</p><h1 id="51-腾讯-ieg"><a class="markdownIt-Anchor" href="#51-腾讯-ieg"></a> 51.  腾讯 IEG</h1><p>1.       jvm 垃圾回收策略<br />2.       hashMap 底层源码<br />3.       Spark on Yarn  提交流程<br />4.        你项目里数据的流转是怎样的</p><h1 id="52-shopee-面试题"><a class="markdownIt-Anchor" href="#52-shopee-面试题"></a> 52. shopee 面试题</h1><p>1.数据结构：hashmap，JUC，JVM，红黑树，二叉树，LSM 树（Hbase 的底层数据结构）， 2.大数据知识：Flink 一次性语义；kafka 高速的原因； 3.问我最近写什么多！我说写业务和 scala 还有 shell 多，转手编程题 4.快排的实现逻辑与时间空间复杂度以及推算</p><h1 id="53-腾讯维保"><a class="markdownIt-Anchor" href="#53-腾讯维保"></a> 53.  腾讯维保</h1><p>1.sqoop 读取数据库的方式：jdbc/binlog?<br />2.sqoop 如何提高效率？<br />3.yarn 的容量调度器和 fair 调度器的适用场景？<br />4.synized 底层加锁的机制？<br />5.hive 导 hbase，impala 导 hbase 等各种乱倒的解决策略；<br />6.canal/maxwell 遇见大量删除操作时如何解决性能瓶颈；压测如何做？<br />7.hbase 的数据 load 方式？ 8.如果子类的方法与父类方法名相同，参数与返回值不同，重载还是重写？<br />9.scala 中的  + -  等元素栈怎么判定效用。</p><h1 id="54-趣头条上海"><a class="markdownIt-Anchor" href="#54-趣头条上海"></a> 54.  趣头条(上海)</h1><p>1.       Spark 提交流程   讲完   就问 Driver 的组成，Executor 内存模型，问的很细，一直问到执行<br />2.       rdd 的组成架构<br />3.       rdd ds df  的不同之处<br />4.       spark shuffle  要每个细节都讲清楚<br />5.       Kafka ACP<br />6.       Kafka 的同步机制<br />7.       zookeeper 的一致性算法<br />8.       Hive 的执行流程<br />9.        基本就是 spark 和 kafka 的底层</p><h1 id="55-领星科技"><a class="markdownIt-Anchor" href="#55-领星科技"></a> 55.  领星科技</h1><p>1.       HBase 怎么搭建实时数仓？<br />2.        手写两道 sql 和 flink 的统计 UV(手写)<br />3.        笔试试卷之前胜哥发在群里过<br />4.        基本不用讲项目，架构师很懂</p><h1 id="56-雷漫"><a class="markdownIt-Anchor" href="#56-雷漫"></a> 56.  雷漫</h1><p>1.       processFunction,keyedProcessFunction,windowProcessFunction 的应用，讲实例<br />2.       redis 保存 offset 写不进去，会有数据不一致的情况怎么解决<br />3.       hbase 底层二级索引原理，不是 phoneix 的二级索引<br />4.       SparkJoin 除了 shuffle 还干了那些事</p><h1 id="57-vivo-外包"><a class="markdownIt-Anchor" href="#57-vivo-外包"></a> 57. vivo 外包</h1><p>1.介绍数仓项目 2.工作中有没有遇到 hive 倾斜问题以及怎么解决<br />3.map 数根据什么设置，设置的什么参数 4.一张理财项目的表大致如下<br />id      user_id      begin_date      end_date<br />1            A            2020-01-01     2020-01-30<br />2            A            2020-01-02     2020-01-30<br />3            A            2020-01-10     2020-02-10<br />4            B            2020-02-11     2020-02-30<br />5            C            …   <br />begin_date 代表买入一笔的时间，end_date 代表将其卖出的时间，求用户历史最大的持仓笔数。</p><h1 id="58-腾讯-teg"><a class="markdownIt-Anchor" href="#58-腾讯-teg"></a> 58.  腾讯 TEG</h1><p>1.       MR 机制，hashmap 原理,数据倾斜怎么优化？<br />2.       JVM 有调优过么，怎么调优，有哪些调优参数？<br />3.        线程池怎么用的？<br />4.        项目中你遇到最大的难点，怎么做的？<br />5.       redis 使用的什么模式？<br />6.        假如往多个数据库一起写数据，某个数据库挂掉了，要怎么实现一致性？<br />7.        有看过哪些源码，讲一下？<br />8.        三面问的的是数据量多大，每天多少条数据？<br />9.        集群的规模，cpu 多少，内存多少？<br />10.     缓慢变化维的同步策略？<br />11.     数仓你做了哪些事？<br />12.     你有看过数仓相关的书籍么？<br />13.     对于每来个需求就分析这样你有没想过优化的方法不用做重复的工作？</p><h1 id="59-字节"><a class="markdownIt-Anchor" href="#59-字节"></a> 59.  字节</h1><p>1.       flink 与 sparkstreaming 的区别<br />2.        假设一种场景，flink 上游发送数据很慢，你认为可能什么原因导致<br />3.        介绍你对泛型的理解，类/成员变量怎么使用泛型，哪些类型不能使用泛型<br />4.       int 和 Integer 的区别，他们表示的数值的范围是一样的么<br />5.        手写 hql,给定一个表 event,字段是 date,timestamp，event_name(有 login_time 和 loginout_time 两种)，uid，写出 PCU(每天同时最大在线人数)<br />6.        手写 java/scala 实现从一个文本里面读取，统计出现次数 top3 的单词</p><h1 id="60-西人马"><a class="markdownIt-Anchor" href="#60-西人马"></a> 60.  西人马</h1><p>1.       sparkstreaming 实时项目有什么瓶颈？为什么要搭建二期实时项目？<br />2.        说一下 spark rdd 的特点，为什么 rdd 是弹性的？<br />3.       hive 架构，hive sql 是如何解析成 mr 任务的？<br />4.       hive 元数据默认存储位置？为什么要改成 mysql,而不是其他的？<br />5.        为什么用 flume 采集数据？有没有其他可用的框架？<br />6.        用什么做的 namenode 高可用？一台挂了另一台如何知道的？<br />7.        用 azkban 调度离线任务的时候，如果有一个 sql 执行失败了，必然会导致整个调度任务的失败，怎么解决？为什么不是一个表一个脚本来调度？</p><h1 id="61-3k-游戏"><a class="markdownIt-Anchor" href="#61-3k-游戏"></a> 61. 3k 游戏</h1><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954108711-737d16c3-299d-4d77-ab1e-b0893c2aa664.png#height=444&amp;width=417" alt="" /></p><h1 id="62-平安寿险"><a class="markdownIt-Anchor" href="#62-平安寿险"></a> 62.  平安寿险</h1><p>1.  讲下你最熟悉的项目<br />2.  我说我公司是大概 5、6 个维度，面试官：“你们的维度是怎么确定的？”<br />3.  你们的标签是怎么确定的呢？<br />4.  某天用户量 50 万，第二天是 20 万，什么原因导致这样？<br />5.  你们怎么保证你们指标的结果是可靠的？</p><h1 id="63-云路科技"><a class="markdownIt-Anchor" href="#63-云路科技"></a> 63.  云路科技</h1><p>1.  数仓流程，遇到过最难的指标，说下思路<br />2.  实时做了哪些指标 3. Spark 和 flink 的区别<br />4.  实时数仓这，你们的结果放在哪？</p><h1 id="64-迷你玩科技"><a class="markdownIt-Anchor" href="#64-迷你玩科技"></a> 64.  迷你玩科技</h1><p>1.  使用的 Hadoop 版本<br />2.  你们集群规模多大？一个节点分配几个 G,几核？一个 executor 分配几个 G,几核？</p><h1 id="65-十方教育"><a class="markdownIt-Anchor" href="#65-十方教育"></a> 65.  十方教育</h1><ol><li>Kafka 优化？</li><li>Spark 和 flink 的 checkpoint<br />3.  说一下你们实时做的指标具体怎么实现的？<br />4. （离线）遇到过比较难的指标，讲一下思路</li><li>Flink 如何做到 exactly-once</li></ol><h1 id="66-恒合互联健康头条"><a class="markdownIt-Anchor" href="#66-恒合互联健康头条"></a> 66.  恒合互联（健康头条）</h1><p>1.    数仓刚搭建，进去后主要做离线，实时会做一点<br />2.   HDFS 读写流程<br />3.   Flink 和 Spark 区别<br />4.   MR 工作原理及优化<br />5.   Kafka 介绍<br />6.   java 集合<br />7.   Flume 介绍</p><h1 id="67-深圳鹏锐信息技术股份有限公司"><a class="markdownIt-Anchor" href="#67-深圳鹏锐信息技术股份有限公司"></a> 67.  深圳鹏锐信息技术股份有限公司</h1><p>1.        你们集群有多少台<br />2.        做哪些指标，没问如何实现<br />3.        介绍 Kafka<br />4.       Flink 有什么优势<br />5.        为什么要用 Flink<br />6.        你们的框架在集群上是怎么分布的<br />7.       NameNode 挂了该怎么办</p><h1 id="68-玉策网络科技深圳有限公司"><a class="markdownIt-Anchor" href="#68-玉策网络科技深圳有限公司"></a> 68.  玉策网络科技（深圳）有限公司</h1><p>1.        你擅长离线还是实时<br />2.        介绍一下你擅长的部分（我介绍的是离线整个数仓的设计，大概 20 分钟，过程中要和面试官有眼神交流）<br />3.        为什么来离职<br />4.        你 sql 能力怎么样，不会进行测试的，只问一下</p><h1 id="69-四格互联深圳信息技术有限公司"><a class="markdownIt-Anchor" href="#69-四格互联深圳信息技术有限公司"></a> 69.  四格互联(深圳)信息技术有限公司</h1><p>1.       MR 优化<br />2.       Kafka 结构<br />3.        数仓模型设计<br />4.       Flink 结合业务问问题<br />5.       Spark 实现哪些指标，如何实现<br />6.       Sql 数据倾斜，如何解决<br />7.        会结合业务问问题<br />8.       Kudu docker K8s</p><h1 id="70-微米科技"><a class="markdownIt-Anchor" href="#70-微米科技"></a> 70.  微米科技</h1><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954111948-0c1f3e0d-22f1-4f86-b691-358ebeaf4f73.png#height=529&amp;width=397" alt="" /></p><h1 id="71-跨越速运"><a class="markdownIt-Anchor" href="#71-跨越速运"></a> 71.  跨越速运</h1><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954112520-4483bc32-1f5d-427a-bee6-93073b93a798.png#height=344&amp;width=339" alt="" /></p><h1 id="72-明源云"><a class="markdownIt-Anchor" href="#72-明源云"></a> 72.  明源云</h1><p>1.Sparkstr 和 flink 的区别<br />2.spark 和 flink 精准一次消费的区别<br />3.hbase 的设计选择和组成 4. Flink 中怎么开窗 5.你做过那些指标？日活多少？<br />6.java 的基本数据类型<br />7.arraylist 和 linkedarraylist.的区别？hashmap 的底层<br />8.spark 和 flink.的双流 join 原理<br />9.flink 的 ck 怎么实现的，在哪里保存</p><h1 id="73-小牛动漫"><a class="markdownIt-Anchor" href="#73-小牛动漫"></a> 73.  小牛动漫</h1><p>1.索引的应用在哪些数据库？MySQL 索引一般怎么使用？ 2.排序算法的复杂度，实现方法 3.数据为什么能被压缩？ 4.实时监控订单下单支付的意义在哪？异常数据怎么处理<br />5.hashmap 底层原理<br />6.hbase 为什么这么快？组成结构？<br />7.hdfs 分布式的意义？</p><h1 id="74-极光"><a class="markdownIt-Anchor" href="#74-极光"></a> 74.  极光</h1><p>1.waterMark 的理解 2.数组的底层是什么<br />3.sql 题<br /><a href="http://4.es">4.es</a> 的索引<br />5.zookeeper 的选举机制</p><h1 id="75-中泓"><a class="markdownIt-Anchor" href="#75-中泓"></a> 75.  中泓</h1><p>1.Flume 组成，taildir 的优势，怎么从日志服务器读取数据到 flume,如果不用 tail 怎么设计断点续传？<br />2.Rdd 是什么？ 3.两个 executor 从同一个 hive 分区读取数据怎么优化，比如一个过滤字段，一个提取某些列？<br />4.spark 算子有哪些，作用是什么？<br />5.dag 怎么理解？任务怎么划分的？<br />6.spark 的 map 在使用前要注意什么？<br />7.flink 的优势是什么？<br />8.flink 的算子有哪些？怎么用的？<br />9.hbase 结构？怎么用 kafka 去读取 hbase 的数据？二级索引是什么？怎么建立的？为什么要建？<br />10.hbase 的 key 打散怎么做的？不打散会怎样？<br />11.spark  的 cache 和 persis 有什么区别？缓存级别有哪些，怎么用？</p><h1 id="76-腾讯外包"><a class="markdownIt-Anchor" href="#76-腾讯外包"></a> 76.  腾讯外包</h1><p>1.实时项目有消费 kafka 数据，kafka api 用过吗？具体使用到哪些类哪些接口？ 2.项目这边我看有使用到 redis，连接 redis 的时候导了哪些包？<br />3.shell 脚本写过吗？shell 脚本第一行写的是什么？ 4.说到离线数仓指标分析，面试官出了一道题目：去年一年登录的用户，找出连续 10 天登录的用户。</p><h1 id="77-任子行"><a class="markdownIt-Anchor" href="#77-任子行"></a> 77.  任子行</h1><p>1、你们使用的开发语言是 java 还是 scala（他们公司主要使用 java 开发，需要非常好的基础）<br />2、你对大数据最熟悉的组件有哪些<br />3、sparkstreaming 怎么精准一次消费的<br />4、你对 ES 熟悉吗？<br />5、Hbase 用过吗？<br />6、java 的 jvm 这些熟悉吗？</p><h1 id="78-深信服"><a class="markdownIt-Anchor" href="#78-深信服"></a> 78.  深信服</h1><p>1、kafka 如果挂了，你如何可以从你想要的地方获取数据；<br />2、数据消息数据积压怎么办？（增加 topic 分区数，同时提升消费者数量，或提高每批次拉取的数量）；<br />3、flink 比 spark 的区别在哪，有什么优势？<br />4、flink 的 watermark 机制；<br />5、flink 的窗口有哪些？<br />6、spark 的 reducebykey 和 groupbykey 有什么区别？<br />7、用 scala 手写一个从 1 加到 10000 的函数，并计算用了多少时间<br />8、flink 中，你们公司用的时间戳是哪个，怎么实现的</p><h1 id="79-华为外包"><a class="markdownIt-Anchor" href="#79-华为外包"></a> 79.  华为外包</h1><p>1、hive 数据倾斜怎么办？<br />2、hdfs 小文件过多怎么处理？<br />3、数据倾斜的表现有哪些？<br />4、flink 和 sparkstreaming 的区别？<br />5、了解 flink 的并行度吗？<br />6、flink 的 checkpoint 有了解么？<br />7、flink 有哪些窗口？<br />8、flink 的时间语义有哪些？<br />9、flink 的 watermark 机制了解吗？<br />10、flink 的 exactly-once 如何保证？<br />11、flink 的分布式快照有了解吗？<br />12、flink 的乱序数据如何处理？<br />13、你们公司 hive 存储的数据格式是什么？压缩方式呢？</p><h1 id="80-万普合心"><a class="markdownIt-Anchor" href="#80-万普合心"></a> 80.  万普合心</h1><p>1.       RDD 有什么特点？<br />2.        有哪些 shuffle 算子？<br />3.       stage 怎么划分的？<br />4.       scala 相比 java 有什么特点？<br />5.        有哪些集合类型？Map 有哪些？List 有哪些？<br />6.        对 java 语言知道多少？<br />7.       MySQL 索引有哪几种？<br />8.       MySQL 殷勤知道多少？</p><h1 id="81-惠安金科"><a class="markdownIt-Anchor" href="#81-惠安金科"></a> 81.  惠安金科</h1><p>1.       HashMap 底层，为什么要重写 hashcode 和 equals 方法？<br />2.       HashMap 的 put 过程<br />3.       flume 用什么 source？flume 挂了怎么办？<br />4.       tairdir 支持递归嘛？<br />5.        用代码怎么遍历多层目录？层次不确定，不用递归的方式？<br />6.        用户表 1-2 亿条，交易表几十亿条，有的用户与很多交易，有的用户交易极少，如何解决这样的数据倾斜？<br />7.        假设用户有几十万条呢？<br />8.       sparkstreaming 分组求 topN，每一组有一百万条，我只输出每组的前十，怎么办？注意：每组数据量很多不可能每组硬刚排序<br />9.        消费 kafka 有不规则的数据，怎么计算如 13 个字段和不足 13 个字段的数据条数？<br />10.    flink 消费 kafka 的数据怎么处理的？是做成流表嘛？</p><h1 id="82-r2games"><a class="markdownIt-Anchor" href="#82-r2games"></a> 82. R2Games</h1><p>1.        离线数仓有哪些比较难的指标？<br />2.        为什么要用到 flink？<br />3.       sparkstreaming 消费到重复的数据怎么办？<br />4.        留存用户的 sql 怎么实现？<br />5.        即席查询熟悉嘛？<br />6.        删除带 abc 字段的目录和文件，目录有多层级<br />7.        游戏经验</p><h1 id="83-小明太极小牛动漫"><a class="markdownIt-Anchor" href="#83-小明太极小牛动漫"></a> 83.  小明太极/小牛动漫</h1><p>Flink 1.请简述 flink 如何关联维表？<br />2.flink kafka consumer offset 在什么时候提交偏移量？如何保证只消费一次？<br />3.flink 有哪些 state backends？有什么区别？<br />4.flink taskmanager 内存模型（或者内存被哪些模块瓜分）？</p><p>Spark 1.请问什么情况下会出现数据倾斜？如何定位问题？如何解决？<br />2.Spark cache 有哪些应用场景？<br />3.Spark 1 个 executor(8core, 8G)和 8 个 executor(1core, 1G)有什么区别？如何选择？<br />4.SparkStreaming 会经常重启，重启后发现有大量的数据积累，这种情况该怎么办？</p><p>1.        在手机上拿些公司的产品问曝光率怎么在数仓中构建？建模过程<br />2.        留存率怎么设计？<br />3.       RDD 是什么？<br />4.        布隆过滤器的底层原理（简历上写了所以会问）<br />5.        数据量等情况</p><h1 id="84-千城攻略"><a class="markdownIt-Anchor" href="#84-千城攻略"></a> 84.  千城攻略</h1><p>1.        隐式转换说一下<br />2.        隐式类的加载顺序<br />3.       flink 有哪几种状态后端<br />4.       flink1.9 到 flink1.12 有哪些新特性？简历上写了关注 flink1.12 的发布所以才会问<br />5.        用过 oozie、impala 吗？<br />6.        什么是 flink cep？<br />7.       spark 用的哪个版本？有哪些常用算子？<br />8.        如何解决 kafka 多分区乱序的问题？<br />9.        有没有听过数据湖？<br />10.     什么叫 flink 的流批一体？<br />11.    RDD 是什么？<br />12.    HBase 中 HMaster 是干什么的？<br />13.    HBase 元数据存在哪里？<br />14.    HBase 的读写流程<br />15.    spark 的 cache,persist,checkpoint 有什么区别？<br />16.    spark 和 mapreduce 的区别有哪些？</p><h1 id="85-迷你玩"><a class="markdownIt-Anchor" href="#85-迷你玩"></a> 85.  迷你玩</h1><p>1.        起多少个线程去消费 kafka 里的数据？<br />2.        怎样分配 executor 的个数？每个 executor 的核数和内存<br />3.        集群规模？每个节点的资源情况，数据量多少？<br />4.        数据怎么采集到 kafka？<br />5.        手写代码题：写 spark 程序用 RDD 读取 json 和 csv 文件，做 join、去重等操作</p><h1 id="86-anker"><a class="markdownIt-Anchor" href="#86-anker"></a> 86. Anker</h1><p>1.        实时架构画出来<br />2.       sparkstreaming 和 flink 区别<br />3.       flinksql 可以解决暗些问题？<br />4.        可以实现纯 flinksql 开发吗？<br />5.       flink 如何实现精准一次性消费？<br />6.        离线分析结果怎么可视化？<br />7.       Azkaban 调度工具有什么缺点？<br />8.        离线数据怎么采集的？<br />9.        实时数据采集中断了，重启如何保证数据的一致性？</p><h1 id="87-纵腾"><a class="markdownIt-Anchor" href="#87-纵腾"></a> 87.  纵腾</h1><p>1.        用 sparkstreaming 做了哪些指标？<br />2.        知道数据湖吗？<br />3.       flink 和 spark 的区别？<br />4.        知道 spark 任务提交吗？</p><h1 id="88-小亿网络"><a class="markdownIt-Anchor" href="#88-小亿网络"></a> 88.  小亿网络</h1><p>1.        解决 flink 集成 hive，解决方案是什么？<br />2.       CEP 是什么？<br />3.        用过哪些存储介质？<br />4.       sparkstreaming 双流 join 分析的什么指标？<br />5.       flink 的 join<br />6.       flink cdc 是什么？<br />7.        说一下 mapreduce<br />8.        对 java 知道多少？经常用哪些集合？线程安全问题<br />9.       jvm 和 gc 呢？<br />10.     类的加载顺序<br />11.     数据超期了怎么处理？允许迟到了还是没到<br />12.     一个项目怎么开展？有规范额流程吗？</p><h1 id="89-网联安瑞"><a class="markdownIt-Anchor" href="#89-网联安瑞"></a> 89.  网联安瑞</h1><p>1.       flink 东西知道哪些？用 CEP 做了什么？<br />2.       watermark 机制，有哪几种状态后端？<br />3.        有没有用到 flinksql？<br />4.       flink 怎么实现精准一次性消费？<br />5.        为什么来深圳发展？<br />6.        事务性了解吗？<br />7.       es 的幂等性是怎么回事？<br />8.        现场 sql 题：<br />        用户表    id,name 等字段<br />        群组        user_id,id 等字段<br />        消息表    user_id,message,create_time<br />每个群最近 5 天发消息的条数大于 100 的用户（会观察你写 sql 的习惯）<br />9.        数据倾斜有哪些思路？<br />10.     近期的职业规划是什么？</p><h1 id="90-乐信"><a class="markdownIt-Anchor" href="#90-乐信"></a> 90.  乐信</h1><p>1.        你哪一个项目负责得多一点？说一下负责得多的这个项目的架构？<br />2.        整个架构数据的一致性是怎么保证的？<br />3.        为什么来深圳找工作？<br />4.        近期的职业规划</p><h1 id="91-云路科技云路信息科技"><a class="markdownIt-Anchor" href="#91-云路科技云路信息科技"></a> 91.  云路科技/云路信息科技</h1><p>1.        说一下离线数仓怎么设计/构建的？<br />2.        离线用到 es 做什么？<br />3.       Azkaban 用来干什么？<br />4.        为什么用 superset？hive 分析结果存到哪里？<br />5.        实时数仓怎么做的？<br />6.       kafka 多少个 topic？日数据量有多少？<br />7.       clickhouse 有哪些缺点？<br />8.        谈专业情况</p><h1 id="92-十方教育-荔枝微课"><a class="markdownIt-Anchor" href="#92-十方教育-荔枝微课"></a> 92.  十方教育-荔枝微课</h1><p>1.什么类型的图标适合用来理解数据分布？ 2.用户第一单购买的行为往往反映了用户对平台的信任度和消费能力。现在数据库中有一张用户交易表 order，其中有 userid（用户 ID）、amount（消费金额）、paytime（支付时间），请写出对应的 HiveSQL 语句，查出每个用户第一单的消费金额。（可以有三种 sql 是实现方式，这句话是我自己加的，笔试题上没有） 3.用户行为表 tracking_log，字段有 userid（用户 ID)，evt（用户行为），evt_time（用户操作时间），计算每天的访客数和他们的平均操作次数；统计每天符合以下条件的用户数：A 操作之后是 B 操作，AB 操作必须相邻。 4.概率题搜不到，也不懂 5.描述一下数仓分层设计的理念，结合应用对每一层的设计进行说明</p><h1 id="93-优学派"><a class="markdownIt-Anchor" href="#93-优学派"></a> 93.  优学派</h1><p>1.        专业问题，为什么要做大数据？<br />2.       flink 有哪些窗口函数<br />3.        用 hbase 做什么？有没有设计过 rowkey?<br />4.        很细致地看求职登记表，多少问问<br />5.        说一下离线数仓怎么构建的？<br />6.        说场景设计 rowkey<br />7.        找工作看重什么？</p><h1 id="94-海致星图"><a class="markdownIt-Anchor" href="#94-海致星图"></a> 94.  海致星图</h1><p>1.       scala 样例类有什么特点？创建对象的时候<br />2.        元组最多多少个元数？<br />3.        函数传参有个数上限吗？<br />4.        说一下隐式转换？<br />5.        模式匹配是什么？<br />6.        如果我把一个集合定义在算子外，然后在算子里调用这个集合会有什么问题？<br />7.       MapReduce 写过吗？是什么样的一个过程？<br />8.       spark 的 stage 划分<br />9.       spark shuffle 有哪些 shuffle？<br />10.    kafka 的选举机制<br />11.     在 zookeeper 里 kafka 的节点名<br />12.    java 知道多少？常用的集合有哪些？<br />13.    HashMap 的底层是什么样的？<br />14.     多线程有用过吗？<br />15.     写过接口吗？<br />16.     日常有做哪些优化？<br />17.    flink watermark 说一下<br />18.    flinksql 的 sink 是什么 sink 来实现的？<br />19.    flinksql 最后输出很多小文件，怎么解决？<br />20.    flink 集成 hive 怎么实现的？</p><h1 id="95-一壶科技"><a class="markdownIt-Anchor" href="#95-一壶科技"></a> 95.  一壶科技</h1><p>有一个表：班级，姓名，性别，身高，求每个班级身高最高的数据<br />现在有一个 2G 的文件 put 到 hdfs 上，假设你是文件系统，怎么存？<br />价格        1     2     3     6     1    <br />下标        0     1     2     3     4<br />现在有 100 秒的几个数据（即下标 0-99），要求先卖后买实现利润最大<br />如给 0720 四个数值，得到 24 小时值得时间 HH:MM 的最大时间，换其他 4 个数字也要输出最大时间<br />讨论 maxwell 和 flink cdc 对数据的同步</p><h1 id="96-富途"><a class="markdownIt-Anchor" href="#96-富途"></a> 96.  富途</h1><p>一、填空题 1.两个人轮流抛硬币，规定第一个抛出正面的人可以吃到苹果，请问先抛的人吃到苹果的概率是**。 2.天气预报说明天白天下午的概率为 60%，则明天上午会下雨的概率**。 3.假设一个 CPU 在 1 个时钟周期内能完成一次加法运算，问在一台 8 核的机器上完成一个长度为 64 的数组的求和计算共需要<strong>个时钟周期。<br />4.Hadoop 中</strong>负责存储数据块，<strong>负责维护文件系统树和文件夹元数据，<strong>负责集群中的资源管理和分配。 5.一个 Kafka 的集群由 5 个节点组成，如果需要正常工作，最多允许</strong>个节点不可用。<br />6.Kafka 的某个 topic 的配置的 partition 的数量是 8，那么 ConsumerGroup 的实例个数最多</strong>个。<br />7.32 为系统，当 int i = __时，表达式 i + 1 &lt; l  的值为真。<br />二、sql 编写<br />（1）有如下用户访问数据：<br />userId     visitDate  visitCount<br />u01  2017/1/21      5<br />u02  2017/1/23      6<br />u03  2017/1/22      8<br />u04  2017/1/20      3<br />u01  2017/1/23      6<br />u01  2017/2/21      8<br />U02 2017/1/23      6<br />U01 2017/2/22      4</p><p>要求使用 SQL 统计出每个用户的累积访问次数，如下表所示：<br />用户 id    月份         小计         累积<br />u01  2017-01  11   11<br />u01  2017-02  12   23<br />u02  2017-01  12   12<br />u03  2017-01  8     8<br />u04  2017-01  3     3</p><p>（2）有 Scores 表，数据如下：<br />±—±------+<br />| Id | Score |<br />±—±------+<br />| 1  | 3.50  |<br />| 2  | 3.65  |<br />| 3  | 4.00  |<br />| 4  | 3.85  |<br />| 5  | 4.00  |<br />| 6  | 3.65  |<br />±—±------+</p><p>按照分数从高到低排序，示例结果如下：<br />±------±------±------+<br />| Score | Rank1 | Rank1 |<br />±------±------±------+<br />| 4.00  | 1     | 1     |<br />| 4.00  | 1     | 1     |<br />| 3.85  | 2     | 3     |<br />| 3.65  | 3     | 4     |<br />| 3.65  | 3     | 4     |<br />| 3.50  | 4     | 6     |<br />±------±------±------+<br />三、编程题<br />在目录/data/log/nginx/下存放着 10 个 nginx 的日志文件：access.0.log, access.1.log, … access.9.log，日志文件的数据结构格式为：来源 IP,HTTP_REQUEST，例如：127.0.0.1,POST /app1/index.html，其中 HTTP_REQUEST 的格式为 HTTP_METHOD+UR，URL 的第一个分段为应用名，如上述 app1。<br />① 写出 spark 程序同基各应用的 PV 和 UV（基于 IP 去重）<br />② 先将日志加载到到 RDD 进行处理，然后转换为 DataFrame，最后用 SparkSQL 统计出伤处结果。<br />四、算法题<br />有一个整型数组 A，请写一个程序把他拆分成两个子数组，要求两个子数组的和相差最小，例如 A=1,2,3,4 拆分成 1,4 和 2,3(提示：不必均分，选用熟悉的语言实现)</p><p>面试<br />1.        问笔试题中 sql 题的实现思路<br />2.       flink 知道哪些？<br />3.        笔试题最后一题的实现思路</p><h1 id="97-任子行"><a class="markdownIt-Anchor" href="#97-任子行"></a> 97.  任子行</h1><p>1.        知道 spark 哪些东西？<br />2.        知道 kafka 哪些东西？<br />3.       flink 相比 spark 有哪些优点？<br />4.        用 hbase 做什么？<br />5.        设计过 rowkey 吗？<br />6.        对 java 知道多少？<br />7.       es 用到什么程度？<br />8.       sparkstreaming 如何设计采集周期？<br />9.        从 kafka 消费 sparkstreaming 如何转成 RDD 的？<br />10.     离职原因（为什么来深圳），期望薪资，职业规划（找工作看中什么）等</p><h1 id="98-华泰期货"><a class="markdownIt-Anchor" href="#98-华泰期货"></a> 98.  华泰期货</h1><p>1.        说一下离线数仓怎么构建的？<br />2.        为什么用维度建模？<br />3.        那你参与了实时平台，怎么搭建的？<br />4.        解释简历上写的产品等公司情况<br />5.        新工作看中什么</p><h1 id="99-道通科技"><a class="markdownIt-Anchor" href="#99-道通科技"></a> 99.  道通科技</h1><p>1.        简历上写的 ads 层用 parquet？hive 分析的结果不是存到 MySQL 吗？因为结果数据量很小了<br />2.        简历上写 ads 层数据导出到 MySQL 主键设计，具体怎么回事？<br />3.        用 superset 可以实现分页展示吗？首页，1-5 页，尾页</p><h1 id="100-翼海云峰华为外包"><a class="markdownIt-Anchor" href="#100-翼海云峰华为外包"></a> 100.         翼海云峰（华为外包）</h1><p>1.        描述 stage 的划分，然后说着说着谈到 shuffle，就说了下有哪些 shuffle 算子<br />2.       shuffle 会有哪些性能问题<br />3.        广播的变量特别大呢？不适合广播怎么办？<br />4.       spark 提交流程<br />5.       10 个 executor，只用到了 6 个节点的 executor，driver 怎么知道从这 6 个节点拉取数据？<br />6.        项目中是否用到广播变量？用到广播变量的目的<br />7.       RDD 的缓存，persist 方法有哪些缓存级别？<br />8.       RDD 缓存，是怎么缓存的？<br />9.       map 和 mapPartition，什么时候用 mapPartition？<br />10.     样例类和普通类有什么不一样？<br />11.     隐式转换的作用和应用场景<br />12.     隐式参数和隐式值怎么用？<br />13.     隐式转换有哪些坑？对于项目工程来说有哪些问题？<br />14.    kafka 消息的 topic 内部是怎么存储的？<br />15.     有哪些 ack 机制？<br />16.    ack=1 leader 落盘，follwer 还没同步，集群内部怎么恢复？但是数据没有同步，怎么办？<br />17.     堆内存有哪几个部分组成？<br />18.    java 的线程池有没有用过？java 有没有多线程的编码经验？<br />19.     模式匹配和 java 里的 swtich case 有什么区别？<br />20.    scala 闭包将一下<br />21.     发生数据倾斜怎么解决？<br />22.     数据结构有了解吗？<br />23.     发现端口被占用了，如何排查被哪个进程占用了？</p><h1 id="101-软通华为外包"><a class="markdownIt-Anchor" href="#101-软通华为外包"></a> 101.         软通（华为外包）</h1><p>1.        擅长哪些？最近做的什么项目？<br />2.        用 flink 实现了什么指标？<br />3.        双流 join 怎么做的？<br />4.        常用的算子有哪些？<br />5.        求 topN，每隔商品的前几名，精确到用到的算子<br />6.        造成内存溢出的 task 怎么知道是哪个 task 造成的内存溢出？接下来怎么定位造成内存溢出的 task？哪些订单造成的 task 算得比较慢？<br />7.        如何保证消息得不丢失不重复？（答了手动保存偏移量和幂等性）<br />8.       kafka 如何保存数据得不丢不重复？<br />9.        如果 kafka 宕机了，何保证数据得不丢失？<br />10.     生产者不重复是怎么保证的？<br />11.     消费者怎么保证消费的不重复？<br />12.    kafka 数据保留多久？<br />13.    kafka 的分区数据有序吗？如何保证全局有序？<br />14.    flink 做了什么优化？<br />15.     一天有几十亿数据，如何解决数据的挤压？如何保证不丢失、不挤压、内存不溢出？</p><h1 id="102-荣耀中软外包"><a class="markdownIt-Anchor" href="#102-荣耀中软外包"></a> 102.         荣耀（中软外包）</h1><p>1.        为什么用 flink 换用 sparkstreaming？<br />2.        离线数仓的数据采集<br />3.        离线四层做了什么事情？<br />4.       dm 层？datamark 数据集市<br />5.        你知道拉链表吗？<br />6.        离职多久了？<br />7.        常用的分析函数有哪些？写 hivesql 用到的<br />8.        数据倾斜有哪些解决办法？<br />9.        接触项目的数据量有多大？十几 G 有多少条？<br />10.    HiveSQL 和其他 SQL 的语法、函数有什么区别？<br />11.    MySQL 支持 update 吗？<br />12.    Hive 怎么更新数据？支持 update、delete 吗？<br />13.     用过哪些时间函数？说函数名即可<br />14.     获取当前日期是本周周几用哪个函数？<br />15.     有哪些字符串函数？<br />16.     关于 SQL 的性能优化<br />17.     关于 hive 参数的优化<br />18.     三个字段学号，科目，成绩，求语文&gt;数学的人的学号，语文成绩，数学成绩，并以语文成绩从高到低排序<br />19.     改文件属主的命令<br />20.     改文件读写的权限<br />21.     根据文本内容搜索是在哪个文件</p><h1 id="103-平安人寿"><a class="markdownIt-Anchor" href="#103-平安人寿"></a> 103.         平安人寿</h1><p>1.        在项目中负责什么？（直接说离线和实时做了什么）<br />2.       sparkstreaming 了解哪些东西？<br />3.       RDD 常见算子<br />4.       MapReduce shuffle 和 spark shuffle 的本质区别<br />5.       kafka 的特性<br />6.       HBase 为什么可以实现实时读写？<br />7.        实时项目的架构、整体工作安排，在其中承担什么工作？<br />8.        原始日志数据一个 topic 几个分区？怎么计算的？<br />9.        每天的数据量有多少？<br />10.     一条数据有多大？做 kafka 的压测，每秒给多少条？<br />11.     给你一个实时需求，你怎么安排这个工作？<br />12.     接收器模式和直连模式有什么区别？<br />13.     数据倾斜怎么处理？<br />14.     数据倾斜的理论基础<br />15.     有没有带项目？<br />16.    spark 源码有没有研究？<br />17.    6 点下班什么时候走合适？<br />18.     压力最大的工作是什么？</p><h1 id="104-维正集团"><a class="markdownIt-Anchor" href="#104-维正集团"></a> 104.         维正集团</h1><p>1.        说说 sparkstreaming 和 flink 的区别<br />2.       RDD 是什么？<br />3.        你最近研究 flink，对什么比较熟悉呢？<br />4.       （场景）业务下跌了，你分析一下原因<br />5.        大表加载到 es，怎么保证性能不出问题？在大家都不熟悉且紧急的情况下，怎么办？</p><h1 id="105-荣耀中软外包"><a class="markdownIt-Anchor" href="#105-荣耀中软外包"></a> 105.         荣耀（中软外包）</h1><p>1.       flink 里面有哪些组件？<br />2.        有没有用过在线分析引擎？比如 clickhouse<br />3.       flink 有没有遇到任务延迟或者数据积压的情况？<br />4.        消费 kafka 有遇到脏数据吗？怎么处理的？<br />5.       flink 里面有脏数据怎么处理？<br />6.        有没有参与过 hadoop 集群的安装部署、运维的工作？<br />7.        数据应用上的开发有没有？分析结果可视化的开发工作<br />8.        数据仓库这一块你知道多少？每一层都做了什么事情？<br />9.        数据冷库和热库的概念</p><h1 id="106-货拉拉"><a class="markdownIt-Anchor" href="#106-货拉拉"></a> 106.         货拉拉</h1><p>1.       flink 中的 slot<br />2.       flink 被压问题<br />3.       flink 并行度的处理，实际生产中怎么设置 flink 的并行度？<br />4.       flink local、standone、yarn 的部署模式分别是什么？<br />5.       flink PerJob、Session 分别是什么？<br />6.       Flink WebUI 怎么看起的 flink job<br />7.       java 的数据结构、jvm、多线程及线程的安全问题等（面试官一再想问 java）<br />8.       ClassNotFonud 可能的原因</p><h1 id="107-akulaku"><a class="markdownIt-Anchor" href="#107-akulaku"></a> 107.        AKULAKU</h1><p>1.        问简历上写的指标的实现过程，细致到如 pv、uv 的代码过程<br />2.       Flink 端到端的一致性，3 种状态后端的区别<br />3.       MySQL 是否支持两阶段提交？<br />4.       barrier 对其和不对齐<br />5.        代码里怎么设置的 ck 和 barrier<br />6.       flink 的并行度，怎么设置 flink 的并行度？<br />7.       flink 的双流 join 怎么实现的？<br />8.       sparkstreaming 的双流 join 为社么要缓存？<br />9.        日活多少？算点击量还是用户数？月活多少？<br />10.     集群数据的规模？<br />11.     从后端日志数据到 kafka 到 sparkstreaming 要多长时间？<br />12.     实时指标怎么上线的？<br />13.     集群台数，几个 namanode？几个 datanode？<br />14.     有没有数据延迟？如上千个指标你能保证每个指标分析正常吗？如何监控每个指标都正常在跑？<br />15.    Hive 不支持中文什么原因导致的？具体到根本原因而不是现象，虽然你解决了<br />16.     怎么给数据脱敏？有哪些敏感数据？（简历上提到数据脱敏）</p><h1 id="108-易马达"><a class="markdownIt-Anchor" href="#108-易马达"></a> 108.         易马达</h1><p>1.       MapReduce shuffle 的过程<br />2.       MapRduce shuffle 和 spark shuffle 的本质区别<br />3.        对于 MySQL 的引擎知道吗？<br />4.       HBase 表的 rowkey 怎么设计的？结合实际业务说明<br />5.       Flink 怎么解决数据的时效性？</p><h1 id="109-华锐金融"><a class="markdownIt-Anchor" href="#109-华锐金融"></a> 109.         华锐金融</h1><p>1.       spark 和 flink 的区别？<br />2.       java 的面向对象的特征？（要求系统性地说）<br />3.       java 的设计模式，一个劲的想问 java<br />4.        就简历上写了关注 flink 1.12，问 flink 1.12 有哪些新特性<br />5.        就一个需求怎么开展工作的？如何处理上下游的关系？</p><h1 id="110-望家欢"><a class="markdownIt-Anchor" href="#110-望家欢"></a> 110.         望家欢</h1><p>1.  讲一下离线数仓的数据采集和各层 2. dwd 层事务表和维度表的构建<br />3.  拉链表怎么实现的？<br />4.  算法题：<br />        日期        1     2     3     4     5     6     7    <br />        入库数    5     10   15   10   5     10   15  <br />        出库数    5     5     10   15   5     5     20    不够可以往前一天借，最多借 15 个<br />        价格        1     2     3     4     5     6     7<br />        当天价格        1     2     3     (10<em>4+5</em>3)/15       5     6     (15<em>7+5</em>10)/20      怎么实现？<br />开窗：5------------10--------------15（允许迟到 5s）,但是 15s 的事件迟迟不来，怎么办？</p><h1 id="111-领星"><a class="markdownIt-Anchor" href="#111-领星"></a> 111.         领星</h1><p>1.        从数据源采集数据采到 HBase，怎么保证数据的一致性？<br />2.        说一下 flink 实现了哪些指标？<br />3.       MySQL 索引<br />4.       rowkey 设计原则<br />5.       HBase 热点问题怎么解决？<br />6.        在众多存储介质中为什么选择 HBase？<br />7.        为什么选择 maxwell？<br />8.       maxwel 支持高可用吗？<br />9.        对调度工具了解吗？<br />10.     举例子说一下数仓的构建<br />11.    Redis 的幂等性是什么？<br />12.    Redis 雪崩了怎么办？<br />13.    Flink 端到端的一致性<br />14.    phoenix 操作 hbase 有什么问题？</p><h1 id="112-智灵时代"><a class="markdownIt-Anchor" href="#112-智灵时代"></a> 112.         智灵时代</h1><p>1.       kafka 怎么保证数据的一致性？<br />2.        说一下离线数仓怎么构建的？<br />3.        消费者消费 kafka 的数据是 kafka 主动推送的还是消费者拉取数据？<br />4.       spark 有哪些组件？<br />5.        什么是 RDD？<br />6.        转换算子和执行算子什么？<br />7.        宽依赖和窄依赖</p><h1 id="113-跨越速运"><a class="markdownIt-Anchor" href="#113-跨越速运"></a> 113.         跨越速运</h1><p>1.       hivesql 有哪些优化？<br />2.       hive 有哪些优化？<br />3.       join 时怎么做比较好？<br />4.        如果表很小，纯 sql 怎么处理？<br />5.       scala 语言有哪些突出优势？<br />6.        项目中用 es 做什么？<br />7.        对 Redis 掌握得怎么样？</p><h1 id="114-万科集团万物云"><a class="markdownIt-Anchor" href="#114-万科集团万物云"></a> 114.         万科集团万物云</h1><p>停车表 user_parking_record<br />字段        uder_id   date start_time      end_time community_id<br />类型        string      yyyy-MM-dd   hh:mm:ss hh:mm:ss string<br />① 分时统计各小区用户停车数<br />② 统计各小区用户停车高峰时段 Top3<br />③ 各用户近两周内最大连续停车天数<br />④ 若非连续停车天数不超过 1 天，也可做连续停车，求各小区最大连续停车天数的用户数分布情况<br />1.       spark 任务提交流程及任务划分、调度机制<br />2.       flink 端到端的状态一致性如何保证？<br />3.       flink 双流 join 及对流 join 的模式和运行原理<br />4.        部门培训体系是怎么构建的？参与培训的哪个环节？（自我介绍提到，简历上写了）<br />5.        哪些业务驱动你要用 flink？spark 不能实现吗？<br />6.       flink 端到端的一致性<br />7.       watermark 及 watermark 怎么保证分区的一致性？<br />8.       kafka 怎么保证数据的一致性？<br />9.        你们公司大数据平台是怎么构建的？<br />10.     为什么要离职并考虑来深圳？<br />11.     除了两阶段提交，还有哪些方式？<br />12.     写到 MySQL 两阶段提交，那写到 es、hbase 呢？<br />13.     你们实时有多少个指标？<br />14.    sparkstreaming 双流 join 的实现方式和 flink 双流 join 的上下界<br />15.     源数据字段发生了改变怎么办？</p><h1 id="115-中邮消费金融"><a class="markdownIt-Anchor" href="#115-中邮消费金融"></a> 115.         中邮消费金融</h1><p>1.       flink 分区？（尝试确认是说的分组吗？）<br />2.       spark 和 flink 的区别<br />3.       flink 的 checkpoint 和 savepoint 的区别？<br />4.       java 怎么起多线程？怎么解决线程安全问题？<br />5.       Map 集合怎么遍历？<br />6.       List 和 Set 集合的区别？<br />7.        简历上写你解决了数据倾斜，说一下场景和怎么解决的？<br />8.        如何实现 ads 导出到 MySQL 主键相同的数据更新？（简历上写了）<br />9.       HBase 不能按照标准 JDBC 写入的原因是什么？（简历上写了）<br />10.     部门培训体系怎么构建的？具体做了什么工作？<br />11.     你觉得培训体系有什么进一步优化的？<br />12.    parquet 压缩知道吗？压缩比还记得吗？</p><h1 id="116-顺丰科技"><a class="markdownIt-Anchor" href="#116-顺丰科技"></a> 116.         顺丰科技</h1><p>1.        采集行为数据的时候 kafka 如何保证日志行为数据的先后顺序？<br />2.        写过 kafka producer 端的代码吗？比如 flink 到 kafka<br />3.        如何理解 flink 精准一次性？<br />4.        下游用到 es，幂等性体现在哪些方面？需要注意什么？<br />5.        用 HBase 有没有设计过 rowkey？<br />6.       java 的并发知道吗？<br />7.        说一说对函数式编程的理解<br />8.       scala 的 Option 是什么？<br />9.       flink 接了 2 个数据源，有一个数据源的 barrier 迟迟不到达，比如慢了 5min，在这 5min 怎么做到精准一次性？（反问说的就是 barrier 的对齐和不对齐）<br />10.    barrier 对齐会有什么影响？<br />11.     背压机制怎么优化？<br />12.    flink 的 watermark 怎么理解？<br />13.    sparkstreaming 和 flink 在时间语义上的区别<br />14.    kafka 对比其他消息中间件有哪些高并发的特性？<br />15.     消费者组的概念有了解吗？与其他消息中间件相比，kafka 为什么要搞消费者组的概念？<br />16.    MySQL 有 2 类索引，说一下有哪 2 类？</p><h1 id="117-极光"><a class="markdownIt-Anchor" href="#117-极光"></a> 117.         极光</h1><p>1.        为什么年底离职？<br />2.        说一下 flink 的 checkpoint<br />3.       barrier 对齐和不对齐<br />4.        背压机制的底层源码有看过吗？<br />5.       RDD 是什么？<br />6.        往 kafka 里写数据，数据没写进去但也没有报错，怎么排错？查看主题确实没有数据<br />7.        有 10 个 partition，8 个消费者怎么分配？<br />8.       10 个 partition，但是只有一个 partition 有数据，其他 partition 没有数据可能是什么原因？<br />9.       hbase 怎么设计 rowkey？<br />10.    hbase 表设计一个列簇和多个列簇有什么优缺点？<br />11.     其中一个 regioserver 挂掉了，集群会有什么响应？<br />12.    cell 有哪些组成？<br />13.    hbase 删除是怎么样的？（回答涉及到小合并和大合并，会追着继续问小合并和大合并）<br />14.     计算 uv 怎么去重？<br />15.     布隆过滤器怎么实现 nv 去重？（简历上有）<br />16.     借助 redis 去重后怎么计数？<br />17.    HashMap 底层有了解吗？<br />18.    java 里字符串拼接有哪些方式？<br />19.     用过哪些日期函数？<br />20.     把一个时间戳转成字符串日期需要用到哪些函数？<br />21.     内部表和外部表有什么区别？<br />22.     同事把内部表删掉了，怎么恢复？<br />23.     数据量有多少？一天的数据量，多少条？<br />24.     日活<br />25.     集群规模</p><h1 id="118-荣耀本部"><a class="markdownIt-Anchor" href="#118-荣耀本部"></a> 118.         荣耀本部</h1><p>1.        离线数仓的分层设计<br />2.        解决了 ads 层导出到 MySQL 的主键设计是怎么回事？（简历上写了）<br />3.        你觉得做好离线数仓比较关键的是什么？<br />4.        具备什么能力/技能能把离线数仓做得更好？<br />5.        到了一个稳定的数仓团队，你觉得要做哪些方面的工作？<br />6.        当时做实时基于什么考虑用 sparkstreaming？<br />7.        做实时几个人？<br />8.        解决了手机端引发数据倾斜的问题是怎么回事？（简历上写了）<br />9.        构建部门新员工的培训体系是你一个人主导的吗？<br />10.     哪些组件比较熟悉？<br />11.    spark 常用算子？<br />12.    RDD 缓存了解多少？<br />13.    spark 和 flink 的区别？<br />14.     平时工作中同事怎么评价你的优点？<br />15.     选择新机会主要看哪些方面？</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="posts/0/"/>
      <url>posts/0/</url>
      
        <content type="html"><![CDATA[<p>尚硅谷大数据项目之 Flink CDC<br />(作者：尚硅谷大数据研发部)<br />版本：V1.12.0</p><h1 id="第-1-章-cdc-简介"><a class="markdownIt-Anchor" href="#第-1-章-cdc-简介"></a> 第 1 章 CDC 简介</h1><h2 id="11-什么是-cdc"><a class="markdownIt-Anchor" href="#11-什么是-cdc"></a> 1.1 什么是 CDC</h2><p>CDC 是 Change Data Capture(变更数据获取)的简称。核心思想是，监测并捕获数据库的变动（包括数据或数据表的插入、更新以及删除等），将这些变更按发生的顺序完整记录下来，写入到消息中间件中以供其他服务进行订阅及消费。</p><h2 id="12-cdc-的种类"><a class="markdownIt-Anchor" href="#12-cdc-的种类"></a> 1.2 CDC 的种类</h2><p>CDC 主要分为基于查询和基于 Binlog 两种方式，我们主要了解一下这两种之间的区别：</p><table><thead><tr><th></th><th>基于查询的 CDC</th><th>基于 Binlog 的 CDC</th></tr></thead><tbody><tr><td>开源产品</td><td>Sqoop、Kafka JDBC Source</td><td>Canal、Maxwell、Debezium</td></tr><tr><td>执行模式</td><td>Batch</td><td>Streaming</td></tr><tr><td>是否可以捕获所有数据变化</td><td>否</td><td>是</td></tr><tr><td>延迟性</td><td>高延迟</td><td>低延迟</td></tr><tr><td>是否增加数据库压力</td><td>是</td><td>否</td></tr></tbody></table><h2 id="13-flink-cdc"><a class="markdownIt-Anchor" href="#13-flink-cdc"></a> 1.3 Flink-CDC</h2><p>Flink 社区开发了 flink-cdc-connectors 组件，这是一个可以直接从 MySQL、PostgreSQL 等数据库直接读取全量数据和增量变更数据的 source 组件。目前也已开源，开源地址：<a href="https://github.com/ververica/flink-cdc-connectors">https://github.com/ververica/flink-cdc-connectors</a><br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954210135-65de98c4-054c-4c40-a8d1-dab58e184564.png#" alt="" /></p><h1 id="第-2-章-flinkcdc-案例实操"><a class="markdownIt-Anchor" href="#第-2-章-flinkcdc-案例实操"></a> 第 2 章 FlinkCDC 案例实操</h1><h2 id="21-datastream-方式的应用"><a class="markdownIt-Anchor" href="#21-datastream-方式的应用"></a> 2.1 DataStream 方式的应用</h2><h3 id="211-导入依赖"><a class="markdownIt-Anchor" href="#211-导入依赖"></a> 2.1.1 导入依赖</h3><p>| <dependencies><br /><dependency><br /><groupId>org.apache.flink</groupId><br /><artifactId>flink-java</artifactId><br /><version>1.12.0</version><br /></dependency></p><pre><code>&lt;dependency&gt;    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;    &lt;artifactId&gt;flink-streaming-java_2.12&lt;/artifactId&gt;    &lt;version&gt;1.12.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;    &lt;artifactId&gt;flink-clients_2.12&lt;/artifactId&gt;    &lt;version&gt;1.12.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;    &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;    &lt;version&gt;3.1.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;mysql&lt;/groupId&gt;    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;    &lt;version&gt;5.1.49&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;com.alibaba.ververica&lt;/groupId&gt;    &lt;artifactId&gt;flink-connector-mysql-cdc&lt;/artifactId&gt;    &lt;version&gt;1.1.1&lt;/version&gt;&lt;/dependency&gt;</code></pre><dependency>        <groupId>com.alibaba</groupId>        <artifactId>fastjson</artifactId>        <version>1.2.75</version>    </dependency></dependencies> || --- |<h3 id="212-编写代码"><a class="markdownIt-Anchor" href="#212-编写代码"></a> 2.1.2 编写代码</h3><p>| import com.alibaba.ververica.cdc.connectors.mysql.MySQLSource;import com.alibaba.ververica.cdc.debezium.DebeziumSourceFunction;import com.alibaba.ververica.cdc.debezium.StringDebeziumDeserializationSchema;import org.apache.flink.api.common.restartstrategy.RestartStrategies;import org.apache.flink.runtime.state.filesystem.FsStateBackend;import org.apache.flink.streaming.api.CheckpointingMode;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.environment.CheckpointConfig;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;<br />import java.util.Properties;<br />public class FlinkCDC {</p><pre><code>public static void main(String[] args) throws Exception &#123;    _//1.创建执行环境    _StreamExecutionEnvironment env = StreamExecutionEnvironment._getExecutionEnvironment_();    env.setParallelism(1);    _//2.Flink-CDC将读取binlog的位置信息以状态的方式保存在CK,如果想要做到断点续传,需要从Checkpoint或者Savepoint启动程序    //2.1 开启Checkpoint,每隔5秒钟做一次CK    _env.enableCheckpointing(5000L);    _//2.2 指定CK的一致性语义    _env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode._EXACTLY_ONCE_);    _//2.3 设置任务关闭的时候保留最后一次CK数据    _env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup._RETAIN_ON_CANCELLATION_);    _//2.4 指定从CK自动重启策略    _env.setRestartStrategy(RestartStrategies._fixedDelayRestart_(3, 2000L));    _//2.5 设置状态后端    _env.setStateBackend(new FsStateBackend(&quot;hdfs://hadoop102:8020/flinkCDC&quot;));    _//2.6 设置访问HDFS的用户名    _System._setProperty_(&quot;HADOOP_USER_NAME&quot;, &quot;atguigu&quot;);    _//3.创建Flink-MySQL-CDC的Source    _Properties properties = new Properties();    _//initial (default): Performs an initial snapshot on the monitored database tables upon first startup, and continue to read the latest binlog.    //latest-offset: Never to perform snapshot on the monitored database tables upon first startup, just read from the end of the binlog which means only have the changes since the connector was started.    //timestamp: Never to perform snapshot on the monitored database tables upon first startup, and directly read binlog from the specified timestamp. The consumer will traverse the binlog from the beginning and ignore change events whose timestamp is smaller than the specified timestamp.    //specific-offset: Never to perform snapshot on the monitored database tables upon first startup, and directly read binlog from the specified offset.    _properties.setProperty(&quot;scan.startup.mode&quot;, &quot;initial&quot;);    DebeziumSourceFunction&lt;String&gt; mysqlSource = MySQLSource.&lt;String&gt;_builder_()            .hostname(&quot;hadoop102&quot;)            .port(3306)            .username(&quot;root&quot;)            .password(&quot;000000&quot;)            .databaseList(&quot;gmall-flink-200821&quot;)          _ _ .tableList(&quot;gmall-flink-200821.z_user_info&quot;)  _       //可选配置项,如果不指定该参数,则会读取上一个配置下的所有表的数据_</code></pre><p>_//注意：指定的时候需要使用&quot;db.table&quot;的方式__ _.debeziumProperties(properties)<br />.deserializer(new StringDebeziumDeserializationSchema())<br />.build();</p><pre><code>    _//4.使用CDC Source从MySQL读取数据    _DataStreamSource&lt;String&gt; mysqlDS = env.addSource(mysqlSource);    _//5.打印数据    _mysqlDS.print();    _//6.执行任务    _env.execute();&#125;</code></pre><table><thead><tr><th>}</th></tr></thead><tbody></tbody></table><h3 id="213-案例测试"><a class="markdownIt-Anchor" href="#213-案例测试"></a> 2.1.3 案例测试</h3><p>1）打包并上传至 Linux<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954210377-f5cc09e0-d0a7-4ac7-8395-c2fa68ab2128.png#" alt="" /></p><p>2）开启 MySQL Binlog 并重启 MySQL</p><p>3）启动 Flink 集群</p><table><thead><tr><th>[atguigu@hadoop102 flink-standalone]$ bin/start-cluster.sh</th></tr></thead><tbody></tbody></table><p>4）启动 HDFS 集群</p><table><thead><tr><th>[atguigu@hadoop102 flink-standalone]$ <a href="http://start-dfs.sh">start-dfs.sh</a></th></tr></thead><tbody></tbody></table><p>5）启动程序</p><table><thead><tr><th>[atguigu@hadoop102 flink-standalone]$ bin/flink run -c com.atguigu.FlinkCDC flink-200821-1.0-SNAPSHOT-jar-with-dependencies.jar</th></tr></thead><tbody></tbody></table><p>6）在 MySQL 的 gmall-flink-200821.z_user_info 表中添加、修改或者删除数据<br />7）给当前的 Flink 程序创建 Savepoint</p><table><thead><tr><th>[atguigu@hadoop102 flink-standalone]$ bin/flink savepoint JobId hdfs://hadoop102:8020/flink/save</th></tr></thead><tbody></tbody></table><p>8）关闭程序以后从 Savepoint 重启程序</p><table><thead><tr><th>[atguigu@hadoop102 flink-standalone]$ bin/flink run -s hdfs://hadoop102:8020/flink/save/… -c com.atguigu.FlinkCDC flink-200821-1.0-SNAPSHOT-jar-with-dependencies.jar</th></tr></thead><tbody></tbody></table><h2 id="22-flinksql-方式的应用"><a class="markdownIt-Anchor" href="#22-flinksql-方式的应用"></a> 2.2 FlinkSQL 方式的应用</h2><h3 id="221-添加依赖"><a class="markdownIt-Anchor" href="#221-添加依赖"></a> 2.2.1 添加依赖</h3><p>| <dependency><br /><groupId>org.apache.flink</groupId><br /><artifactId>flink-table-planner-blink_2.12</artifactId><br /><version>1.12.0</version></p><table><thead><tr><th></dependency></th></tr></thead><tbody></tbody></table><h3 id="222-代码实现"><a class="markdownIt-Anchor" href="#222-代码实现"></a> 2.2.2 代码实现</h3><p>| import org.apache.flink.api.common.restartstrategy.RestartStrategies;import org.apache.flink.runtime.state.filesystem.FsStateBackend;import org.apache.flink.streaming.api.CheckpointingMode;import org.apache.flink.streaming.api.environment.CheckpointConfig;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;<br />public class FlinkSQL_CDC {</p><pre><code>public static void main(String[] args) throws Exception &#123;    _//1.创建执行环境    _StreamExecutionEnvironment env = StreamExecutionEnvironment._getExecutionEnvironment_();    env.setParallelism(1);    StreamTableEnvironment tableEnv = StreamTableEnvironment._create_(env);_        _//2.创建Flink-MySQL-CDC的Source    _tableEnv.executeSql(&quot;CREATE TABLE user_info (&quot; +            &quot;  id INT,&quot; +            &quot;  name STRING,&quot; +            &quot;  phone_num STRING&quot; +            &quot;) WITH (&quot; +            &quot;  'connector' = 'mysql-cdc',&quot; +            &quot;  'hostname' = 'hadoop102',&quot; +            &quot;  'port' = '3306',&quot; +            &quot;  'username' = 'root',&quot; +            &quot;  'password' = '000000',&quot; +            &quot;  'database-name' = 'gmall-flink-200821',&quot; +            &quot;  'table-name' = 'z_user_info'&quot; +            &quot;)&quot;);    tableEnv.executeSql(&quot;select * from user_info&quot;).print();    env.execute();&#125;</code></pre><table><thead><tr><th>}</th></tr></thead><tbody></tbody></table><h2 id="23-自定义反序列化器"><a class="markdownIt-Anchor" href="#23-自定义反序列化器"></a> 2.3 自定义反序列化器</h2><h3 id="231-代码实现"><a class="markdownIt-Anchor" href="#231-代码实现"></a> 2.3.1 代码实现</h3><p>| import com.alibaba.fastjson.JSONObject;import com.alibaba.ververica.cdc.connectors.mysql.MySQLSource;import com.alibaba.ververica.cdc.debezium.DebeziumDeserializationSchema;import com.alibaba.ververica.cdc.debezium.DebeziumSourceFunction;import io.debezium.data.Envelope;import org.apache.flink.api.common.restartstrategy.RestartStrategies;import org.apache.flink.api.common.typeinfo.TypeInformation;import org.apache.flink.runtime.state.filesystem.FsStateBackend;import org.apache.flink.streaming.api.CheckpointingMode;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.environment.CheckpointConfig;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.util.Collector;import org.apache.kafka.connect.data.Field;import org.apache.kafka.connect.data.Struct;import org.apache.kafka.connect.source.SourceRecord;<br />import java.util.Properties;<br />public class Flink_CDCWithCustomerSchema {</p><pre><code>public static void main(String[] args) throws Exception &#123;    _//1.创建执行环境    _StreamExecutionEnvironment env = StreamExecutionEnvironment._getExecutionEnvironment_();    env.setParallelism(1);    _//2.创建Flink-MySQL-CDC的Source    _Properties properties = new Properties();    _//initial (default): Performs an initial snapshot on the monitored database tables upon first startup, and continue to read the latest binlog.    //latest-offset: Never to perform snapshot on the monitored database tables upon first startup, just read from the end of the binlog which means only have the changes since the connector was started.    //timestamp: Never to perform snapshot on the monitored database tables upon first startup, and directly read binlog from the specified timestamp. The consumer will traverse the binlog from the beginning and ignore change events whose timestamp is smaller than the specified timestamp.    //specific-offset: Never to perform snapshot on the monitored database tables upon first startup, and directly read binlog from the specified offset.    _properties.setProperty(&quot;debezium.snapshot.mode&quot;, &quot;initial&quot;);    DebeziumSourceFunction&lt;String&gt; mysqlSource = MySQLSource.&lt;String&gt;_builder_()            .hostname(&quot;hadoop102&quot;)            .port(3306)            .username(&quot;root&quot;)            .password(&quot;000000&quot;)            .databaseList(&quot;gmall-flink-200821&quot;)            .tableList(&quot;gmall-flink-200821.z_user_info&quot;)         _//可选配置项,如果不指定该参数,则会读取上一个配置下的所有表的数据,注意：指定的时候需要使用&quot;db.table&quot;的方式            _.debeziumProperties(properties)            .deserializer(new DebeziumDeserializationSchema&lt;String&gt;() &#123;  _//自定义数据解析器                _@Override                public void deserialize(SourceRecord sourceRecord, Collector&lt;String&gt; collector) throws Exception &#123;                    _//获取主题信息,包含着数据库和表名  mysql_binlog_source.gmall-flink-200821.z_user_info                    _String topic = sourceRecord.topic();                    String[] arr = topic.split(&quot;\\.&quot;);                    String db = arr[1];                    String tableName = arr[2];                    _//获取操作类型 READ DELETE UPDATE CREATE                    _Envelope.Operation operation = Envelope._operationFor_(sourceRecord);                    _//获取值信息并转换为Struct类型                    _Struct value = (Struct) sourceRecord.value();                    _//获取变化后的数据                    _Struct after = value.getStruct(&quot;after&quot;);                    _//创建JSON对象用于存储数据信息                    _JSONObject data = new JSONObject();                    for (Field field : after.schema().fields()) &#123;                        Object o = after.get(field);                        data.put(field.name(), o);                    &#125;                    _//创建JSON对象用于封装最终返回值数据信息                    _JSONObject result = new JSONObject();                    result.put(&quot;operation&quot;, operation.toString().toLowerCase());                    result.put(&quot;data&quot;, data);                    result.put(&quot;database&quot;, db);                    result.put(&quot;table&quot;, tableName);                    _//发送数据至下游                    _collector.collect(result.toJSONString());                &#125;                @Override                public TypeInformation&lt;String&gt; getProducedType() &#123;                    return TypeInformation._of_(String.class);                &#125;            &#125;)            .build();    _//3.使用CDC Source从MySQL读取数据    _DataStreamSource&lt;String&gt; mysqlDS = env.addSource(mysqlSource);    _//4.打印数据    _mysqlDS.print();    _//5.执行任务    _env.execute();&#125;</code></pre><table><thead><tr><th>}</th></tr></thead><tbody></tbody></table>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="posts/0/"/>
      <url>posts/0/</url>
      
        <content type="html"><![CDATA[<p>尚硅谷大数据技术之 Hadoop（生产调优手册）<br />（作者：尚硅谷大数据研发部）</p><p>版本：V3.3</p><h1 id="第-1-章-hdfs核心参数"><a class="markdownIt-Anchor" href="#第-1-章-hdfs核心参数"></a> 第 1 章 HDFS—核心参数</h1><h2 id="11-namenode-内存生产配置"><a class="markdownIt-Anchor" href="#11-namenode-内存生产配置"></a> 1.1 NameNode 内存生产配置</h2><p>1）NameNode 内存计算<br />每个文件块大概占用 150byte，一台服务器 128G 内存为例，能存储多少文件块呢？<br />128 _ 1024 _ 1024 * 1024 / 150Byte ≈ 9.1 亿<br />G MB KB Byte<br />2）Hadoop2.x 系列，配置 NameNode 内存<br />NameNode 内存默认 2000m，如果服务器内存 4G，NameNode 内存可以配置 3g。在 <a href="http://hadoop-env.sh">hadoop-env.sh</a> 文件中配置如下。<br />HADOOP_NAMENODE_OPTS=-Xmx3072m<br />3）Hadoop3.x 系列，配置 NameNode 内存<br />（1）<a href="http://hadoop-env.sh">hadoop-env.sh</a> 中描述 Hadoop 的内存是动态分配的</p><h1 id="the-maximum-amount-of-heap-to-use-java-xmx-if-no-unit"><a class="markdownIt-Anchor" href="#the-maximum-amount-of-heap-to-use-java-xmx-if-no-unit"></a> The maximum amount of heap to use (Java -Xmx). If no unit</h1><h1 id="is-provided-it-will-be-converted-to-mb-daemons-will"><a class="markdownIt-Anchor" href="#is-provided-it-will-be-converted-to-mb-daemons-will"></a> is provided, it will be converted to MB. Daemons will</h1><h1 id="prefer-any-xmx-setting-in-their-respective-_opt-variable"><a class="markdownIt-Anchor" href="#prefer-any-xmx-setting-in-their-respective-_opt-variable"></a> prefer any Xmx setting in their respective _OPT variable.</h1><h1 id="there-is-no-default-the-jvm-will-autoscale-based-upon-machine"><a class="markdownIt-Anchor" href="#there-is-no-default-the-jvm-will-autoscale-based-upon-machine"></a> There is no default; the JVM will autoscale based upon machine</h1><h1 id="memory-size"><a class="markdownIt-Anchor" href="#memory-size"></a> memory size.</h1><h1 id="export-hadoop_heapsize_max"><a class="markdownIt-Anchor" href="#export-hadoop_heapsize_max"></a> export HADOOP_HEAPSIZE_MAX=</h1><h1 id="the-minimum-amount-of-heap-to-use-java-xms-if-no-unit"><a class="markdownIt-Anchor" href="#the-minimum-amount-of-heap-to-use-java-xms-if-no-unit"></a> The minimum amount of heap to use (Java -Xms). If no unit</h1><h1 id="is-provided-it-will-be-converted-to-mb-daemons-will-2"><a class="markdownIt-Anchor" href="#is-provided-it-will-be-converted-to-mb-daemons-will-2"></a> is provided, it will be converted to MB. Daemons will</h1><h1 id="prefer-any-xms-setting-in-their-respective-_opt-variable"><a class="markdownIt-Anchor" href="#prefer-any-xms-setting-in-their-respective-_opt-variable"></a> prefer any Xms setting in their respective _OPT variable.</h1><h1 id="there-is-no-default-the-jvm-will-autoscale-based-upon-machine-2"><a class="markdownIt-Anchor" href="#there-is-no-default-the-jvm-will-autoscale-based-upon-machine-2"></a> There is no default; the JVM will autoscale based upon machine</h1><h1 id="memory-size-2"><a class="markdownIt-Anchor" href="#memory-size-2"></a> memory size.</h1><h1 id="export-hadoop_heapsize_min"><a class="markdownIt-Anchor" href="#export-hadoop_heapsize_min"></a> export HADOOP_HEAPSIZE_MIN=</h1><p>HADOOP_NAMENODE_OPTS=-Xmx102400m<br />（2）查看 NameNode 占用内存<br />[atguigu@hadoop102 ~]$ jps<br />3088 NodeManager<br />2611 NameNode<br />3271 JobHistoryServer<br />2744 DataNode<br />3579 Jps<br />[atguigu@hadoop102 ~]$ jmap -heap 2611<br />Heap Configuration:<br />MaxHeapSize = 1031798784 (984.0MB)<br />（3）查看 DataNode 占用内存<br />[atguigu@hadoop102 ~]$ jmap -heap 2744<br />Heap Configuration:<br />MaxHeapSize = 1031798784 (984.0MB)<br />查看发现 hadoop102 上的 NameNode 和 DataNode 占用内存都是自动分配的，且相等。不是很合理。<br />经验参考：<br /><a href="https://docs.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_hardware_requirements.html#concept_fzz_dq4_gbb">https://docs.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_hardware_requirements.html#concept_fzz_dq4_gbb</a><br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954234898-5fca5c61-4a06-4914-b3c9-8d49ea0b1c2c.png#" alt="" /> <img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954235468-9d0a66eb-3e1c-473d-9b4e-02628691ddfc.png#" alt="" /><br />具体修改：<a href="http://hadoop-env.sh">hadoop-env.sh</a><br />export HDFS_NAMENODE_OPTS=&quot;-Dhadoop.security.logger=INFO,RFAS -Xmx1024m&quot;</p><p>export HDFS_DATANODE_OPTS=&quot;-Dhadoop.security.logger=ERROR,RFAS -Xmx1024m&quot;</p><h2 id="12-namenode-心跳并发配置"><a class="markdownIt-Anchor" href="#12-namenode-心跳并发配置"></a> 1.2 NameNode 心跳并发配置</h2><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954235905-df8917e2-1181-4cef-84d5-38cfecb368b0.png#" alt="" /><br />1）hdfs-site.xml<br />The number of Namenode RPC server threads that listen to requests from clients. If dfs.namenode.servicerpc-address is not configured then Namenode RPC server threads listen to requests from all nodes.<br />NameNode 有一个工作线程池，用来处理不同 DataNode 的并发心跳以及客户端并发的元数据操作。<br />对于大集群或者有大量客户端的集群来说，通常需要增大该参数。默认值是 10。<br /><property><br /><name>dfs.namenode.handler.count</name><br /><value>21</value><br /></property><br />企业经验：dfs.namenode.handler.count=，比如集群规模（DataNode 台数）为 3 台时，此参数设置为 21。可通过简单的 python 代码计算该值，代码如下。<br />[atguigu@hadoop102 ~]$ sudo yum install -y python<br />[atguigu@hadoop102 ~]$ python<br />Python 2.7.5 (default, Apr 11 2018, 07:36:10)<br />[GCC 4.8.5 20150623 (Red Hat 4.8.5-28)] on linux2<br />Type “help”, “copyright”, “credits” or “license” for more information.</p><blockquote><blockquote><blockquote><p>import math<br />print int(20*math.log(3))<br />21<br />quit()</p></blockquote></blockquote></blockquote><h2 id="13-开启回收站配置"><a class="markdownIt-Anchor" href="#13-开启回收站配置"></a> 1.3 开启回收站配置</h2><p>开启回收站功能，可以将删除的文件在不超时的情况下，恢复原数据，起到防止误删除、备份等作用。<br /><strong>1）回收站工作机制</strong><br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954236208-b169fb98-cfe4-4b75-baf2-3cd1e3b3eb47.png#" alt="" /><br />2）开启回收站功能参数说明<br />（1）默认值 fs.trash.interval = 0，0 表示禁用回收站；其他值表示设置文件的存活时间。<br />（2）默认值 fs.trash.checkpoint.interval = 0，检查回收站的间隔时间。如果该值为 0，则该值设置和 fs.trash.interval 的参数值相等。<br />（3）要求 fs.trash.checkpoint.interval &lt;= fs.trash.interval。<br />3）启用回收站<br />修改 core-site.xml，配置垃圾回收时间为 1 分钟。<br /><property><br /><name>fs.trash.interval</name><br /><value>1</value><br /></property><br />4）查看回收站<br />回收站目录在 HDFS 集群中的路径：/user/atguigu/.Trash/….<br />5）注意：通过网页上直接删除的文件也不会走回收站。<br />6）通过程序删除的文件不会经过回收站，需要调用 moveToTrash()才进入回收站<br />Trash trash = New Trash(conf);<br />trash.moveToTrash(path);<br />7）只有在命令行利用 hadoop fs -rm 命令删除的文件才会走回收站。<br />[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -rm -r /user/atguigu/input<br />2021-07-14 16:13:42,643 INFO fs.TrashPolicyDefault: Moved: ‘hdfs://hadoop102:9820/user/atguigu/input’ to trash at: hdfs://hadoop102:9820/user/atguigu/.Trash/Current/user/atguigu/input<br />8）恢复回收站数据<br />[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -mv<br />/user/atguigu/.Trash/Current/user/atguigu/input /user/atguigu/input</p><h1 id="第-2-章-hdfs集群压测"><a class="markdownIt-Anchor" href="#第-2-章-hdfs集群压测"></a> 第 2 章 HDFS—集群压测</h1><p>在企业中非常关心每天从 Java 后台拉取过来的数据，需要多久能上传到集群？消费者关心多久能从 HDFS 上拉取需要的数据？<br />为了搞清楚 HDFS 的读写性能，生产环境上非常需要对集群进行压测。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954236562-192d427b-5997-407d-98cd-d49d1145bce3.png#" alt="" /><br />HDFS 的读写性能主要受<strong>网络和磁盘</strong>影响比较大。为了方便测试，将 hadoop102、hadoop103、hadoop104 虚拟机网络都设置为 100mbps。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954236993-49ee92ed-e31e-4239-b187-a1b86e749381.png#" alt="" /><br />100Mbps 单位是 bit；10M/s 单位是 byte ; 1byte=8bit，100Mbps/8=12.5M/s。<br />测试网速：来到 hadoop102 的/opt/module 目录，创建一个<br />[atguigu@hadoop102 software]$ python -m SimpleHTTPServer</p><h2 id="21-测试-hdfs-写性能"><a class="markdownIt-Anchor" href="#21-测试-hdfs-写性能"></a> 2.1 测试 HDFS 写性能</h2><p>0）写测试底层原理<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954237554-288d328b-c350-4cb5-8ff3-7a264cedec17.png#" alt="" /><br />1）测试内容：向 HDFS 集群写 10 个 128M 的文件<br />[atguigu@hadoop102 mapreduce]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 128MB</p><p>2021-02-09 10:43:16,853 INFO fs.TestDFSIO: ----- TestDFSIO ----- : write<br />2021-02-09 10:43:16,854 INFO fs.TestDFSIO: Date &amp; time: Tue Feb 09 10:43:16 CST 2021<br />2021-02-09 10:43:16,854 INFO fs.TestDFSIO: Number of files: 10<br />2021-02-09 10:43:16,854 INFO fs.TestDFSIO: Total MBytes processed: 1280<br />2021-02-09 10:43:16,854 INFO fs.TestDFSIO: Throughput mb/sec: 1.61<br />2021-02-09 10:43:16,854 INFO fs.TestDFSIO: Average IO rate mb/sec: 1.9<br />2021-02-09 10:43:16,854 INFO fs.TestDFSIO: IO rate std deviation: 0.76<br />2021-02-09 10:43:16,854 INFO fs.TestDFSIO: Test exec time sec: 133.05<br />2021-02-09 10:43:16,854 INFO fs.TestDFSIO:<br />注意：nrFiles n 为生成 mapTask 的数量，生产环境一般可通过 hadoop103:8088 查看 CPU 核数，设置为（CPU 核数 - 1）</p><ul><li>Number of files：生成 mapTask 数量，一般是集群中（CPU 核数-1），我们测试虚拟机就按照实际的物理内存-1 分配即可</li><li>Total MBytes processed：单个 map 处理的文件大小</li><li>Throughput mb/sec:单个 mapTak 的吞吐量</li></ul><p>计算方式：处理的总文件大小/每一个 mapTask 写数据的时间累加<br />集群整体吞吐量：生成 mapTask 数量*单个 mapTak 的吞吐量</p><ul><li>Average IO rate mb/sec::平均 mapTak 的吞吐量</li></ul><p>计算方式：每个 mapTask 处理文件大小/每一个 mapTask 写数据的时间<br />全部相加除以 task 数量</p><ul><li>IO rate std deviation:方差、反映各个 mapTask 处理的差值，越小越均衡</li></ul><p>2）注意：如果测试过程中，出现异常<br />（1）可以在 yarn-site.xml 中设置虚拟内存检测为 false</p><!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true --><property>     <name>yarn.nodemanager.vmem-check-enabled</name>     <value>false</value></property>（2）分发配置并重启Yarn集群3）测试结果分析（1）由于副本1就在本地，所以该副本不参与测试![](https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954237975-73142f99-d5e2-4135-94ef-b1669db09524.png#)一共参与测试的文件：10个文件 * 2个副本 = 20个压测后的速度：1.61实测速度：1.61M/s * 20个文件 ≈ 32M/s三台服务器的带宽：12.5 + 12.5 + 12.5 ≈ 30m/s所有网络资源都已经用满。**如果实测速度远远小于网络，并且实测速度不能满足工作需求，可以考虑采用固态硬盘或者增加磁盘个数。**（2）如果客户端不在集群节点，那就三个副本都参与计算![](https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954238377-76fc9f51-b06b-47a7-8ce8-2779fd21d183.png#)<h2 id="22-测试-hdfs-读性能"><a class="markdownIt-Anchor" href="#22-测试-hdfs-读性能"></a> 2.2 测试 HDFS 读性能</h2><p>1）测试内容：读取 HDFS 集群 10 个 128M 的文件<br />[atguigu@hadoop102 mapreduce]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 128MB</p><p>2021-02-09 11:34:15,847 INFO fs.TestDFSIO: ----- TestDFSIO ----- : read<br />2021-02-09 11:34:15,847 INFO fs.TestDFSIO: Date &amp; time: Tue Feb 09 11:34:15 CST 2021<br />2021-02-09 11:34:15,847 INFO fs.TestDFSIO: Number of files: 10<br />2021-02-09 11:34:15,847 INFO fs.TestDFSIO: Total MBytes processed: 1280<br />2021-02-09 11:34:15,848 INFO fs.TestDFSIO: Throughput mb/sec: 200.28<br />2021-02-09 11:34:15,848 INFO fs.TestDFSIO: Average IO rate mb/sec: 266.74<br />2021-02-09 11:34:15,848 INFO fs.TestDFSIO: IO rate std deviation: 143.12<br />2021-02-09 11:34:15,848 INFO fs.TestDFSIO: Test exec time sec: 20.83<br />2）删除测试生成数据<br />[atguigu@hadoop102 mapreduce]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -clean<br />3）测试结果分析：为什么读取文件速度大于网络带宽？由于目前只有三台服务器，且有三个副本，数据读取就近原则，相当于都是读取的本地磁盘数据，没有走网络。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954238648-6e978932-14e7-4e86-bcb8-aea1a36bdc17.png#" alt="" /></p><h1 id="第-3-章-hdfs多目录"><a class="markdownIt-Anchor" href="#第-3-章-hdfs多目录"></a> 第 3 章 HDFS—多目录</h1><h2 id="31-namenode-多目录配置"><a class="markdownIt-Anchor" href="#31-namenode-多目录配置"></a> 3.1 NameNode 多目录配置</h2><p>1）NameNode 的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954239014-bbf17d0c-f327-400f-877a-d949b027821c.png#" alt="" /><br />2）具体配置如下<br />（1）在 hdfs-site.xml 文件中添加如下内容<br /><property><br /><name>dfs.namenode.name.dir</name><br /><value>file://<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi>h</mi><mi>a</mi><mi>d</mi><mi>o</mi><mi>o</mi><mi>p</mi><mi mathvariant="normal">.</mi><mi>t</mi><mi>m</mi><mi>p</mi><mi mathvariant="normal">.</mi><mi>d</mi><mi>i</mi><mi>r</mi></mrow><mi mathvariant="normal">/</mi><mi>d</mi><mi>f</mi><mi>s</mi><mi mathvariant="normal">/</mi><mi>n</mi><mi>a</mi><mi>m</mi><mi>e</mi><mn>1</mn><mo separator="true">,</mo><mi>f</mi><mi>i</mi><mi>l</mi><mi>e</mi><mo>:</mo><mi mathvariant="normal">/</mi><mi mathvariant="normal">/</mi></mrow><annotation encoding="application/x-tex">{hadoop.tmp.dir}/dfs/name1,file://</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="mord mathdefault">a</span><span class="mord mathdefault">d</span><span class="mord mathdefault">o</span><span class="mord mathdefault">o</span><span class="mord mathdefault">p</span><span class="mord">.</span><span class="mord mathdefault">t</span><span class="mord mathdefault">m</span><span class="mord mathdefault">p</span><span class="mord">.</span><span class="mord mathdefault">d</span><span class="mord mathdefault">i</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span></span><span class="mord">/</span><span class="mord mathdefault">d</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord mathdefault">s</span><span class="mord">/</span><span class="mord mathdefault">n</span><span class="mord mathdefault">a</span><span class="mord mathdefault">m</span><span class="mord mathdefault">e</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord mathdefault">i</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">e</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">/</span><span class="mord">/</span></span></span></span>{hadoop.tmp.dir}/dfs/name2</value><br /></property><br />注意：因为每台服务器节点的磁盘情况不同，所以这个配置配完之后，可以选择不分发<br />（2）停止集群，删除三台节点的 data 和 logs 中所有数据。<br />[atguigu@hadoop102 hadoop-3.1.3]$ rm -rf data/ logs/<br />[atguigu@hadoop103 hadoop-3.1.3]$ rm -rf data/ logs/<br />[atguigu@hadoop104 hadoop-3.1.3]$ rm -rf data/ logs/<br />（3）格式化集群并启动。<br />[atguigu@hadoop102 hadoop-3.1.3]$ bin/hdfs namenode -format<br />[atguigu@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh<br /><strong>3）查看结果</strong><br />[atguigu@hadoop102 dfs]$ ll<br />总用量 12<br />drwx------. 3 atguigu atguigu 4096 12 月 11 08:03 data<br />drwxrwxr-x. 3 atguigu atguigu 4096 12 月 11 08:03 name1<br />drwxrwxr-x. 3 atguigu atguigu 4096 12 月 11 08:03 name2<br /><strong>检查 name1 和 name2 里面的内容，发现一模一样。</strong></p><h2 id="32-datanode-多目录配置"><a class="markdownIt-Anchor" href="#32-datanode-多目录配置"></a> 3.2 DataNode 多目录配置</h2><p>1）DataNode 可以配置成多个目录，每个目录存储的数据不一样（数据不是副本）<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954239666-97785873-afcf-447b-b153-e3f631ca136f.png#" alt="" /><br />2）具体配置如下<br />在 hdfs-site.xml 文件中添加如下内容<br /><property><br /><name>dfs.datanode.data.dir</name><br /><value>file://<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi>h</mi><mi>a</mi><mi>d</mi><mi>o</mi><mi>o</mi><mi>p</mi><mi mathvariant="normal">.</mi><mi>t</mi><mi>m</mi><mi>p</mi><mi mathvariant="normal">.</mi><mi>d</mi><mi>i</mi><mi>r</mi></mrow><mi mathvariant="normal">/</mi><mi>d</mi><mi>f</mi><mi>s</mi><mi mathvariant="normal">/</mi><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mn>1</mn><mo separator="true">,</mo><mi>f</mi><mi>i</mi><mi>l</mi><mi>e</mi><mo>:</mo><mi mathvariant="normal">/</mi><mi mathvariant="normal">/</mi></mrow><annotation encoding="application/x-tex">{hadoop.tmp.dir}/dfs/data1,file://</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="mord mathdefault">a</span><span class="mord mathdefault">d</span><span class="mord mathdefault">o</span><span class="mord mathdefault">o</span><span class="mord mathdefault">p</span><span class="mord">.</span><span class="mord mathdefault">t</span><span class="mord mathdefault">m</span><span class="mord mathdefault">p</span><span class="mord">.</span><span class="mord mathdefault">d</span><span class="mord mathdefault">i</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span></span><span class="mord">/</span><span class="mord mathdefault">d</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord mathdefault">s</span><span class="mord">/</span><span class="mord mathdefault">d</span><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord mathdefault">i</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">e</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">/</span><span class="mord">/</span></span></span></span>{hadoop.tmp.dir}/dfs/data2</value><br /></property><br /><strong>3）查看结果</strong><br />[atguigu@hadoop102 dfs]$ ll<br />总用量 12<br />drwx------. 3 atguigu atguigu 4096 4 月 4 14:22 data1<br />drwx------. 3 atguigu atguigu 4096 4 月 4 14:22 data2<br />drwxrwxr-x. 3 atguigu atguigu 4096 12 月 11 08:03 name1<br />drwxrwxr-x. 3 atguigu atguigu 4096 12 月 11 08:03 name2<br /><strong>4）向集群上传一个文件，再次观察两个文件夹里面的内容发现不一致（一个有数一个没有）</strong><br />[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -put wcinput/word.txt /</p><h2 id="33-集群数据均衡之磁盘间数据均衡"><a class="markdownIt-Anchor" href="#33-集群数据均衡之磁盘间数据均衡"></a> 3.3 集群数据均衡之磁盘间数据均衡</h2><p>生产环境，由于硬盘空间不足，往往需要增加一块硬盘。刚加载的硬盘没有数据时，可以执行磁盘数据均衡命令。（Hadoop3.x 新特性）<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954240199-ce0d63df-dddc-4ec3-8518-9f4a47a0ef5e.png#" alt="" /><br />（1）生成均衡计划（<strong>我们只有一块磁盘，不会生成计划</strong>）<br />hdfs diskbalancer -plan hadoop103<br />（2）执行均衡计划<br />hdfs diskbalancer -execute hadoop103.plan.json<br />（3）查看当前均衡任务的执行情况<br />hdfs diskbalancer -query hadoop103<br />（4）取消均衡任务<br />hdfs diskbalancer -cancel hadoop103.plan.json</p><h1 id="第-4-章-hdfs集群扩容及缩容"><a class="markdownIt-Anchor" href="#第-4-章-hdfs集群扩容及缩容"></a> 第 4 章 HDFS—集群扩容及缩容</h1><h2 id="41-添加白名单"><a class="markdownIt-Anchor" href="#41-添加白名单"></a> 4.1 添加白名单</h2><p>白名单：表示在白名单的主机 IP 地址可以，用来存储数据。<br />企业中：配置白名单，可以尽量防止黑客恶意访问攻击。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954240799-b1c76ebb-eafe-4644-9362-0548586a7d38.png#" alt="" /><br />配置白名单步骤如下：<br />1）在 NameNode 节点的/opt/module/hadoop-3.1.3/etc/hadoop 目录下分别创建 whitelist 和 blacklist 文件<br />（1）创建白名单<br />[atguigu@hadoop102 hadoop]$ vim whitelist<br />在 whitelist 中添加如下主机名称，假如集群正常工作的节点为 102 103<br />hadoop102<br />hadoop103<br />（2）创建黑名单<br />[atguigu@hadoop102 hadoop]$ touch blacklist<br />保持空的就可以<br />2）在 hdfs-site.xml 配置文件中增加 dfs.hosts 配置参数</p><!-- 白名单 --><property>     <name>dfs.hosts</name>     <value>/opt/module/hadoop-3.1.3/etc/hadoop/whitelist</value></property><!-- 黑名单 --><property>     <name>dfs.hosts.exclude</name>     <value>/opt/module/hadoop-3.1.3/etc/hadoop/blacklist</value></property>3）分发配置文件whitelist，hdfs-site.xml[atguigu@hadoop104 hadoop]$ xsync hdfs-site.xml whitelist4）第一次添加白名单必须重启集群，不是第一次，只需要刷新NameNode节点即可[atguigu@hadoop102 hadoop-3.1.3]$ myhadoop.sh stop[atguigu@hadoop102 hadoop-3.1.3]$ myhadoop.sh start5）在web浏览器上查看DN，http://hadoop102:9870/dfshealth.html#tab-datanode![](https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954241329-3344609b-8f2e-455a-94ea-14a798930a12.png#)6）在hadoop104上执行上传数据数据失败[atguigu@hadoop104 hadoop-3.1.3]$ hadoop fs -put NOTICE.txt /7）二次修改白名单，增加hadoop104[atguigu@hadoop102 hadoop]$ vim whitelist修改为如下内容hadoop102hadoop103hadoop1048）刷新NameNode[atguigu@hadoop102 hadoop-3.1.3]$ hdfs dfsadmin -refreshNodesRefresh nodes successful9）在web浏览器上查看DN，http://hadoop102:9870/dfshealth.html#tab-datanode![](https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954241865-62241528-28ee-4b77-a7bb-3ac8319206b5.png#)<h2 id="42-服役新服务器"><a class="markdownIt-Anchor" href="#42-服役新服务器"></a> 4.2 服役新服务器</h2><p>1）需求<br />随着公司业务的增长，数据量越来越大，原有的数据节点的容量已经不能满足存储数据的需求，需要在原有集群基础上动态添加新的数据节点。<br />2）环境准备<br />（1）在 hadoop100 主机上再克隆一台 hadoop105 主机<br />（2）修改 IP 地址和主机名称<br />[root@hadoop105 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33<br />[root@hadoop105 ~]# vim /etc/hostname<br />（3）拷贝 hadoop102 的/opt/module 目录和/etc/profile.d/my_env.sh 到 hadoop105<br />[atguigu@hadoop102 opt]$ scp -r module/* atguigu@hadoop105:/opt/module/</p><p>[atguigu@hadoop102 opt]$ sudo scp /etc/profile.d/my_env.sh root@hadoop105:/etc/profile.d/my_env.sh</p><p>[atguigu@hadoop105 hadoop-3.1.3]$ source /etc/profile<br />（4）删除 hadoop105 上 Hadoop 的历史数据，data 和 log 数据<br />[atguigu@hadoop105 hadoop-3.1.3]$ rm -rf data/ logs/<br />（5）配置 hadoop102 和 hadoop103 到 hadoop105 的 ssh 无密登录<br />[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop105</p><p>[atguigu@hadoop103 .ssh]$ ssh-copy-id hadoop105<br />3）服役新节点具体步骤<br />（1）直接启动 DataNode，即可关联到集群<br />[atguigu@hadoop105 hadoop-3.1.3]$ hdfs --daemon start datanode<br />[atguigu@hadoop105 hadoop-3.1.3]$ yarn --daemon start nodemanager<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954242404-3a446be1-7585-4739-b32c-81b47e5cfd2a.png#" alt="" /><br />4）在白名单中增加新服役的服务器<br />（1）在白名单 whitelist 中增加 hadoop104、hadoop105，并重启集群<br />[atguigu@hadoop102 hadoop]$ vim whitelist<br />修改为如下内容<br />hadoop102<br />hadoop103<br />hadoop104<br />hadoop105<br />（2）分发<br />[atguigu@hadoop102 hadoop]$ xsync whitelist<br />（3）刷新 NameNode<br />[atguigu@hadoop102 hadoop-3.1.3]$ hdfs dfsadmin -refreshNodes<br />Refresh nodes successful<br />5）在 hadoop105 上上传文件<br />[atguigu@hadoop105 hadoop-3.1.3]$ hadoop fs -put /opt/module/hadoop-3.1.3/LICENSE.txt /<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954242891-da64c840-764c-4fd9-aa64-4ee8db48334c.png#" alt="" /><br />思考：如果数据不均衡（hadoop105 数据少，其他节点数据多），怎么处理？</p><h2 id="43-服务器间数据均衡"><a class="markdownIt-Anchor" href="#43-服务器间数据均衡"></a> 4.3 服务器间数据均衡</h2><p><strong>1）企业经验：</strong><br />在企业开发中，如果经常在 hadoop102 和 hadoop104 上提交任务，且副本数为 2，由于数据本地性原则，就会导致 hadoop102 和 hadoop104 数据过多，hadoop103 存储的数据量小。<br />另一种情况，就是新服役的服务器数据量比较少，需要执行集群均衡命令。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954243351-f10cbaf8-016b-4ee6-a40a-89128f41bb15.png#" alt="" /><br /><strong>2）开启数据均衡命令：</strong><br />[atguigu@hadoop105 hadoop-3.1.3]$ sbin/start-balancer.sh -threshold 10<br />对于参数 10，代表的是集群中各个节点的磁盘空间利用率相差不超过 10%，可根据实际情况进行调整。<br /><strong>3）停止数据均衡命令：</strong><br />[atguigu@hadoop105 hadoop-3.1.3]$ sbin/stop-balancer.sh<br />注意：由于 HDFS 需要启动单独的 Rebalance Server 来执行 Rebalance 操作，<a href="http://xn--NameNodestart-balancer-sy68a2bx05ag98dux2afg9ay6af924fm2hzl3e.sh">所以尽量不要在 NameNode 上执行 start-balancer.sh</a>，而是找一台比较空闲的机器。</p><h2 id="44-黑名单退役服务器"><a class="markdownIt-Anchor" href="#44-黑名单退役服务器"></a> 4.4 黑名单退役服务器</h2><p>黑名单：表示在黑名单的主机 IP 地址不可以，用来存储数据。<br />企业中：配置黑名单，用来退役服务器。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954243887-08e81d45-de38-4d72-86ba-94b05d2037b4.png#" alt="" /><br />黑名单配置步骤如下：<br />1）编辑/opt/module/hadoop-3.1.3/etc/hadoop 目录下的 blacklist 文件<br />[atguigu@hadoop102 hadoop] vim blacklist<br />添加如下主机名称（要退役的节点）<br />hadoop105<br />注意：如果白名单中没有配置，需要在 hdfs-site.xml 配置文件中增加 dfs.hosts 配置参数</p><!-- 黑名单 --><property>     <name>dfs.hosts.exclude</name>     <value>/opt/module/hadoop-3.1.3/etc/hadoop/blacklist</value></property>2）分发配置文件blacklist，hdfs-site.xml[atguigu@hadoop104 hadoop]$ xsync hdfs-site.xml blacklist3）第一次添加黑名单必须重启集群，不是第一次，只需要刷新NameNode节点即可[atguigu@hadoop102 hadoop-3.1.3]$ hdfs dfsadmin -refreshNodesRefresh nodes successful4）检查Web浏览器，退役节点的状态为decommission in progress（退役中），说明数据节点正在复制块到其他节点![](https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954244354-f9455a8c-bbb5-4b8d-8e93-9db3346372dc.png#)![](https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954244615-b822d4d1-80e4-475a-8144-fd5bb0f98a29.png#)5）等待退役节点状态为decommissioned（所有块已经复制完成），停止该节点及节点资源管理器。注意：如果副本数是3，服役的节点小于等于3，是不能退役成功的，需要修改副本数后才能退役![](https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954244888-012446c6-0db1-446b-8253-b26b99e80d27.png#)[atguigu@hadoop105 hadoop-3.1.3]$ hdfs --daemon stop datanodestopping datanode[atguigu@hadoop105 hadoop-3.1.3]$ yarn --daemon stop nodemanagerstopping nodemanager6）如果数据不均衡，可以用命令实现集群的再平衡[atguigu@hadoop102 hadoop-3.1.3]$ sbin/start-balancer.sh -threshold 10<h1 id="第-5-章-hdfs存储优化"><a class="markdownIt-Anchor" href="#第-5-章-hdfs存储优化"></a> 第 5 章 HDFS—存储优化</h1><p>注：演示纠删码和异构存储需要一共 5 台虚拟机。尽量拿另外一套集群。提前准备 5 台服务器的集群。</p><h2 id="51-纠删码"><a class="markdownIt-Anchor" href="#51-纠删码"></a> 5.1 纠删码</h2><h3 id="511-纠删码原理"><a class="markdownIt-Anchor" href="#511-纠删码原理"></a> 5.1.1 纠删码原理</h3><p>HDFS 默认情况下，一个文件有 3 个副本，这样提高了数据的可靠性，但也带来了 2 倍的冗余开销。Hadoop3.x 引入了纠删码，采用计算的方式，可以节省约 50％左右的存储空间。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954245440-01af38fe-b478-4593-9710-14c2a4390c54.png#" alt="" /><br /><strong>1）纠删码操作相关的命令</strong><br />[atguigu@hadoop102 hadoop-3.1.3]$ hdfs ec<br />Usage: bin/hdfs ec [COMMAND]<br />[-listPolicies]<br />[-addPolicies -policyFile <file>]<br />[-getPolicy -path <path>]<br />[-removePolicy -policy <policy>]<br />[-setPolicy -path <path> [-policy <policy>] [-replicate]]<br />[-unsetPolicy -path <path>]<br />[-listCodecs]<br />[-enablePolicy -policy <policy>]<br />[-disablePolicy -policy <policy>]<br />[-help <command-name>].<br /><strong>2）查看当前支持的纠删码策略</strong><br />[atguigu@hadoop102 hadoop-3.1.3] hdfs ec -listPolicies</p><p>Erasure Coding Policies:<br />ErasureCodingPolicy=[Name=RS-10-4-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=10, numParityUnits=4]], CellSize=1048576, Id=5], State=DISABLED</p><p>ErasureCodingPolicy=[Name=RS-3-2-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=3, numParityUnits=2]], CellSize=1048576, Id=2], State=DISABLED</p><p>ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1], State=<strong>ENABLED</strong><br />ErasureCodingPolicy=[Name=RS-LEGACY-6-3-1024k, Schema=[ECSchema=[Codec=rs-legacy, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=3], State=DISABLED</p><p>ErasureCodingPolicy=[Name=XOR-2-1-1024k, Schema=[ECSchema=[Codec=xor, numDataUnits=2, numParityUnits=1]], CellSize=1048576, Id=4], State=DISABLED<br /><strong>3）纠删码策略解释:</strong><br />RS-3-2-1024k：使用 RS 编码，每 3 个数据单元，生成 2 个校验单元，共 5 个单元，也就是说：这 5 个单元中，只要有任意的 3 个单元存在（不管是数据单元还是校验单元，只要总数=3），就可以得到原始数据。每个单元的大小是 1024k=1024<em>1024=1048576。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954246076-d451a4b5-8035-42a9-9b9a-1bc91d703359.png#" alt="" /><br />RS-10-4-1024k：使用 RS 编码，每 10 个数据单元（cell），生成 4 个校验单元，共 14 个单元，也就是说：这 14 个单元中，只要有任意的 10 个单元存在（不管是数据单元还是校验单元，只要总数=10），就可以得到原始数据。每个单元的大小是 1024k=1024</em>1024=1048576。<br />RS-6-3-1024k：使用 RS 编码，每 6 个数据单元，生成 3 个校验单元，共 9 个单元，也就是说：这 9 个单元中，只要有任意的 6 个单元存在（不管是数据单元还是校验单元，只要总数=6），就可以得到原始数据。每个单元的大小是 1024k=1024<em>1024=1048576。<br />RS-LEGACY-6-3-1024k：策略和上面的 RS-6-3-1024k 一样，只是编码的算法用的是 rs-legacy。<br />XOR-2-1-1024k：使用 XOR 编码（速度比 RS 编码快），每 2 个数据单元，生成 1 个校验单元，共 3 个单元，也就是说：这 3 个单元中，只要有任意的 2 个单元存在（不管是数据单元还是校验单元，只要总数= 2），就可以得到原始数据。每个单元的大小是 1024k=1024</em>1024=1048576。</p><h3 id="512-纠删码案例实操"><a class="markdownIt-Anchor" href="#512-纠删码案例实操"></a> 5.1.2 纠删码案例实操</h3><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954246533-e94506cc-fcbb-4576-961c-a1be14d20495.png#" alt="" /><br />纠删码策略是给具体一个路径设置。所有往此路径下存储的文件，都会执行此策略。<br />默认只开启对 RS-6-3-1024k 策略的支持，如要使用别的策略需要提前启用。<br />**1）需求：**将/input 目录设置为 RS-3-2-1024k 策略<br /><strong>2）具体步骤</strong><br />（1）开启对 RS-3-2-1024k 策略的支持<br />[atguigu@hadoop102 hadoop-3.1.3]$ hdfs ec -enablePolicy -policy RS-3-2-1024k<br />Erasure coding policy RS-3-2-1024k is enabled<br />（2）在 HDFS 创建目录，并设置 RS-3-2-1024k 策略<br />[atguigu@hadoop102 hadoop-3.1.3]$ hdfs dfs -mkdir /input</p><p>[atguigu@hadoop202 hadoop-3.1.3]$ hdfs ec -setPolicy -path /input -policy RS-3-2-1024k<br />（3）上传文件，并查看文件编码后的存储情况<br />[atguigu@hadoop102 hadoop-3.1.3]$ hdfs dfs -put web.log /input<br />注：你所上传的文件需要大于 2M 才能看出效果。（低于 2M，只有一个数据单元和两个校验单元）<br />（4）查看存储路径的数据单元和校验单元，并作破坏实验</p><h2 id="52-异构存储冷热数据分离"><a class="markdownIt-Anchor" href="#52-异构存储冷热数据分离"></a> 5.2 异构存储（冷热数据分离）</h2><p>异构存储主要解决，不同的数据，存储在不同类型的硬盘中，达到最佳性能的问题。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954247048-0bf9db3b-9e76-4d42-935c-001b74f98af1.png#" alt="" /></p><h3 id="521-异构存储-shell-操作"><a class="markdownIt-Anchor" href="#521-异构存储-shell-操作"></a> 5.2.1 异构存储 Shell 操作</h3><p>（1）查看当前有哪些存储策略可以用<br />[atguigu@hadoop102 hadoop-3.1.3]$ hdfs storagepolicies -listPolicies<br />（2）为指定路径（数据存储目录）设置指定的存储策略<br />hdfs storagepolicies -setStoragePolicy -path xxx -policy xxx<br />（3）获取指定路径（数据存储目录或文件）的存储策略<br />hdfs storagepolicies -getStoragePolicy -path xxx<br />（4）取消存储策略；执行改命令之后该目录或者文件，以其上级的目录为准，如果是根目录，那么就是 HOT<br />hdfs storagepolicies -unsetStoragePolicy -path xxx<br />（5）查看文件块的分布<br />bin/hdfs fsck xxx -files -blocks -locations<br />（6）查看集群节点<br />hadoop dfsadmin -report</p><h3 id="522-测试环境准备"><a class="markdownIt-Anchor" href="#522-测试环境准备"></a> 5.2.2 测试环境准备</h3><p><strong>1）测试环境描述</strong><br />服务器规模：5 台<br />集群配置：副本数为 2，创建好带有存储类型的目录（提前创建）<br />集群规划：</p><table><thead><tr><th>节点</th><th>存储类型分配</th></tr></thead><tbody><tr><td>hadoop102</td><td>RAM_DISK，SSD</td></tr><tr><td>hadoop103</td><td>SSD，DISK</td></tr><tr><td>hadoop104</td><td>DISK，RAM_DISK</td></tr><tr><td>hadoop105</td><td>ARCHIVE</td></tr><tr><td>hadoop106</td><td>ARCHIVE</td></tr></tbody></table><p><strong>2）配置文件信息</strong><br />（1）为 hadoop102 节点的 hdfs-site.xml 添加如下信息<br /><property><br /><name>dfs.replication</name><br /><value>2</value><br /></property><br /><property><br /><name>dfs.storage.policy.enabled</name><br /><value>true</value><br /></property><br /><property><br /><name>dfs.datanode.data.dir</name><br /><value>[SSD]file:///opt/module/hadoop-3.1.3/hdfsdata/ssd,[RAM_DISK]file:///opt/module/hadoop-3.1.3/hdfsdata/ram_disk</value><br /></property><br />（2）为 hadoop103 节点的 hdfs-site.xml 添加如下信息<br /><property><br /><name>dfs.replication</name><br /><value>2</value><br /></property><br /><property><br /><name>dfs.storage.policy.enabled</name><br /><value>true</value><br /></property><br /><property><br /><name>dfs.datanode.data.dir</name><br /><value>[SSD]file:///opt/module/hadoop-3.1.3/hdfsdata/ssd,[DISK]file:///opt/module/hadoop-3.1.3/hdfsdata/disk</value><br /></property><br />（3）为 hadoop104 节点的 hdfs-site.xml 添加如下信息<br /><property><br /><name>dfs.replication</name><br /><value>2</value><br /></property><br /><property><br /><name>dfs.storage.policy.enabled</name><br /><value>true</value><br /></property><br /><property><br /><name>dfs.datanode.data.dir</name><br /><value>[RAM_DISK]file:///opt/module/hdfsdata/ram_disk,[DISK]file:///opt/module/hadoop-3.1.3/hdfsdata/disk</value><br /></property><br />（4）为 hadoop105 节点的 hdfs-site.xml 添加如下信息<br /><property><br /><name>dfs.replication</name><br /><value>2</value><br /></property><br /><property><br /><name>dfs.storage.policy.enabled</name><br /><value>true</value><br /></property><br /><property><br /><name>dfs.datanode.data.dir</name><br /><value>[ARCHIVE]file:///opt/module/hadoop-3.1.3/hdfsdata/archive</value><br /></property><br />（5）为 hadoop106 节点的 hdfs-site.xml 添加如下信息<br /><property><br /><name>dfs.replication</name><br /><value>2</value><br /></property><br /><property><br /><name>dfs.storage.policy.enabled</name><br /><value>true</value><br /></property><br /><property><br /><name>dfs.datanode.data.dir</name><br /><value>[ARCHIVE]file:///opt/module/hadoop-3.1.3/hdfsdata/archive</value><br /></property><br /><strong>3）数据准备</strong><br />（1）启动集群<br />[atguigu@hadoop102 hadoop-3.1.3]$ hdfs namenode -format<br />[atguigu@hadoop102 hadoop-3.1.3]$ <a href="http://myhadoop.sh">myhadoop.sh</a> start<br />（1）并在 HDFS 上创建文件目录<br />[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -mkdir /hdfsdata<br />（2）并将文件资料上传<br />[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -put /opt/module/hadoop-3.1.3/NOTICE.txt /hdfsdata</p><h3 id="523-hot-存储策略案例"><a class="markdownIt-Anchor" href="#523-hot-存储策略案例"></a> 5.2.3 HOT 存储策略案例</h3><p>（1）最开始我们未设置存储策略的情况下，我们获取该目录的存储策略<br />[atguigu@hadoop102 hadoop-3.1.3]$ hdfs storagepolicies -getStoragePolicy -path /hdfsdata<br />（2）我们查看上传的文件块分布<br />[atguigu@hadoop102 hadoop-3.1.3]$ hdfs fsck /hdfsdata -files -blocks -locations<br />[DatanodeInfoWithStorage[192.168.10.104:9866,DS-0b133854-7f9e-48df-939b-5ca6482c5afb,DISK], DatanodeInfoWithStorage[192.168.10.103:9866,DS-ca1bd3b9-d9a5-4101-9f92-3da5f1baa28b,DISK]]<br />未设置存储策略，所有文件块都存储在 DISK 下。所以，默认存储策略为 HOT。</p><h3 id="524-warm-存储策略测试"><a class="markdownIt-Anchor" href="#524-warm-存储策略测试"></a> 5.2.4 WARM 存储策略测试</h3><p>（1）接下来我们为数据降温<br />[atguigu@hadoop102 hadoop-3.1.3]$ hdfs storagepolicies -setStoragePolicy -path /hdfsdata -policy WARM<br />（2）再次查看文件块分布，我们可以看到文件块依然放在原处。<br />[atguigu@hadoop102 hadoop-3.1.3]$ hdfs fsck /hdfsdata -files -blocks -locations<br />（3）我们需要让他 HDFS 按照存储策略自行移动文件块<br />[atguigu@hadoop102 hadoop-3.1.3]$ hdfs mover /hdfsdata<br />（4）再次查看文件块分布，<br />[atguigu@hadoop102 hadoop-3.1.3]$ hdfs fsck /hdfsdata -files -blocks -locations<br />[DatanodeInfoWithStorage[192.168.10.105:9866,DS-d46d08e1-80c6-4fca-b0a2-4a3dd7ec7459,ARCHIVE], DatanodeInfoWithStorage[192.168.10.103:9866,DS-ca1bd3b9-d9a5-4101-9f92-3da5f1baa28b,DISK]]<br />文件块一半在 DISK，一半在 ARCHIVE，符合我们设置的 WARM 策略</p><h3 id="525-cold-策略测试"><a class="markdownIt-Anchor" href="#525-cold-策略测试"></a> 5.2.5 COLD 策略测试</h3><p>（1）我们继续将数据降温为 cold<br />[atguigu@hadoop102 hadoop-3.1.3]$ hdfs storagepolicies -setStoragePolicy -path /hdfsdata -policy COLD<br />注意：当我们将目录设置为 COLD 并且我们未配置 ARCHIVE 存储目录的情况下，不可以向该目录直接上传文件，会报出异常。<br />（2）手动转移<br />[atguigu@hadoop102 hadoop-3.1.3]$ hdfs mover /hdfsdata<br />（3）检查文件块的分布<br />[atguigu@hadoop102 hadoop-3.1.3]$ bin/hdfs fsck /hdfsdata -files -blocks -locations<br />[DatanodeInfoWithStorage[192.168.10.105:9866,DS-d46d08e1-80c6-4fca-b0a2-4a3dd7ec7459,ARCHIVE], DatanodeInfoWithStorage[192.168.10.106:9866,DS-827b3f8b-84d7-47c6-8a14-0166096f919d,ARCHIVE]]<br />所有文件块都在 ARCHIVE，符合 COLD 存储策略。</p><h3 id="526-one_ssd-策略测试"><a class="markdownIt-Anchor" href="#526-one_ssd-策略测试"></a> 5.2.6 ONE_SSD 策略测试</h3><p>（1）接下来我们将存储策略从默认的 HOT 更改为 One_SSD<br />[atguigu@hadoop102 hadoop-3.1.3]$ hdfs storagepolicies -setStoragePolicy -path /hdfsdata -policy One_SSD<br />（2）手动转移文件块<br />[atguigu@hadoop102 hadoop-3.1.3]$ hdfs mover /hdfsdata<br />（3）转移完成后，我们查看文件块分布，<br />[atguigu@hadoop102 hadoop-3.1.3]$ bin/hdfs fsck /hdfsdata -files -blocks -locations<br />[DatanodeInfoWithStorage[192.168.10.104:9866,DS-0b133854-7f9e-48df-939b-5ca6482c5afb,DISK], DatanodeInfoWithStorage[192.168.10.103:9866,DS-2481a204-59dd-46c0-9f87-ec4647ad429a,SSD]]<br />文件块分布为一半在 SSD，一半在 DISK，符合 One_SSD 存储策略。</p><h3 id="527-all_ssd-策略测试"><a class="markdownIt-Anchor" href="#527-all_ssd-策略测试"></a> 5.2.7 ALL_SSD 策略测试</h3><p>（1）接下来，我们再将存储策略更改为 All_SSD<br />[atguigu@hadoop102 hadoop-3.1.3]$ hdfs storagepolicies -setStoragePolicy -path /hdfsdata -policy All_SSD<br />（2）手动转移文件块<br />[atguigu@hadoop102 hadoop-3.1.3]$ hdfs mover /hdfsdata<br />（3）查看文件块分布，我们可以看到，<br />[atguigu@hadoop102 hadoop-3.1.3]$ bin/hdfs fsck /hdfsdata -files -blocks -locations<br />[DatanodeInfoWithStorage[192.168.10.102:9866,DS-c997cfb4-16dc-4e69-a0c4-9411a1b0c1eb,SSD], DatanodeInfoWithStorage[192.168.10.103:9866,DS-2481a204-59dd-46c0-9f87-ec4647ad429a,SSD]]<br />所有的文件块都存储在 SSD，符合 All_SSD 存储策略。</p><h3 id="528-lazy_persist-策略测试"><a class="markdownIt-Anchor" href="#528-lazy_persist-策略测试"></a> 5.2.8 LAZY_PERSIST 策略测试</h3><p>（1）继续改变策略，将存储策略改为 lazy_persist<br />[atguigu@hadoop102 hadoop-3.1.3]$ hdfs storagepolicies -setStoragePolicy -path /hdfsdata -policy lazy_persist<br />（2）手动转移文件块<br />[atguigu@hadoop102 hadoop-3.1.3]$ hdfs mover /hdfsdata<br />（3）查看文件块分布<br />[atguigu@hadoop102 hadoop-3.1.3]$ hdfs fsck /hdfsdata -files -blocks -locations<br />[DatanodeInfoWithStorage[192.168.10.104:9866,DS-0b133854-7f9e-48df-939b-5ca6482c5afb,DISK], DatanodeInfoWithStorage[192.168.10.103:9866,DS-ca1bd3b9-d9a5-4101-9f92-3da5f1baa28b,DISK]]<br />这里我们发现所有的文件块都是存储在 DISK，按照理论一个副本存储在 RAM_DISK，其他副本存储在 DISK 中，这是因为，我们还需要配置“dfs.datanode.max.locked.memory”，“dfs.block.size”参数。<br />那么出现存储策略为 LAZY_PERSIST 时，文件块副本都存储在 DISK 上的原因有如下两点：<br />（1）当客户端所在的 DataNode 节点没有 RAM_DISK 时，则会写入客户端所在的 DataNode 节点的 DISK 磁盘，其余副本会写入其他节点的 DISK 磁盘。<br />（2）当客户端所在的 DataNode 有 RAM_DISK，但“dfs.datanode.max.locked.memory”参数值未设置或者设置过小（小于“dfs.block.size”参数值）时，则会写入客户端所在的 DataNode 节点的 DISK 磁盘，其余副本会写入其他节点的 DISK 磁盘。<br />但是由于虚拟机的“max locked memory”为 64KB，所以，如果参数配置过大，还会报出错误：<br />ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain<br />java.lang.RuntimeException: Cannot start datanode because the configured max locked memory size (dfs.datanode.max.locked.memory) of 209715200 bytes is more than the datanode’s available RLIMIT_MEMLOCK ulimit of 65536 bytes.<br />我们可以通过该命令查询此参数的内存<br />[atguigu@hadoop102 hadoop-3.1.3]$ ulimit -a<br />max locked memory (kbytes, -l) 64</p><h1 id="第-6-章-hdfs故障排除"><a class="markdownIt-Anchor" href="#第-6-章-hdfs故障排除"></a> 第 6 章 HDFS—故障排除</h1><p>注意：采用三台服务器即可，恢复到 Yarn 开始的服务器快照。</p><h2 id="61-namenode-故障处理"><a class="markdownIt-Anchor" href="#61-namenode-故障处理"></a> 6.1 NameNode 故障处理</h2><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954270054-e62b73b2-84aa-4e4e-b978-27aabe02b120.png#" alt="" /><br />1）需求：<br />NameNode 进程挂了并且存储的数据也丢失了，如何恢复 NameNode<br />2）故障模拟<br />（1）kill -9 NameNode 进程<br />[atguigu@hadoop102 current]$ kill -9 19886<br />（2）删除 NameNode 存储的数据（/opt/module/hadoop-3.1.3/data/tmp/dfs/name）<br />[atguigu@hadoop102 hadoop-3.1.3]$ rm -rf /opt/module/hadoop-3.1.3/data/dfs/name/_<br />3）问题解决<br />（1）拷贝 SecondaryNameNode 中数据到原 NameNode 存储数据目录<br />[atguigu@hadoop102 dfs]$ scp -r atguigu@hadoop104:/opt/module/hadoop-3.1.3/data/dfs/namesecondary/_ ./name/<br />（2）重新启动 NameNode<br />[atguigu@hadoop102 hadoop-3.1.3]$ hdfs --daemon start namenode<br />（3）向集群上传一个文件</p><h2 id="62-集群安全模式磁盘修复"><a class="markdownIt-Anchor" href="#62-集群安全模式磁盘修复"></a> 6.2 集群安全模式&amp;磁盘修复</h2><p>**1）安全模式：**文件系统只接受读数据请求，而不接受删除、修改等变更请求<br /><strong>2）进入安全模式场景</strong></p><ul><li>NameNode 在加载镜像文件和编辑日志期间处于安全模式；</li><li>NameNode 再接收 DataNode 注册时，处于安全模式</li></ul><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954270645-ea0360c3-912b-41ac-a235-da6909680c4e.png#" alt="" /><br /><strong>3）退出安全模式条件</strong><br />dfs.namenode.safemode.min.datanodes:最小可用 datanode 数量，默认 0<br />dfs.namenode.safemode.threshold-pct:副本数达到最小要求的 block 占系统总 block 数的百分比，默认 0.999f。（只允许丢一个块）<br />dfs.namenode.safemode.extension:稳定时间，默认值 30000 毫秒，即 30 秒<br />4）基本语法<br />集群处于安全模式，不能执行重要操作（写操作）。集群启动完成后，自动退出安全模式。<br />（1）bin/hdfs dfsadmin -safemode get （功能描述：查看安全模式状态）<br />（2）bin/hdfs dfsadmin -safemode enter （功能描述：进入安全模式状态）<br />（3）bin/hdfs dfsadmin -safemode leave （功能描述：离开安全模式状态）<br />（4）bin/hdfs dfsadmin -safemode wait （功能描述：等待安全模式状态）<br />5）案例 1：启动集群进入安全模式<br />（1）重新启动集群<br />[atguigu@hadoop102 subdir0]$ <a href="http://myhadoop.sh">myhadoop.sh</a> stop<br />[atguigu@hadoop102 subdir0]$ <a href="http://myhadoop.sh">myhadoop.sh</a> start<br />（2）集群启动后，立即来到集群上删除数据，提示集群处于安全模式<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954271167-547423ad-4e25-49f1-8b60-6d86d0408706.png#" alt="" /><br />6）案例 2：磁盘修复<br />需求：数据块损坏，进入安全模式，如何处理<br />（1）分别进入 hadoop102、hadoop103、hadoop104 的/opt/module/hadoop-3.1.3/data/dfs/data/current/BP-1015489500-192.168.10.102-1611909480872/current/finalized/subdir0/subdir0 目录，统一删除某 2 个块信息<br />[atguigu@hadoop102 subdir0]$ pwd<br />/opt/module/hadoop-3.1.3/data/dfs/data/current/BP-1015489500-192.168.10.102-1611909480872/current/finalized/subdir0/subdir0</p><p>[atguigu@hadoop102 subdir0]$ rm -rf blk_1073741847 blk_1073741847_1023.meta<br />[atguigu@hadoop102 subdir0]$ rm -rf blk_1073741865 blk_1073741865_1042.meta<br />说明：hadoop103/hadoop104 重复执行以上命令<br />（2）重新启动集群<br />[atguigu@hadoop102 subdir0]$ <a href="http://myhadoop.sh">myhadoop.sh</a> stop<br />[atguigu@hadoop102 subdir0]$ <a href="http://myhadoop.sh">myhadoop.sh</a> start<br />（3）观察 <a href="http://hadoop102:9870/dfshealth.html#tab-overview">http://hadoop102:9870/dfshealth.html#tab-overview</a><br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954271649-d8f53667-7829-4a1d-83d7-d01aeccdae25.png#" alt="" /><br />说明：安全模式已经打开，块的数量没有达到要求。<br />（4）离开安全模式<br />[atguigu@hadoop102 subdir0]$ hdfs dfsadmin -safemode get<br />Safe mode is ON<br />[atguigu@hadoop102 subdir0]$ hdfs dfsadmin -safemode leave<br />Safe mode is OFF<br />（5）观察 <a href="http://hadoop102:9870/dfshealth.html#tab-overview">http://hadoop102:9870/dfshealth.html#tab-overview</a><br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954272180-ea471541-2156-4129-b987-68dc574b6786.png#" alt="" /><br />（6）将元数据删除<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954272718-85f9c737-bd8a-4b96-b534-c082ea7ebce3.png#" alt="" /><br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954273234-4c2f7a87-3bc1-45e3-b52d-34f797cf737c.png#" alt="" /><br />（7）观察[<a href="http://hadoop102:9870/dfshealth.html#tab-overview">http://hadoop102:9870/dfshealth.html#tab-overview</a>](<a href="http://hadoop102:9870/dfshealth.html">http://hadoop102:9870/dfshealth.html</a>&quot; \l &quot;tab-overview)，集群已经正常<br />7）案例 3：<br />需求：模拟等待安全模式<br />（1）查看当前模式<br />[atguigu@hadoop102 hadoop-3.1.3]$ hdfs dfsadmin -safemode get<br />Safe mode is OFF<br />（2）先进入安全模式<br />[atguigu@hadoop102 hadoop-3.1.3]$ bin/hdfs dfsadmin -safemode enter<br />（3）创建并执行下面的脚本<br />在/opt/module/hadoop-3.1.3 路径上，编辑一个脚本 <a href="http://safemode.sh">safemode.sh</a><br />[atguigu@hadoop102 hadoop-3.1.3]$ vim <a href="http://safemode.sh">safemode.sh</a></p><p>#!/bin/bash<br />hdfs dfsadmin -safemode wait<br />hdfs dfs -put /opt/module/hadoop-3.1.3/README.txt /</p><p>[atguigu@hadoop102 hadoop-3.1.3]$ chmod 777 <a href="http://safemode.sh">safemode.sh</a></p><p>[atguigu@hadoop102 hadoop-3.1.3]$ ./safemode.sh<br />（4）再打开一个窗口，执行<br />[atguigu@hadoop102 hadoop-3.1.3]$ bin/hdfs dfsadmin -safemode leave<br />（5）再观察上一个窗口<br />Safe mode is OFF<br />（6）HDFS 集群上已经有上传的数据了<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954273718-c0546ef5-593e-489b-a5da-cba894963aaa.png#" alt="" /></p><h2 id="63-慢磁盘监控"><a class="markdownIt-Anchor" href="#63-慢磁盘监控"></a> 6.3 慢磁盘监控</h2><p>“慢磁盘”指的时写入数据非常慢的一类磁盘。其实慢性磁盘并不少见，当机器运行时间长了，上面跑的任务多了，磁盘的读写性能自然会退化，严重时就会出现写入数据延时的问题。<br />如何发现慢磁盘？<br />正常在 HDFS 上创建一个目录，只需要不到 1s 的时间。如果你发现创建目录超过 1 分钟及以上，而且这个现象并不是每次都有。只是偶尔慢了一下，就很有可能存在慢磁盘。<br />可以采用如下方法找出是哪块磁盘慢：<br /><strong>1）通过心跳未联系时间。</strong><br />一般出现慢磁盘现象，会影响到 DataNode 与 NameNode 之间的心跳。正常情况心跳时间间隔是 3s。超过 3s 说明有异常。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954274192-ba3a8617-dd07-43ab-95fd-ad6905e38fb6.png#" alt="" /><br /><strong>2）fio 命令，测试磁盘的读写性能</strong><br />（1）顺序读测试<br />[atguigu@hadoop102 ~]# sudo yum install -y fio<br />[atguigu@hadoop102 ~]# sudo fio -filename=/home/atguigu/test.log -direct=1 -iodepth 1 -thread -rw=read -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=test_r</p><p>Run status group 0 (all jobs):<br />READ: bw=360MiB/s (378MB/s), 360MiB/s-360MiB/s (378MB/s-378MB/s), io=20.0GiB (21.5GB), run=56885-56885msec<br />结果显示，磁盘的总体顺序读速度为<strong>360MiB/s</strong>。<br />（2）顺序写测试<br />[atguigu@hadoop102 ~]# sudo fio -filename=/home/atguigu/test.log -direct=1 -iodepth 1 -thread -rw=write -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=test_w</p><p>Run status group 0 (all jobs):<br />WRITE: bw=341MiB/s (357MB/s), 341MiB/s-341MiB/s (357MB/s-357MB/s), io=19.0GiB (21.4GB), run=60001-60001msec<br />结果显示，磁盘的总体顺序写速度为<strong>341MiB/s</strong>。<br />（3）随机写测试<br />[atguigu@hadoop102 ~]# sudo fio -filename=/home/atguigu/test.log -direct=1 -iodepth 1 -thread -rw=randwrite -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=test_randw</p><p>Run status group 0 (all jobs):<br />WRITE: bw=309MiB/s (324MB/s), 309MiB/s-309MiB/s (324MB/s-324MB/s), io=18.1GiB (19.4GB), run=60001-60001msec<br />结果显示，磁盘的总体随机写速度为<strong>309MiB/s</strong>。<br />（4）混合随机读写：<br />[atguigu@hadoop102 ~]# sudo fio -filename=/home/atguigu/test.log -direct=1 -iodepth 1 -thread -rw=randrw -rwmixread=70 -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=test_r_w -ioscheduler=noop</p><p>Run status group 0 (all jobs):<br />READ: bw=220MiB/s (231MB/s), 220MiB/s-220MiB/s (231MB/s-231MB/s), io=12.9GiB (13.9GB), run=60001-60001msec<br />WRITE: bw=94.6MiB/s (99.2MB/s), 94.6MiB/s-94.6MiB/s (99.2MB/s-99.2MB/s), io=5674MiB (5950MB), run=60001-60001msec<br />结果显示，磁盘的总体混合随机读写，读速度为<strong>220MiB/s</strong>，写速度<strong>94.6MiB/s。</strong></p><h2 id="64-小文件归档"><a class="markdownIt-Anchor" href="#64-小文件归档"></a> 6.4 小文件归档</h2><p><strong>1）HDFS 存储小文件弊端</strong><br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954274500-429b14ae-f214-4eeb-8571-5549ba238be3.png#" alt="" /><br />每个文件均按块存储，每个块的元数据存储在 NameNode 的内存中，因此 HDFS 存储小文件会非常低效。因为大量的小文件会耗尽 NameNode 中的大部分内存。但注意，存储小文件所需要的磁盘容量和数据块的大小无关。例如，一个 1MB 的文件设置为 128MB 的块存储，实际使用的是 1MB 的磁盘空间，而不是 128MB。<br /><strong>2）解决存储小文件办法之一</strong><br />HDFS 存档文件或 HAR 文件，是一个更高效的文件存档工具，它将文件存入 HDFS 块，在减少 NameNode 内存使用的同时，允许对文件进行透明的访问。具体说来，HDFS 存档文件对内还是一个一个独立文件，对 NameNode 而言却是一个整体，减少了 NameNode 的内存。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954274815-b2c71694-73df-4b1d-96df-ef3820130fc1.png#" alt="" /><br />3）案例实操<br />（1）需要启动 YARN 进程<br />[atguigu@hadoop102 hadoop-3.1.3]$ <a href="http://start-yarn.sh">start-yarn.sh</a><br />（2）归档文件<br />把/input 目录里面的所有文件归档成一个叫 input.har 的归档文件，并把归档后文件存储到/output 路径下。<br />[atguigu@hadoop102 hadoop-3.1.3]$ hadoop archive -archiveName input.har -p /input /output<br />（3）查看归档<br />[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -ls /output/input.har<br />[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -ls har:///output/input.har<br />（4）解归档文件<br />[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -cp har:///output/input.har/* /</p><h1 id="第-7-章-hdfs集群迁移"><a class="markdownIt-Anchor" href="#第-7-章-hdfs集群迁移"></a> 第 7 章 HDFS—集群迁移</h1><h2 id="71-apache-和-apache-集群间数据拷贝"><a class="markdownIt-Anchor" href="#71-apache-和-apache-集群间数据拷贝"></a> 7.1 Apache 和 Apache 集群间数据拷贝</h2><p>1）scp 实现两个远程主机之间的文件复制<br />scp -r hello.txt <a href="mailto:root@hadoop103:/user/atguigu/hello.txt">root@hadoop103:/user/atguigu/hello.txt</a> // 推 push<br />scp -r <a href="mailto:root@hadoop103:/user/atguigu/hello.txt%20%20hello.txt">root@hadoop103:/user/atguigu/hello.txt hello.txt</a> // 拉 pull<br />scp -r <a href="mailto:root@hadoop103:/user/atguigu/hello.txt">root@hadoop103:/user/atguigu/hello.txt</a> root@hadoop104:/user/atguigu //是通过本地主机中转实现两个远程主机的文件复制；如果在两个远程主机之间 ssh 没有配置的情况下可以使用该方式。<br />2）采用 distcp 命令实现两个 Hadoop 集群之间的递归数据复制<br />[atguigu@hadoop102 hadoop-3.1.3]$ bin/hadoop distcp hdfs://hadoop102:8020/user/atguigu/hello.txt hdfs://hadoop105:8020/user/atguigu/hello.txt</p><h2 id="72-apache-和-cdh-集群间数据拷贝"><a class="markdownIt-Anchor" href="#72-apache-和-cdh-集群间数据拷贝"></a> 7.2 Apache 和 CDH 集群间数据拷贝</h2><h1 id="第-8-章-mapreduce-生产经验"><a class="markdownIt-Anchor" href="#第-8-章-mapreduce-生产经验"></a> 第 8 章 MapReduce 生产经验</h1><h2 id="81-mapreduce-跑的慢的原因"><a class="markdownIt-Anchor" href="#81-mapreduce-跑的慢的原因"></a> 8.1 MapReduce 跑的慢的原因</h2><p>MapReduce 程序效率的瓶颈在于两点：<br /><strong>1）计算机性能</strong><br />CPU、内存、磁盘、网络<br /><strong>2）I/O 操作优化</strong><br />（1）数据倾斜<br />（2）Map 运行时间太长，导致 Reduce 等待过久<br />（3）小文件过多</p><h2 id="82-mapreduce-常用调优参数"><a class="markdownIt-Anchor" href="#82-mapreduce-常用调优参数"></a> 8.2 MapReduce 常用调优参数</h2><h2 id="83-mapreduce-数据倾斜问题"><a class="markdownIt-Anchor" href="#83-mapreduce-数据倾斜问题"></a> 8.3 MapReduce 数据倾斜问题</h2><p><strong>1）数据倾斜现象</strong><br />数据频率倾斜——某一个区域的数据量要远远大于其他区域。<br />数据大小倾斜——部分记录的大小远远大于平均值。<br /><img src="https://cdn.nlark.com/yuque/0/2021/jpeg/12672933/1620954320104-f05fd3fe-83ed-4876-ac4b-a4fee9351f70.jpeg#" alt="" /><br /><strong>2）减少数据倾斜的方法</strong><br />** （1）首先检查是否空值过多造成的数据倾斜**<br />** **生产环境，可以直接过滤掉空值；如果想保留空值，就自定义分区，将空值加随机数打散。最后再二次聚合。<br />**（2）能在 map 阶段提前处理，最好先在 Map 阶段处理。如：Combiner、MapJoin**<br /><strong>（3）设置多个 reduce 个数</strong></p><h1 id="第-9-章-hadoop-yarn-生产经验"><a class="markdownIt-Anchor" href="#第-9-章-hadoop-yarn-生产经验"></a> 第 9 章 Hadoop-Yarn 生产经验</h1><h2 id="91-常用的调优参数"><a class="markdownIt-Anchor" href="#91-常用的调优参数"></a> 9.1 常用的调优参数</h2><p>1）调优参数列表<br />（1）Resourcemanager 相关<br />yarn.resourcemanager.scheduler.client.thread-count ResourceManager 处理调度器请求的线程数量<br />yarn.resourcemanager.scheduler.class 配置调度器<br />（2）Nodemanager 相关<br />yarn.nodemanager.resource.memory-mb NodeManager 使用内存数<br />yarn.nodemanager.resource.system-reserved-memory-mb NodeManager 为系统保留多少内存，和上一个参数二者取一即可</p><p>yarn.nodemanager.resource.cpu-vcores NodeManager 使用 CPU 核数<br />yarn.nodemanager.resource.count-logical-processors-as-cores 是否将虚拟核数当作 CPU 核数<br />yarn.nodemanager.resource.pcores-vcores-multiplier 虚拟核数和物理核数乘数，例如：4 核 8 线程，该参数就应设为 2<br />yarn.nodemanager.resource.detect-hardware-capabilities 是否让 yarn 自己检测硬件进行配置</p><p>yarn.nodemanager.pmem-check-enabled 是否开启物理内存检查限制 container<br />yarn.nodemanager.vmem-check-enabled 是否开启虚拟内存检查限制 container<br />yarn.nodemanager.vmem-pmem-ratio 虚拟内存物理内存比例<br />（3）Container 容器相关<br />yarn.scheduler.minimum-allocation-mb 容器最小内存<br />yarn.scheduler.maximum-allocation-mb 容器最大内存<br />yarn.scheduler.minimum-allocation-vcores 容器最小核数<br />yarn.scheduler.maximum-allocation-vcores 容器最大核数<br />2）参数具体使用案例<br />详见《尚硅谷大数据技术之 Hadoop（Yarn）》，第 2.1 节。</p><h2 id="92-容量调度器使用"><a class="markdownIt-Anchor" href="#92-容量调度器使用"></a> 9.2 容量调度器使用</h2><p>详见《尚硅谷大数据技术之 Hadoop（Yarn）》，第 2.2 节。</p><h2 id="93-公平调度器使用"><a class="markdownIt-Anchor" href="#93-公平调度器使用"></a> 9.3 公平调度器使用</h2><p>详见《尚硅谷大数据技术之 Hadoop（Yarn）》，第 2.3 节。</p><h1 id="第-10-章-hadoop-综合调优"><a class="markdownIt-Anchor" href="#第-10-章-hadoop-综合调优"></a> 第 10 章 Hadoop 综合调优</h1><h2 id="101-hadoop-小文件优化方法"><a class="markdownIt-Anchor" href="#101-hadoop-小文件优化方法"></a> 10.1 Hadoop 小文件优化方法</h2><h3 id="1011-hadoop-小文件弊端"><a class="markdownIt-Anchor" href="#1011-hadoop-小文件弊端"></a> 10.1.1 Hadoop 小文件弊端</h3><p>HDFS 上每个文件都要在 NameNode 上创建对应的元数据，这个元数据的大小约为 150byte，这样当小文件比较多的时候，就会产生很多的元数据文件，一方面会大量占用 NameNode 的内存空间，另一方面就是元数据文件过多，使得寻址索引速度变慢。<br />小文件过多，在进行 MR 计算时，会生成过多切片，需要启动过多的 MapTask。每个 MapTask 处理的数据量小，导致 MapTask 的处理时间比启动时间还小，白白消耗资源。</p><h3 id="1012-hadoop-小文件解决方案"><a class="markdownIt-Anchor" href="#1012-hadoop-小文件解决方案"></a> 10.1.2 Hadoop 小文件解决方案</h3><p><strong>1）在数据采集的时候，就将小文件或小批数据合成大文件再上传 HDFS（数据源头）</strong><br /><strong>2）Hadoop Archive（存储方向）</strong><br />是一个高效的将小文件放入 HDFS 块中的文件存档工具，能够将多个小文件打包成一个 HAR 文件，从而达到减少 NameNode 的内存使用<br /><strong>3）CombineTextInputFormat（计算方向）</strong><br />CombineTextInputFormat 用于将多个小文件在切片过程中生成一个单独的切片或者少量的切片。<br /><strong>4）开启 uber 模式，实现 JVM 重用（计算方向）</strong><br />默认情况下，每个 Task 任务都需要启动一个 JVM 来运行，如果 Task 任务计算的数据量很小，我们可以让同一个 Job 的多个 Task 运行在一个 JVM 中，不必为每个 Task 都开启一个 JVM。<br />（1）未开启 uber 模式，在/input 路径上上传多个小文件并执行 wordcount 程序<br />[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output2<br />（2）观察控制台<br />2021-02-14 16:13:50,607 INFO mapreduce.Job: Job job_1613281510851_0002 running in uber mode : false<br />（3）观察<a href="http://hadoop103:8088/cluster">http://hadoop103:8088/cluster</a><br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954320449-58fc03a5-2090-4c9c-9170-be078caa5f77.png#" alt="" /> <img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954320931-32d992c0-2c63-45b3-ba97-c2f41fcee19c.png#" alt="" /><br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954321389-2dab0cd9-a074-48b0-8eba-7255b276b00c.png#" alt="" /><br />（4）开启 uber 模式，在 mapred-site.xml 中添加如下配置</p><!--  开启uber模式，默认关闭 --><property>  <name>mapreduce.job.ubertask.enable</name>  <value>true</value></property><!-- uber模式中最大的mapTask数量，可向下修改  --><property>  <name>mapreduce.job.ubertask.maxmaps</name>  <value>9</value></property><!-- uber模式中最大的reduce数量，可向下修改 --><property>  <name>mapreduce.job.ubertask.maxreduces</name>  <value>1</value></property><!-- uber模式中最大的输入数据量，默认使用dfs.blocksize 的值，可向下修改 --><property>  <name>mapreduce.job.ubertask.maxbytes</name>  <value></value></property>（5）分发配置[atguigu@hadoop102 hadoop]$ xsync mapred-site.xml（6）再次执行wordcount程序[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output2（7）观察控制台2021-02-14 16:28:36,198 INFO mapreduce.Job: Job job_1613281510851_0003 running in uber mode : true（8）观察[http://hadoop103:8088/cluster](http://hadoop103:8088/cluster)![](https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954321655-bafe67a4-c057-4cb2-9226-43d84092e72a.png#)<h2 id="102-测试-mapreduce-计算性能"><a class="markdownIt-Anchor" href="#102-测试-mapreduce-计算性能"></a> 10.2 测试 MapReduce 计算性能</h2><p>使用 Sort 程序评测 MapReduce<br />注：一个虚拟机不超过 150G 磁盘尽量不要执行这段代码<br />（1）使用 RandomWriter 来产生随机数，每个节点运行 10 个 Map 任务，每个 Map 产生大约 1G 大小的二进制随机数<br />[atguigu@hadoop102 mapreduce]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar randomwriter random-data<br />（2）执行 Sort 程序<br />[atguigu@hadoop102 mapreduce]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar sort random-data sorted-data<br />（3）验证数据是否真正排好序了<br />[atguigu@hadoop102 mapreduce]$<br />hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar testmapredsort -sortInput random-data -sortOutput sorted-data</p><h2 id="103-企业开发场景案例"><a class="markdownIt-Anchor" href="#103-企业开发场景案例"></a> 10.3 企业开发场景案例</h2><h3 id="1031-需求"><a class="markdownIt-Anchor" href="#1031-需求"></a> 10.3.1 需求</h3><p>（1）需求：从 1G 数据中，统计每个单词出现次数。服务器 3 台，每台配置 4G 内存，4 核 CPU，4 线程。<br />（2）需求分析：<br />1G / 128m = 8 个 MapTask；1 个 ReduceTask；1 个 mrAppMaster<br />平均每个节点运行 10 个 / 3 台 ≈ 3 个任务（4 3 3）</p><h3 id="1032-hdfs-参数调优"><a class="markdownIt-Anchor" href="#1032-hdfs-参数调优"></a> 10.3.2 HDFS 参数调优</h3><p>（1）修改：<a href="http://hadoop-env.sh">hadoop-env.sh</a><br />export HDFS_NAMENODE_OPTS=&quot;-Dhadoop.security.logger=INFO,RFAS -Xmx1024m&quot;</p><p>export HDFS_DATANODE_OPTS=&quot;-Dhadoop.security.logger=ERROR,RFAS -Xmx1024m&quot;<br />（2）修改 hdfs-site.xml</p><!-- NameNode有一个工作线程池，默认值是10 --><property>    <name>dfs.namenode.handler.count</name>    <value>21</value></property>（3）修改core-site.xml<!-- 配置垃圾回收时间为60分钟 --><property>    <name>fs.trash.interval</name>    <value>60</value></property>（4）分发配置[atguigu@hadoop102 hadoop]$ xsync hadoop-env.sh hdfs-site.xml core-site.xml<h3 id="1033-mapreduce-参数调优"><a class="markdownIt-Anchor" href="#1033-mapreduce-参数调优"></a> 10.3.3 MapReduce 参数调优</h3><p>（1）修改 mapred-site.xml</p><!-- 环形缓冲区大小，默认100m --><property>  <name>mapreduce.task.io.sort.mb</name>  <value>100</value></property><!-- 环形缓冲区溢写阈值，默认0.8 --><property>  <name>mapreduce.map.sort.spill.percent</name>  <value>0.80</value></property><!-- merge合并次数，默认10个 --><property>  <name>mapreduce.task.io.sort.factor</name>  <value>10</value></property><!-- maptask内存，默认1g； maptask堆内存大小默认和该值大小一致mapreduce.map.java.opts --><property>  <name>mapreduce.map.memory.mb</name>  <value>-1</value>  <description>The amount of memory to request from the scheduler for each    map task. If this is not specified or is non-positive, it is inferred from mapreduce.map.java.opts and mapreduce.job.heap.memory-mb.ratio. If java-opts are also not specified, we set it to 1024.  </description></property><!-- matask的CPU核数，默认1个 --><property>  <name>mapreduce.map.cpu.vcores</name>  <value>1</value></property><!-- matask异常重试次数，默认4次 --><property>  <name>mapreduce.map.maxattempts</name>  <value>4</value></property><!-- 每个Reduce去Map中拉取数据的并行数。默认值是5 --><property>  <name>mapreduce.reduce.shuffle.parallelcopies</name>  <value>5</value></property><!-- Buffer大小占Reduce可用内存的比例，默认值0.7 --><property>  <name>mapreduce.reduce.shuffle.input.buffer.percent</name>  <value>0.70</value></property><!-- Buffer中的数据达到多少比例开始写入磁盘，默认值0.66。 --><property>  <name>mapreduce.reduce.shuffle.merge.percent</name>  <value>0.66</value></property><!-- reducetask内存，默认1g；reducetask堆内存大小默认和该值大小一致mapreduce.reduce.java.opts --><property>  <name>mapreduce.reduce.memory.mb</name>  <value>-1</value>  <description>The amount of memory to request from the scheduler for each    reduce task. If this is not specified or is non-positive, it is inferred    from mapreduce.reduce.java.opts and mapreduce.job.heap.memory-mb.ratio.    If java-opts are also not specified, we set it to 1024.  </description></property><!-- reducetask的CPU核数，默认1个 --><property>  <name>mapreduce.reduce.cpu.vcores</name>  <value>2</value></property><!-- reducetask失败重试次数，默认4次 --><property>  <name>mapreduce.reduce.maxattempts</name>  <value>4</value></property><!-- 当MapTask完成的比例达到该值后才会为ReduceTask申请资源。默认是0.05 --><property>  <name>mapreduce.job.reduce.slowstart.completedmaps</name>  <value>0.05</value></property><!-- 如果程序在规定的默认10分钟内没有读到数据，将强制超时退出 --><property>  <name>mapreduce.task.timeout</name>  <value>600000</value></property>（2）分发配置[atguigu@hadoop102 hadoop]$ xsync mapred-site.xml<h3 id="1034-yarn-参数调优"><a class="markdownIt-Anchor" href="#1034-yarn-参数调优"></a> 10.3.4 Yarn 参数调优</h3><p>（1）修改 yarn-site.xml 配置参数如下：</p><!-- 选择调度器，默认容量 --><property><description>The class to use as the resource scheduler.</description><name>yarn.resourcemanager.scheduler.class</name><value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value></property><!-- ResourceManager处理调度器请求的线程数量,默认50；如果提交的任务数大于50，可以增加该值，但是不能超过3台 * 4线程 = 12线程（去除其他应用程序实际不能超过8） --><property><description>Number of threads to handle scheduler interface.</description><name>yarn.resourcemanager.scheduler.client.thread-count</name><value>8</value></property><!-- 是否让yarn自动检测硬件进行配置，默认是false，如果该节点有很多其他应用程序，建议手动配置。如果该节点没有其他应用程序，可以采用自动 --><property><description>Enable auto-detection of node capabilities such asmemory and CPU.</description><name>yarn.nodemanager.resource.detect-hardware-capabilities</name><value>false</value></property><!-- 是否将虚拟核数当作CPU核数，默认是false，采用物理CPU核数 --><property><description>Flag to determine if logical processors(such ashyperthreads) should be counted as cores. Only applicable on Linuxwhen yarn.nodemanager.resource.cpu-vcores is set to -1 andyarn.nodemanager.resource.detect-hardware-capabilities is true.</description><name>yarn.nodemanager.resource.count-logical-processors-as-cores</name><value>false</value></property><!-- 虚拟核数和物理核数乘数，默认是1.0 --><property><description>Multiplier to determine how to convert phyiscal cores tovcores. This value is used if yarn.nodemanager.resource.cpu-vcoresis set to -1(which implies auto-calculate vcores) andyarn.nodemanager.resource.detect-hardware-capabilities is set to true. Thenumber of vcores will be calculated asnumber of CPUs * multiplier.</description><name>yarn.nodemanager.resource.pcores-vcores-multiplier</name><value>1.0</value></property><!-- NodeManager使用内存数，默认8G，修改为4G内存 --><property><description>Amount of physical memory, in MB, that can be allocated for containers. If set to -1 andyarn.nodemanager.resource.detect-hardware-capabilities is true, it isautomatically calculated(in case of Windows and Linux).In other cases, the default is 8192MB.</description><name>yarn.nodemanager.resource.memory-mb</name><value>4096</value></property><!-- nodemanager的CPU核数，不按照硬件环境自动设定时默认是8个，修改为4个 --><property><description>Number of vcores that can be allocatedfor containers. This is used by the RM scheduler when allocatingresources for containers. This is not used to limit the number ofCPUs used by YARN containers. If it is set to -1 andyarn.nodemanager.resource.detect-hardware-capabilities is true, it isautomatically determined from the hardware in case of Windows and Linux.In other cases, number of vcores is 8 by default.</description><name>yarn.nodemanager.resource.cpu-vcores</name><value>4</value></property><!-- 容器最小内存，默认1G --><property><description>The minimum allocation for every container request at the RMin MBs. Memory requests lower than this will be set to the value of thisproperty. Additionally, a node manager that is configured to have less memorythan this value will be shut down by the resource manager.</description><name>yarn.scheduler.minimum-allocation-mb</name><value>1024</value></property><!-- 容器最大内存，默认8G，修改为2G --><property><description>The maximum allocation for every container request at the RMin MBs. Memory requests higher than this will throw anInvalidResourceRequestException.</description><name>yarn.scheduler.maximum-allocation-mb</name><value>2048</value></property><!-- 容器最小CPU核数，默认1个 --><property><description>The minimum allocation for every container request at the RMin terms of virtual CPU cores. Requests lower than this will be set to thevalue of this property. Additionally, a node manager that is configured tohave fewer virtual cores than this value will be shut down by the resourcemanager.</description><name>yarn.scheduler.minimum-allocation-vcores</name><value>1</value></property><!-- 容器最大CPU核数，默认4个，修改为2个 --><property><description>The maximum allocation for every container request at the RMin terms of virtual CPU cores. Requests higher than this will throw anInvalidResourceRequestException.</description><name>yarn.scheduler.maximum-allocation-vcores</name><value>2</value></property><!-- 虚拟内存检查，默认打开，修改为关闭 --><property><description>Whether virtual memory limits will be enforced forcontainers.</description><name>yarn.nodemanager.vmem-check-enabled</name><value>false</value></property><!-- 虚拟内存和物理内存设置比例,默认2.1 --><property><description>Ratio between virtual memory to physical memory whensetting memory limits for containers. Container allocations areexpressed in terms of physical memory, and virtual memory usageis allowed to exceed this allocation by this ratio.</description><name>yarn.nodemanager.vmem-pmem-ratio</name><value>2.1</value></property>（2）分发配置[atguigu@hadoop102 hadoop]$ xsync yarn-site.xml<h3 id="1035-执行程序"><a class="markdownIt-Anchor" href="#1035-执行程序"></a> 10.3.5 执行程序</h3><p>（1）重启集群<br />[atguigu@hadoop102 hadoop-3.1.3]$ sbin/stop-yarn.sh<br />[atguigu@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh<br />（2）执行 WordCount 程序<br />[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output<br />（3）观察 Yarn 任务执行页面<br /><a href="http://hadoop103:8088/cluster/apps">http://hadoop103:8088/cluster/apps</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="posts/0/"/>
      <url>posts/0/</url>
      
        <content type="html"><![CDATA[<p>尚硅谷大数据项目之实时项目<br />优化<br />(作者：尚硅谷大数据研发部)<br />版本：V2.0</p><h1 id="1-资源配置调优"><a class="markdownIt-Anchor" href="#1-资源配置调优"></a> 1 资源配置调优</h1><p>Flink 性能调优的第一步，就是为任务分配合适的资源，在一定范围内，增加资源的分配与性能的提升是成正比的，实现了最优的资源配置后，在此基础上再考虑进行后面论述的性能调优策略。<br />提交方式主要是 yarn-per-job，资源的分配在使用脚本提交 Flink 任务时进行指定。</p><ul><li>标准的 Flink 任务提交脚本（Generic CLI 模式）</li></ul><p>从 1.11 开始，增加了通用客户端模式，参数使用-D &lt;property=value&gt;指定<br />bin/flink run <br />-t yarn-per-job <br />-d <br />-p 5 \ 指定并行度<br />-Dyarn.application.queue=test \ 指定 yarn 队列<br />-Djobmanager.memory.process.size=1024mb \ 指定 JM 的总进程大小<br />-Dtaskmanager.memory.process.size=1024mb \ 指定每个 TM 的总进程大小<br />-Dtaskmanager.numberOfTaskSlots=2 \ 指定每个 TM 的 slot 数<br />-c com.atguigu.app.dwd.LogBaseApp <br />/opt/module/gmall-flink/gmall-realtime-1.0-SNAPSHOT-jar-with-dependencies.jar<br />参数列表：<br /><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/config.html">https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/config.html</a></p><h2 id="11-内存设置"><a class="markdownIt-Anchor" href="#11-内存设置"></a> 1.1 内存设置</h2><p>生产资源配置：<br />bin/flink run <br />-t yarn-per-job <br />-d <br />-p 5 \ 指定并行度<br />-Dyarn.application.queue=test \ 指定 yarn 队列<br />-Djobmanager.memory.process.size=2048mb \ JM2~4G 足够<br />-Dtaskmanager.memory.process.size=6144mb \ 单个 TM2~8G 足够<br />-Dtaskmanager.numberOfTaskSlots=2 \ 与容器核数 1core：1slot 或 1core：2slot<br />-c com.atguigu.app.dwd.LogBaseApp <br />/opt/module/gmall-flink/gmall-realtime-1.0-SNAPSHOT-jar-with-dependencies.jar<br />Flink 是实时流处理，关键在于资源情况能不能抗住高峰时期每秒的数据量，通常用 QPS/TPS 来描述数据情况。</p><h2 id="12-并行度设置"><a class="markdownIt-Anchor" href="#12-并行度设置"></a> 1.2 并行度设置</h2><h3 id="121-最优并行度计算"><a class="markdownIt-Anchor" href="#121-最优并行度计算"></a> 1.2.1 最优并行度计算</h3><p>开发完成后，先进性压测。任务并行度给 10 以下，测试单个并行度的处理上限。然后 总 QPS/单并行度的处理能力 = 并行度<br />不能只从 QPS 去得出并行度，因为有些字段少、逻辑简单的任务，单并行度一秒处理几万条数据。而有些数据字段多，处理逻辑复杂，单并行度一秒只能处理 1000 条数据。<br />最好根据高峰期的 QPS 压测，并行度*1.2 倍，富余一些资源。</p><h3 id="122-source-端并行度的配置"><a class="markdownIt-Anchor" href="#122-source-端并行度的配置"></a> 1.2.2 Source 端并行度的配置</h3><p>数据源端是 Kafka，Source 的并行度设置为 Kafka 对应 Topic 的分区数。<br />如果已经等于 Kafka 的分区数，消费速度仍跟不上数据生产速度，考虑下 Kafka 要扩大分区，同时调大并行度等于分区数。<br />Flink 的一个并行度可以处理一至多个分区的数据，如果并行度多于 Kafka 的分区数，那么就会造成有的并行度空闲，浪费资源。</p><h3 id="123-transform-端并行度的配置"><a class="markdownIt-Anchor" href="#123-transform-端并行度的配置"></a> 1.2.3 Transform 端并行度的配置</h3><ul><li>Keyby 之前的算子</li></ul><p>一般不会做太重的操作，都是比如 map、filter、flatmap 等处理较快的算子，并行度可以和 source 保持一致。</p><ul><li>Keyby 之后的算子</li></ul><p>如果并发较大，建议设置并行度为 2 的整数次幂，例如：128、256、512；<br />小并发任务的并行度不一定需要设置成 2 的整数次幂；<br />大并发任务如果没有 KeyBy，并行度也无需设置为 2 的整数次幂；</p><h3 id="124-sink-端并行度的配置"><a class="markdownIt-Anchor" href="#124-sink-端并行度的配置"></a> 1.2.4 Sink 端并行度的配置</h3><p>Sink 端是数据流向下游的地方，可以根据 Sink 端的数据量及下游的服务抗压能力进行评估。如果 Sink 端是 Kafka，可以设为 Kafka 对应 Topic 的分区数。<br />Sink 端的数据量小，比较常见的就是监控告警的场景，并行度可以设置的小一些。<br />Source 端的数据量是最小的，拿到 Source 端流过来的数据后做了细粒度的拆分，数据量不断的增加，到 Sink 端的数据量就非常大。那么在 Sink 到下游的存储中间件的时候就需要提高并行度。<br />另外 Sink 端要与下游的服务进行交互，并行度还得根据下游的服务抗压能力来设置，如果在 Flink Sink 这端的数据量过大的话，且 Sink 处并行度也设置的很大，但下游的服务完全撑不住这么大的并发写入，可能会造成下游服务直接被写挂，所以最终还是要在 Sink 处的并行度做一定的权衡。</p><h2 id="13-rocksdb-大状态调优"><a class="markdownIt-Anchor" href="#13-rocksdb-大状态调优"></a> 1.3 RocksDB 大状态调优</h2><p>RocksDB 是基于 LSM Tree 实现的（类似 HBase），写数据都是先缓存到内存中，所以 RocksDB 的写请求效率比较高。RocksDB 使用内存结合磁盘的方式来存储数据，每次获取数据时，先从内存中 blockcache 中查找，如果内存中没有再去磁盘中查询。优化后差不多单并行度 TPS 5000 record/s，性能瓶颈主要在于 RocksDB 对磁盘的读请求，所以当处理性能不够时，仅需要横向扩展并行度即可提高整个 Job 的吞吐量。以下几个调优参数：</p><ul><li><strong>设置本地 RocksDB 多目录</strong></li></ul><p>在 flink-conf.yaml 中配置：<br />state.backend.rocksdb.localdir: /data1/flink/rocksdb,/data2/flink/rocksdb,/data3/flink/rocksdb<br />注意：不要配置单块磁盘的多个目录，务必将目录配置到多块不同的磁盘上，让多块磁盘来分担压力。当设置多个 RocksDB 本地磁盘目录时，Flink 会随机选择要使用的目录，所以就可能存在三个并行度共用同一目录的情况。如果服务器磁盘数较多，一般不会出现该情况，但是如果任务重启后吞吐量较低，可以检查是否发生了多个并行度共用同一块磁盘的情况。<br />当一个 TaskManager 包含 3 个 slot 时，那么单个服务器上的三个并行度都对磁盘造成频繁读写，从而导致三个并行度的之间相互争抢同一个磁盘 io，这样务必导致三个并行度的吞吐量都会下降。设置多目录实现三个并行度使用不同的硬盘从而减少资源竞争。<br />如下所示是测试过程中磁盘的 IO 使用率，可以看出三个大状态算子的并行度分别对应了三块磁盘，这三块磁盘的 IO 平均使用率都保持在 45% 左右，IO 最高使用率几乎都是 100%，而其他磁盘的 IO 平均使用率相对低很多。由此可见使用 RocksDB 做为状态后端且有大状态的频繁读取时， 对磁盘 IO 性能消耗确实比较大。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954333939-677ae56c-8fc5-4759-b761-aa7337ea04b6.png#" alt="" /><br />如下图所示，其中两个并行度共用了 sdb 磁盘，一个并行度使用 sdj 磁盘。可以看到 sdb 磁盘的 IO 使用率已经达到了 91.6%，就会导致 sdb 磁盘对应的两个并行度吞吐量大大降低，从而使得整个 Flink 任务吞吐量降低。如果每个服务器上有一两块 SSD，强烈建议将 RocksDB 的本地磁盘目录配置到 SSD 的目录下，从 HDD 改为 SSD 对于性能的提升可能比配置 10 个优化参数更有效。<img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954334393-8661d86e-d236-4a5c-a65c-19c2cef08462.png#" alt="" /></p><ul><li>**state.backend.incremental：**开启增量检查点，默认 false，改为 true。</li><li>**state.backend.rocksdb.predefined-options：**SPINNING_DISK_OPTIMIZED_HIGH_MEM 设置为机械硬盘+内存模式，有条件上 SSD，指定为 FLASH_SSD_OPTIMIZED</li><li><strong>state.backend.rocksdb.block.cache-size</strong>: 整个 RocksDB 共享一个 block cache，读数据时内存的 cache 大小，该参数越大读数据时缓存命中率越高，默认大小为 8 MB，建议设置到 64 ~ 256 MB。</li><li><strong>state.backend.rocksdb.thread.num</strong>: 用于后台 flush 和合并 sst 文件的线程数，默认为 1，建议调大，机械硬盘用户可以改为 4 等更大的值。</li><li><strong>state.backend.rocksdb.writebuffer.size</strong>: RocksDB 中，每个 State 使用一个 Column Family，每个 Column Family 使用独占的 write buffer，建议调大，例如：32M</li><li><strong>state.backend.rocksdb.writebuffer.count</strong>: 每个 Column Family 对应的 writebuffer 数目，默认值是 2，对于机械磁盘来说，如果内存⾜够大，可以调大到 5 左右</li><li><strong>state.backend.rocksdb.writebuffer.number-to-merge</strong>: 将数据从 writebuffer 中 flush 到磁盘时，需要合并的 writebuffer 数量，默认值为 1，可以调成 3。</li><li><strong>state.backend.local-recovery</strong>: 设置本地恢复，当 Flink 任务失败时，可以基于本地的状态信息进行恢复任务，可能不需要从 hdfs 拉取数据</li></ul><h2 id="14-checkpoint-设置"><a class="markdownIt-Anchor" href="#14-checkpoint-设置"></a> 1.4 Checkpoint 设置</h2><p>一般我们的 Checkpoint 时间间隔可以设置为分钟级别，例如 1 分钟、3 分钟，对于状态很大的任务每次 Checkpoint 访问 HDFS 比较耗时，可以设置为 5~10 分钟一次 Checkpoint，并且调大两次 Checkpoint 之间的暂停间隔，例如设置两次 Checkpoint 之间至少暂停 4 或 8 分钟。<br />如果 Checkpoint 语义配置为 EXACTLY_ONCE，那么在 Checkpoint 过程中还会存在 barrier 对齐的过程，可以通过 Flink Web UI 的 Checkpoint 选项卡来查看 Checkpoint 过程中各阶段的耗时情况，从而确定到底是哪个阶段导致 Checkpoint 时间过长然后针对性的解决问题。<br />RocksDB 相关参数在 1.3 中已说明，可以在 flink-conf.yaml 指定，也可以在 Job 的代码中调用 API 单独指定，这里不再列出。<br />// 使⽤ RocksDBStateBackend 做为状态后端，并开启增量 Checkpoint<br />RocksDBStateBackend rocksDBStateBackend = new RocksDBStateBackend(“hdfs://hadoop1:8020/flink/checkpoints”, true);<br />env.setStateBackend(rocksDBStateBackend);</p><p>// 开启 Checkpoint，间隔为 3 分钟<br />env.enableCheckpointing(TimeUnit.MINUTES.toMillis(3));<br />// 配置 Checkpoint<br />CheckpointConfig checkpointConf = env.getCheckpointConfig();<br />checkpointConf.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE)<br />// 最小间隔 4 分钟<br />checkpointConf.setMinPauseBetweenCheckpoints(TimeUnit.MINUTES.toMillis(4))<br />// 超时时间 10 分钟<br />checkpointConf.setCheckpointTimeout(TimeUnit.MINUTES.toMillis(10));<br />// 保存 checkpoint<br />checkpointConf.enableExternalizedCheckpoints(<br />CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);</p><h2 id="15-使用-flink-parametertool-读取配置"><a class="markdownIt-Anchor" href="#15-使用-flink-parametertool-读取配置"></a> 1.5 使用 Flink ParameterTool 读取配置</h2><p>在实际开发中，有各种环境（开发、测试、预发、生产），作业也有很多的配置：算子的并行度配置、Kafka 数据源的配置（broker 地址、topic 名、<a href="http://group.id">group.id</a>）、Checkpoint 是否开启、状态后端存储路径、数据库地址、用户名和密码等各种各样的配置，可能每个环境的这些配置对应的值都是不一样的。<br />如果你是直接在代码⾥⾯写死的配置，每次换个环境去运行测试作业，都要重新去修改代码中的配置，然后编译打包，提交运行，这样就要花费很多时间在这些重复的劳动力上了。在 Flink 中可以通过使用 ParameterTool 类读取配置，它可以读取环境变量、运行参数、配置文件。<br />ParameterTool 是可序列化的，所以你可以将它当作参数进行传递给算子的自定义函数类。</p><h3 id="151-读取运行参数"><a class="markdownIt-Anchor" href="#151-读取运行参数"></a> 1.5.1 读取运行参数</h3><p>我们可以在 Flink 的提交脚本添加运行参数，格式：</p><ul><li>–参数名 参数值</li><li>-参数名 参数值</li></ul><p>在 Flink 程序中可以直接使用 ParameterTool.fromArgs(args) 获取到所有的参数，也可以通过 parameterTool.get(“username”) 方法获取某个参数对应的值。<br />举例：通过运行参数指定 jobname<br />bin/flink run <br />-t yarn-per-job <br />-d <br />-p 5 \ 指定并行度<br />-Dyarn.application.queue=test \ 指定 yarn 队列<br />-Djobmanager.memory.process.size=1024mb \ 指定 JM 的总进程大小<br />-Dtaskmanager.memory.process.size=1024mb \ 指定每个 TM 的总进程大小<br />-Dtaskmanager.numberOfTaskSlots=2 \ 指定每个 TM 的 slot 数<br />-c com.atguigu.app.dwd.LogBaseApp <br />/opt/module/gmall-flink/gmall-realtime-1.0-SNAPSHOT-jar-with-dependencies.jar <br />–jobname dwd-LogBaseApp //参数名自己随便起，代码里对应上即可<br />在代码里获取参数值：<br />ParameterTool parameterTool = ParameterTool.fromArgs(args);<br />String myJobname = parameterTool.get(“jobname”); //参数名对应<br />env.execute(myJobname);</p><h3 id="152-读取系统属性"><a class="markdownIt-Anchor" href="#152-读取系统属性"></a> 1.5.2 读取系统属性</h3><p>ParameterTool 还⽀持通过 ParameterTool.fromSystemProperties() 方法读取系统属性。做个打印：<br />ParameterTool parameterTool = ParameterTool.fromSystemProperties();<br />System.out.println(parameterTool.toMap().toString());<br />可以得到全面的系统属性，部分结果：<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954334727-7c9b0c67-4ed7-4b70-898c-915312907c6c.png#" alt="" /></p><h3 id="153-读取配置文件"><a class="markdownIt-Anchor" href="#153-读取配置文件"></a> 1.5.3 读取配置文件</h3><p>可以使用 ParameterTool.fromPropertiesFile(&quot;/application.properties&quot;) 读取 properties 配置文件。可以将所有要配置的地方（比如并行度和一些 Kafka、MySQL 等配置）都写成可配置的，然后其对应的 key 和 value 值都写在配置文件中，最后通过 ParameterTool 去读取配置文件获取对应的值。</p><h3 id="154-注册全局参数"><a class="markdownIt-Anchor" href="#154-注册全局参数"></a> 1.5.4 注册全局参数</h3><p>在 ExecutionConfig 中可以将 ParameterTool 注册为全作业参数的参数，这样就可以被 JobManager 的 web 端以及用户⾃定义函数中以配置值的形式访问。<br />StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.getConfig().setGlobalJobParameters(ParameterTool.fromArgs(args));<br />可以不用将 ParameterTool 当作参数传递给算子的自定义函数，直接在用户⾃定义的 Rich 函数中直接获取到参数值了。<br />env.addSource(new RichSourceFunction() {<br />@Override<br />public void run(SourceContext sourceContext) throws Exception {<br />while (true) {<br />ParameterTool parameterTool = (ParameterTool)getRuntimeContext().getExecutionConfig().getGlobalJobParameters();<br />}<br />}<br />@Override<br />public void cancel() {<br />}<br />})</p><h2 id="16-压测方式"><a class="markdownIt-Anchor" href="#16-压测方式"></a> 1.6 压测方式</h2><pre><code>压测的方式很简单，先在kafka中积压数据，之后开启Flink任务，出现反压，就是处理瓶颈。相当于水库先积水，一下子泄洪。数据可以是自己造的模拟数据，也可以是生产中的部分数据。</code></pre><h1 id="2-反压处理"><a class="markdownIt-Anchor" href="#2-反压处理"></a> 2 反压处理</h1><p>反压（BackPressure）通常产生于这样的场景：短时间的负载高峰导致系统接收数据的速率远高于它处理数据的速率。许多日常问题都会导致反压，例如，垃圾回收停顿可能会导致流入的数据快速堆积，或遇到大促、秒杀活动导致流量陡增。反压如果不能得到正确的处理，可能会导致资源耗尽甚至系统崩溃。<br />反压机制是指系统能够自己检测到被阻塞的 Operator，然后自适应地降低源头或上游数据的发送速率，从而维持整个系统的稳定。Flink 任务一般运行在多个节点上，数据从上游算子发送到下游算子需要网络传输，若系统在反压时想要降低数据源头或上游算子数据的发送速率，那么肯定也需要网络传输。所以下面先来了解一下 Flink 的网络流控（Flink 对网络数据流量的控制）机制。</p><h2 id="21-反压现象及定位"><a class="markdownIt-Anchor" href="#21-反压现象及定位"></a> 2.1 反压现象及定位</h2><p>Flink 的反压太过于天然了，导致无法简单地通过监控 BufferPool 的使用情况来判断反压状态。Flink 通过对运行中的任务进行采样来确定其反压，如果一个 Task 因为反压导致处理速度降低了，那么它肯定会卡在向 LocalBufferPool 申请内存块上。那么该 Task 的 stack trace 应该是这样：<br />java.lang.Object.wait(Native Method)<br />o.a.f.[…].LocalBufferPool.requestBuffer(LocalBufferPool.java:163) o.a.f.[…].LocalBufferPool.requestBufferBlocking(LocalBufferPool.java:133) […]<br />监控对正常的任务运行有一定影响，因此只有当 Web 页面切换到 Job 的 BackPressure 页面时，JobManager 才会对该 Job 触发反压监控。默认情况下，JobManager 会触发 100 次 stack trace 采样，每次间隔 50ms 来确定反压。Web 界面看到的比率表示在内部方法调用中有多少 stack trace 被卡在 LocalBufferPool.requestBufferBlocking()，例如: 0.01 表示在 100 个采样中只有 1 个被卡在 LocalBufferPool.requestBufferBlocking()。采样得到的比例与反压状态的对应关系如下：</p><ul><li>OK: 0 &lt;= 比例 &lt;= 0.10</li><li>LOW: 0.10 &lt; 比例 &lt;= 0.5</li><li>HIGH: 0.5 &lt; 比例 &lt;= 1</li></ul><p>Task 的状态为 OK 表示没有反压，HIGH 表示这个 Task 被反压。</p><h3 id="211-利用-flink-web-ui-定位产生反压的位置"><a class="markdownIt-Anchor" href="#211-利用-flink-web-ui-定位产生反压的位置"></a> 2.1.1 利用 Flink Web UI 定位产生反压的位置</h3><p>在 Flink Web UI 中有 BackPressure 的页面，通过该页面可以查看任务中 subtask 的反压状态，如下两图所示，分别展示了状态是 OK 和 HIGH 的场景。<img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954335119-fc785a68-9530-4c27-a0d7-92519ee4aadd.png#" alt="" /><br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954335716-f7331d94-1cc4-4a32-8b6a-9b9a5edfcd8b.png#" alt="" /></p><h3 id="212-利用-metrics-定位反压位置"><a class="markdownIt-Anchor" href="#212-利用-metrics-定位反压位置"></a> 2.1.2 利用 Metrics 定位反压位置</h3><p>当某个 Task 吞吐量下降时，基于 Credit 的反压机制，上游不会给该 Task 发送数据，所以该 Task 不会频繁卡在向 Buffer Pool 去申请 Buffer。反压监控实现原理就是监控 Task 是否卡在申请 buffer 这一步，所以遇到瓶颈的 Task 对应的反压⻚⾯必然会显示 OK，即表示没有受到反压。<br />如果该 Task 吞吐量下降，造成该 Task 上游的 Task 出现反压时，必然会存在：该 Task 对应的 InputChannel 变满，已经申请不到可用的 Buffer 空间。如果该 Task 的 InputChannel 还能申请到可用 Buffer，那么上游就可以给该 Task 发送数据，上游 Task 也就不会被反压了，所以说遇到瓶颈且导致上游 Task 受到反压的 Task 对应的 InputChannel 必然是满的（这⾥不考虑⽹络遇到瓶颈的情况）。从这个思路出发，可以对该 Task 的 InputChannel 的使用情况进行监控，如果 InputChannel 使用率 100%，那么该 Task 就是我们要找的反压源。Flink 1.9 及以上版本 inPoolUsage 表示 inputFloatingBuffersUsage 和 inputExclusiveBuffersUsage 的总和。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954336194-5899a5b5-17ae-4e22-b026-7ea7ac92c5db.png#" alt="" /><br />反压时，可以看到遇到瓶颈的该 Task 的 inPoolUage 为 1。</p><h2 id="22-反压的原因及处理"><a class="markdownIt-Anchor" href="#22-反压的原因及处理"></a> 2.2 反压的原因及处理</h2><p>先检查基本原因，然后再深入研究更复杂的原因，最后找出导致瓶颈的原因。下面列出从最基本到比较复杂的一些反压潜在原因。<br />注意：反压可能是暂时的，可能是由于负载高峰、CheckPoint 或作业重启引起的数据积压而导致反压。如果反压是暂时的，应该忽略它。另外，请记住，断断续续的反压会影响我们分析和解决问题。</p><h3 id="221-系统资源"><a class="markdownIt-Anchor" href="#221-系统资源"></a> 2.2.1 系统资源</h3><p>检查涉及服务器基本资源的使用情况，如 CPU、网络或磁盘 I/O，目前 Flink 任务使用最主要的还是内存和 CPU 资源，本地磁盘、依赖的外部存储资源以及网卡资源一般都不会是瓶颈。如果某些资源被充分利用或大量使用，可以借助分析工具，分析性能瓶颈（JVM Profiler+ FlameGraph 生成火焰图）。<br />参考连接：<a href="http://www.54tianzhisheng.cn/2020/10/05/flink-jvm-profiler/">http://www.54tianzhisheng.cn/2020/10/05/flink-jvm-profiler/</a></p><ul><li>针对特定的资源调优 Flink</li><li>通过增加并行度或增加集群中的服务器数量来横向扩展</li><li>减少瓶颈算子上游的并行度，从而减少瓶颈算子接收的数据量（不建议，可能造成整个 Job 数据延迟增大）</li></ul><h3 id="222-垃圾收集gc"><a class="markdownIt-Anchor" href="#222-垃圾收集gc"></a> 2.2.2 垃圾收集（GC）</h3><p>长时间 GC 暂停会导致性能问题。可以通过打印调试 GC 日志（通过-XX:+PrintGCDetails）或使用某些内存或 GC 分析器（GCViewer 工具）来验证是否处于这种情况。</p><ul><li>在 Flink 提交脚本中,设置 JVM 参数，打印 GC 日志：</li></ul><p>bin/flink run <br />-t yarn-per-job <br />-d <br />-p 5 \ 指定并行度<br />-Dyarn.application.queue=test \ 指定 yarn 队列<br />-Djobmanager.memory.process.size=1024mb \ 指定 JM 的总进程大小<br />-Dtaskmanager.memory.process.size=1024mb \ 指定每个 TM 的总进程大小<br />-Dtaskmanager.numberOfTaskSlots=2 \ 指定每个 TM 的 slot 数<br />-Denv.java.opts=&quot;-XX:+PrintGCDetails -XX:+PrintGCDateStamps&quot;<br />-c com.atguigu.app.dwd.LogBaseApp <br />/opt/module/gmall-flink/gmall-realtime-1.0-SNAPSHOT-jar-with-dependencies.jar</p><ul><li>下载 GC 日志的方式：</li></ul><p>因为是 on yarn 模式，运行的节点一个一个找比较麻烦。可以打开 WebUI，选择 JobManager 或者 TaskManager，点击 Stdout，即可看到 GC 日志，点击下载按钮即可将 GC 日志通过 HTTP 的方式下载下来。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954336764-24ce1216-f5e4-41fc-b3c4-7295a00c3559.png#" alt="" /></p><ul><li>分析 GC 日志：</li></ul><p>通过 GC 日志分析出单个 Flink Taskmanager 堆总大小、年轻代、老年代分配的内存空间、Full GC 后老年代剩余大小等，相关指标定义可以去 Github 具体查看。<br />GCViewer 地址：<a href="https://github.com/chewiebug/GCViewer">https://github.com/chewiebug/GCViewer</a><br /><em>扩展：最重要的指标是<strong>Full GC 后老年代剩余大小</strong>这个指标，按照《Java 性能优化权威指南》这本书 Java 堆大小计算法则，设 Full GC 后老年代剩余大小空间为 M，那么堆的大小建议 3 ~ 4 倍 M，新生代为 1 ~ 1.5 倍 M，老年代应为 2 ~ 3 倍 M。</em></p><h3 id="223-cpu线程瓶颈"><a class="markdownIt-Anchor" href="#223-cpu线程瓶颈"></a> 2.2.3 CPU/线程瓶颈</h3><p>有时，一个或几个线程导致 CPU 瓶颈，而整个机器的 CPU 使用率仍然相对较低，则可能无法看到 CPU 瓶颈。例如，48 核的服务器上，单个 CPU 瓶颈的线程仅占用 2％的 CPU 使用率，就算单个线程发生了 CPU 瓶颈，我们也看不出来。可以考虑使用 2.2.1 提到的分析工具，它们可以显示每个线程的 CPU 使用情况来识别热线程。</p><h3 id="224-线程竞争"><a class="markdownIt-Anchor" href="#224-线程竞争"></a> 2.2.4 线程竞争</h3><p>与上⾯的 CPU/线程瓶颈问题类似，subtask 可能会因为共享资源上高负载线程的竞争而成为瓶颈。同样，可以考虑使用 2.2.1 提到的分析工具，考虑在用户代码中查找同步开销、锁竞争，尽管避免在用户代码中添加同步。</p><h3 id="225-负载不平衡"><a class="markdownIt-Anchor" href="#225-负载不平衡"></a> 2.2.5 负载不平衡</h3><p>如果瓶颈是由数据倾斜引起的，可以尝试通过将数据分区的 key 进行加盐或通过实现本地预聚合来减轻数据倾斜的影响。（关于数据倾斜的详细解决方案，会在下一章节详细讨论）</p><h3 id="226-外部依赖"><a class="markdownIt-Anchor" href="#226-外部依赖"></a> 2.2.6 外部依赖</h3><p>如果发现我们的 Source 端数据读取性能比较低或者 Sink 端写入性能较差，需要检查第三方组件是否遇到瓶颈。例如，Kafka 集群是否需要扩容，Kafka 连接器是否并行度较低，HBase 的 rowkey 是否遇到热点问题。关于第三方组件的性能问题，需要结合具体的组件来分析。</p><h1 id="3-数据倾斜"><a class="markdownIt-Anchor" href="#3-数据倾斜"></a> 3 数据倾斜</h1><h2 id="31-判断是否存在数据倾斜"><a class="markdownIt-Anchor" href="#31-判断是否存在数据倾斜"></a> 3.1 判断是否存在数据倾斜</h2><p>相同 Task 的多个 Subtask 中，个别 Subtask 接收到的数据量明显大于其他 Subtask 接收到的数据量，通过 Flink Web UI 可以精确地看到每个 Subtask 处理了多少数据，即可判断出 Flink 任务是否存在数据倾斜。通常，数据倾斜也会引起反压。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954337219-d638a96a-b547-41f3-9b06-8b87ce6f5c98.png#" alt="" /></p><h2 id="32-数据倾斜的解决"><a class="markdownIt-Anchor" href="#32-数据倾斜的解决"></a> 3.2 数据倾斜的解决</h2><h3 id="321-keyby-后的聚合操作存在数据倾斜"><a class="markdownIt-Anchor" href="#321-keyby-后的聚合操作存在数据倾斜"></a> 3.2.1 keyBy 后的聚合操作存在数据倾斜</h3><p>使用 LocalKeyBy 的思想：在 keyBy 上游算子数据发送之前，首先在上游算子的本地<br />对数据进行聚合后再发送到下游，使下游接收到的数据量大大减少，从而使得 keyBy 之后的聚合操作不再是任务的瓶颈。类似 MapReduce 中 Combiner 的思想，但是这要求聚合操作必须是多条数据或者一批数据才能聚合，单条数据没有办法通过聚合来减少数据量。从 Flink LocalKeyBy 实现原理来讲，必然会存在一个积攒批次的过程，在上游算子中必须攒够一定的数据量，对这些数据聚合后再发送到下游。<br />**注意：**Flink 是实时流处理，如果 keyby 之后的聚合操作存在数据倾斜，且没有开窗口的情况下，简单的认为使用两阶段聚合，是不能解决问题的。因为这个时候 Flink 是来一条处理一条，且向下游发送一条结果，对于原来 keyby 的维度（第二阶段聚合）来讲，数据量并没有减少，且结果重复计算（非 FlinkSQL，未使用回撤流），如下图所示：<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954337667-6668945e-1027-49a3-a7e4-8086fd64a6a2.png#" alt="" /></p><ul><li>**实现方式：**以计算 PV 为例，keyby 之前，使用 flatMap 实现 LocalKeyby</li></ul><p>class LocalKeyByFlatMap extends RichFlatMapFunction&lt;String, Tuple2&lt;String,<br />//Checkpoint 时为了保证 Exactly Once，将 buffer 中的数据保存到该 ListState 中<br />private ListState&lt;Tuple2&lt;String, Long&gt;&gt; localPvStatListState;<br />//本地 buffer，存放 local 端缓存的 app 的 pv 信息<br />private HashMap&lt;String, Long&gt; localPvStat;<br />//缓存的数据量大小，即：缓存多少数据再向下游发送<br />private int batchSize;<br />//计数器，获取当前批次接收的数据量<br />private AtomicInteger currentSize;</p><p>//构造器，批次大小传参<br />LocalKeyByFlatMap(int batchSize){<br />this.batchSize = batchSize;<br />}</p><p>@Override<br />public void flatMap(String in, Collector collector) throws Exception {<br />// 将新来的数据添加到 buffer 中 zhisheng<br />Long pv = localPvStat.getOrDefault(in, 0L);<br />localPvStat.put(in, pv + 1);<br />// 如果到达设定的批次，则将 buffer 中的数据发送到下游<br />if(currentSize.incrementAndGet() &gt;= batchSize){<br />// 遍历 Buffer 中数据，发送到下游<br />for(Map.Entry&lt;String, Long&gt; appIdPv: localPvStat.entrySet()) {<br />collector.collect(Tuple2.of(appIdPv.getKey(), appIdPv.getVa<br />}<br />// Buffer 清空，计数器清零<br />localPvStat.clear();<br />currentSize.set(0);<br />}<br />}</p><p>@Override<br />public void snapshotState(FunctionSnapshotContext functionSnapshotConte<br />// 将 buffer 中的数据保存到状态中，来保证 Exactly Once<br />localPvStatListState.clear();<br />for(Map.Entry&lt;String, Long&gt; appIdPv: localPvStat.entrySet()) {<br />localPvStatListState.add(Tuple2.of(appIdPv.getKey(), <a href="http://appIdPv.ge">appIdPv.ge</a><br />}<br />}</p><p>@Override<br />public void initializeState(FunctionInitializationContext context) {<br />// 从状态中恢复 buffer 中的数据<br />localPvStatListState = context.getOperatorStateStore().getListState<br />new ListStateDescriptor&lt;&gt;(“localPvStat”,<br />TypeInformation.of(new TypeHint&lt;Tuple2&lt;String, Long&gt;&gt;})));<br />localPvStat = new HashMap();<br />if(context.isRestored()) {<br />// 从状态中恢复数据到 localPvStat 中<br />for(Tuple2&lt;String, Long&gt; appIdPv: localPvStatListState.get()){<br />long pv = localPvStat.getOrDefault(appIdPv.f0, 0L);<br />// 如果出现 pv != 0,说明改变了并行度，<br />// ListState 中的数据会被均匀分发到新的 subtask 中<br />// 所以单个 subtask 恢复的状态中可能包含两个相同的 app 的数据<br />localPvStat.put(appIdPv.f0, pv + appIdPv.f1);<br />}<br />// 从状态恢复时，默认认为 buffer 中数据量达到了 batchSize，需要向下游发<br />currentSize = new AtomicInteger(batchSize);<br />} else {<br />currentSize = new AtomicInteger(0);<br />}<br />}</p><p>}</p><h3 id="322-keyby-之前发生数据倾斜"><a class="markdownIt-Anchor" href="#322-keyby-之前发生数据倾斜"></a> 3.2.2 keyBy 之前发生数据倾斜</h3><p>如果 keyBy 之前就存在数据倾斜，上游算子的某些实例可能处理的数据较多，某些实例可能处理的数据较少，产生该情况可能是因为数据源的数据本身就不均匀，例如由于某些原因 Kafka 的 topic 中某些 partition 的数据量较大，某些 partition 的数据量较少。对于不存在 keyBy 的 Flink 任务也会出现该情况。<br />这种情况，需要让 Flink 任务强制进行 shuffle。使用 shuffle、rebalance 或 rescale 算子即可将数据均匀分配，从而解决数据倾斜的问题。</p><h3 id="323-keyby-后的窗口聚合操作存在数据倾斜"><a class="markdownIt-Anchor" href="#323-keyby-后的窗口聚合操作存在数据倾斜"></a> 3.2.3 keyBy 后的窗口聚合操作存在数据倾斜</h3><p>因为使用了窗口，变成了有界数据的处理（3.2.1 已分析过），窗口默认是触发时才会输出一条结果发往下游，所以可以使用两阶段聚合的方式：<br /><strong>实现思路：</strong></p><ul><li>第一阶段聚合：key 拼接随机数前缀或后缀，进行 keyby、开窗、聚合</li></ul><p><em>注意：__聚合完不再是 WindowedStream，要获取 WindowEnd 作为窗口标记作为第二阶段分组依据，避免不同窗口的结果聚合到一起）</em></p><ul><li>第二阶段聚合：去掉随机数前缀或后缀，按照原来的 key 及 windowEnd 作 keyby、聚合</li></ul><h1 id="4-kafkasource-调优"><a class="markdownIt-Anchor" href="#4-kafkasource-调优"></a> 4 KafkaSource 调优</h1><h2 id="41-动态发现分区"><a class="markdownIt-Anchor" href="#41-动态发现分区"></a> 4.1 动态发现分区</h2><p>当 FlinkKafkaConsumer 初始化时，每个 subtask 会订阅一批 partition，但是当 Flink 任务运行过程中，如果被订阅的 topic 创建了新的 partition，FlinkKafkaConsumer 如何实现动态发现新创建的 partition 并消费呢？<br />在使用 FlinkKafkaConsumer 时，可以开启 partition 的动态发现。通过 Properties 指定参数开启（单位是毫秒）：<br />FlinkKafkaConsumerBase.KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS<br />该参数表示间隔多久检测一次是否有新创建的 partition。默认值是 Long 的最小值，表示不开启，大于 0 表示开启。开启时会启动一个线程根据传入的 interval 定期获取 Kafka 最新的元数据，新 partition 对应的那一个 subtask 会自动发现并从 earliest 位置开始消费，新创建的 partition 对其他 subtask 并不会产生影响。<br />代码如下所示：<br />properties.setProperty(FlinkKafkaConsumerBase.KEY_PARTITION_DISCOVERY_INTERVAL_MILLIS, 30 * 1000 + “”);</p><h2 id="42-从-kafka-数据源生成-watermark"><a class="markdownIt-Anchor" href="#42-从-kafka-数据源生成-watermark"></a> 4.2 从 kafka 数据源生成 watermark</h2><p>Kafka 单分区内有序，多分区间无序。在这种情况下，可以使用 Flink 中可识别 Kafka 分区的 watermark 生成机制。使用此特性，将在 Kafka 消费端内部针对每个 Kafka 分区生成 watermark，并且不同分区 watermark 的合并方式与在数据流 shuffle 时的合并方式相同。<br />在单分区内有序的情况下，使用时间戳单调递增按分区生成的 watermark 将生成完美的全局 watermark。<br />可以不使用  TimestampAssigner，直接用 Kafka 记录自身的时间戳：<br />StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</p><p>Properties properties = new Properties();<br />properties.setProperty(“bootstrap.servers”, “hadoop1:9092,hadoop2:9092,hadoop3:9092”);<br />properties.setProperty(“<a href="http://group.id">group.id</a>”, “fffffffffff”);</p><p>FlinkKafkaConsumer<String> kafkaSourceFunction = new FlinkKafkaConsumer&lt;&gt;(<br />“flinktest”,<br />new SimpleStringSchema(),<br />properties<br />);</p><p>kafkaSourceFunction.assignTimestampsAndWatermarks(<br />WatermarkStrategy<br />.forBoundedOutOfOrderness(Duration.ofMinutes(2))<br />);</p><p>env.addSource(kafkaSourceFunction)</p><h2 id="43-设置空闲等待"><a class="markdownIt-Anchor" href="#43-设置空闲等待"></a> 4.3 设置空闲等待</h2><p>如果数据源中的某一个分区/分片在一段时间内未发送事件数据，则意味着 WatermarkGenerator 也不会获得任何新数据去生成 watermark。我们称这类数据源为空闲输入或空闲源。在这种情况下，当某些其他分区仍然发送事件数据的时候就会出现问题。比如 Kafka 的 Topic 中，由于某些原因，造成个别 Partition 一直没有新的数据。由于下游算子 watermark 的计算方式是取所有不同的上游并行数据源 watermark 的最小值，则其 watermark 将不会发生变化，导致窗口、定时器等不会被触发。<br />为了解决这个问题，你可以使用 WatermarkStrategy 来检测空闲输入并将其标记为空闲状态。<br />StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</p><p>Properties properties = new Properties();<br />properties.setProperty(“bootstrap.servers”, “hadoop1:9092,hadoop2:9092,hadoop3:9092”);<br />properties.setProperty(“<a href="http://group.id">group.id</a>”, “fffffffffff”);</p><p>FlinkKafkaConsumer<String> kafkaSourceFunction = new FlinkKafkaConsumer&lt;&gt;(<br />“flinktest”,<br />new SimpleStringSchema(),<br />properties<br />);</p><p>kafkaSourceFunction.assignTimestampsAndWatermarks(<br />WatermarkStrategy<br />.forBoundedOutOfOrderness(Duration.ofMinutes(2))<br />.withIdleness(Duration.ofMinutes(5))<br />);</p><p>env.addSource(kafkaSourceFunction)</p><h2 id="44-kafka-的-offset-消费策略"><a class="markdownIt-Anchor" href="#44-kafka-的-offset-消费策略"></a> 4.4 Kafka 的 offset 消费策略</h2><p>FlinkKafkaConsumer 可以调用以下 API，注意与”auto.offset.reset”区分开：</p><ul><li>setStartFromGroupOffsets()：默认消费策略，默认读取上次保存的 offset 信息，如果是应用第一次启动，读取不到上次的 offset 信息，则会根据这个参数 auto.offset.reset 的值来进行消费数据。建议使用这个。</li><li>setStartFromEarliest()：从最早的数据开始进行消费，忽略存储的 offset 信息</li><li>setStartFromLatest()：从最新的数据进行消费，忽略存储的 offset 信息</li><li>setStartFromSpecificOffsets(Map)：从指定位置进行消费</li><li>setStartFromTimestamp(long)：从 topic 中指定的时间点开始消费，指定时间点之前的数据忽略</li><li>当 checkpoint 机制开启的时候，KafkaConsumer 会定期把 kafka 的 offset 信息还有其他 operator 的状态信息一块保存起来。当 job 失败重启的时候，Flink 会从最近一次的 checkpoint 中进行恢复数据，重新从保存的 offset 消费 kafka 中的数据（也就是说，上面几种策略，只有第一次启动的时候起作用）。</li><li>为了能够使用支持容错的 kafka Consumer，需要开启 checkpoint</li></ul><h1 id="5-flinksql-调优"><a class="markdownIt-Anchor" href="#5-flinksql-调优"></a> 5 FlinkSQL 调优</h1><p>FlinkSQL 官网配置参数：<br /><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/config.html">https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/config.html</a></p><h2 id="51-group-aggregate-优化"><a class="markdownIt-Anchor" href="#51-group-aggregate-优化"></a> 5.1 Group Aggregate 优化</h2><h3 id="511-开启-minibatch提升吞吐"><a class="markdownIt-Anchor" href="#511-开启-minibatch提升吞吐"></a> 5.1.1 开启 MiniBatch（提升吞吐）</h3><p>MiniBatch 是微批处理，原理是缓存一定的数据后再触发处理，以减少对 State 的访问，从而提升吞吐并减少数据的输出量。MiniBatch 主要依靠在每个 Task 上注册的 Timer 线程来触发微批，需要消耗一定的线程调度性能。</p><ul><li><strong>MiniBatch 默认关闭，开启方式如下:</strong></li></ul><p>// 初始化 table environment<br />TableEnvironment tEnv = …</p><p>// 获取 tableEnv 的配置对象<br />Configuration configuration = tEnv.getConfig().getConfiguration();</p><p>// 设置参数：<br />// 开启 miniBatch<br />configuration.setString(“table.exec.mini-batch.enabled”, “true”);<br />// 批量输出的间隔时间<br />configuration.setString(“table.exec.mini-batch.allow-latency”, “5 s”);<br />// 防止 OOM 设置每个批次最多缓存数据的条数，可以设为 2 万条<br />configuration.setString(“table.exec.mini-batch.size”, “20000”);</p><ul><li>FlinkSQL 参数配置列表：</li></ul><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/config.html">https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/config.html</a></p><ul><li>适用场景</li></ul><p>微批处理通过增加延迟换取高吞吐，如果有超低延迟的要求，不建议开启微批处理。通常对于聚合的场景，微批处理可以显著的提升系统性能，建议开启。</p><ul><li><strong>注意事项：</strong></li></ul><p>1）目前，key-value 配置项仅被 Blink planner 支持。<br />2）1.12 之前的版本有 bug，开启 miniBatch，不会清理过期状态，也就是说如果设置状态的 TTL，无法清理过期状态。1.12 版本才修复这个问题。<br />参考 ISSUE：<a href="https://issues.apache.org/jira/browse/FLINK-17096">https://issues.apache.org/jira/browse/FLINK-17096</a></p><h3 id="512-开启-localglobal解决常见数据热点问题"><a class="markdownIt-Anchor" href="#512-开启-localglobal解决常见数据热点问题"></a> 5.1.2 开启 LocalGlobal（解决常见数据热点问题）</h3><p>LocalGlobal 优化将原先的 Aggregate 分成 Local+Global 两阶段聚合，即 MapReduce 模型中的 Combine+Reduce 处理模式。第一阶段在上游节点本地攒一批数据进行聚合（localAgg），并输出这次微批的增量值（Accumulator）。第二阶段再将收到的 Accumulator 合并（Merge），得到最终的结果（GlobalAgg）。<br />LocalGlobal 本质上能够靠 LocalAgg 的聚合筛除部分倾斜数据，从而降低 GlobalAgg 的热点，提升性能。结合下图理解 LocalGlobal 如何解决数据倾斜的问题。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954338358-ec453893-dab7-49c8-a79f-86ff956fe44b.png#" alt="" /><br />由上图可知：</p><ul><li>未开启 LocalGlobal 优化，由于流中的数据倾斜，Key 为红色的聚合算子实例需要处理更多的记录，这就导致了热点问题。</li><li>开启 LocalGlobal 优化后，先进行本地聚合，再进行全局聚合。可大大减少 GlobalAgg 的热点，提高性能。</li><li><strong>LocalGlobal 开启方式：</strong></li></ul><p>1）LocalGlobal 优化需要先开启 MiniBatch，依赖于 MiniBatch 的参数。<br />2）table.optimizer.agg-phase-strategy: 聚合策略。默认 AUTO，支持参数 AUTO、TWO_PHASE(使用 LocalGlobal 两阶段聚合)、ONE_PHASE(仅使用 Global 一阶段聚合)。<br />// 初始化 table environment<br />TableEnvironment tEnv = …</p><p>// 获取 tableEnv 的配置对象<br />Configuration configuration = tEnv.getConfig().getConfiguration();</p><p>// 设置参数：<br />// 开启 miniBatch<br />configuration.setString(“table.exec.mini-batch.enabled”, “true”);<br />// 批量输出的间隔时间<br />configuration.setString(“table.exec.mini-batch.allow-latency”, “5 s”);<br />// 防止 OOM 设置每个批次最多缓存数据的条数，可以设为 2 万条<br />configuration.setString(“table.exec.mini-batch.size”, “20000”);<br />// 开启 LocalGlobal<br />configuration.setString(“table.optimizer.agg-phase-strategy”, “TWO_PHASE”);</p><ul><li>判断是否生效</li></ul><p>观察最终生成的拓扑图的节点名字中是否包含 GlobalGroupAggregate 或 LocalGroupAggregate。</p><ul><li>适用场景</li></ul><p>LocalGlobal 适用于提升如 SUM、COUNT、MAX、MIN 和 AVG 等普通聚合的性能，以及解决这些场景下的数据热点问题。</p><ul><li><strong>注意事项：</strong></li></ul><p>1）需要先开启 MiniBatch<br />2）开启 LocalGlobal 需要 UDAF 实现 Merge 方法。</p><h3 id="513-开启-split-distinct解决-count-distinct-热点问题"><a class="markdownIt-Anchor" href="#513-开启-split-distinct解决-count-distinct-热点问题"></a> 5.1.3 开启 Split Distinct（解决 COUNT DISTINCT 热点问题）</h3><p>LocalGlobal 优化针对普通聚合（例如 SUM、COUNT、MAX、MIN 和 AVG）有较好的效果，对于 COUNT DISTINCT 收效不明显，因为 COUNT DISTINCT 在 Local 聚合时，对于 DISTINCT KEY 的去重率不高，导致在 Global 节点仍然存在热点。<br />之前，为了解决 COUNT DISTINCT 的热点问题，通常需要手动改写为两层聚合（增加按 Distinct Key 取模的打散层）。<br />从 Flink1.9.0 版本开始，提供了 COUNT DISTINCT 自动打散功能，不需要手动重写。Split Distinct 和 LocalGlobal 的原理对比参见下图。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954339118-2a4c5b1d-5fbd-4b28-8eaf-ccbf491c744c.png#" alt="" /><br /><strong>举例：统计一天的 UV</strong><br />SELECT day, COUNT(DISTINCT user_id)<br />FROM T<br />GROUP BY day<br /><strong>如果手动实现两阶段聚合：</strong><br />SELECT day, SUM(cnt)<br />FROM (<br />SELECT day, COUNT(DISTINCT user_id) as cnt<br />FROM T<br />GROUP BY day, MOD(HASH_CODE(user_id), 1024)<br />)<br />GROUP BY day<br />第一层聚合: 将 Distinct Key 打散求 COUNT DISTINCT。<br />第二层聚合: 对打散去重后的数据进行 SUM 汇总。</p><ul><li><strong>Split Distinct 开启方式</strong></li></ul><p>默认不开启，使用参数显式开启：</p><ul><li>table.optimizer.distinct-agg.split.enabled: true，默认 false。</li><li>table.optimizer.distinct-agg.split.bucket-num: Split Distinct 优化在第一层聚合中，被打散的 bucket 数目。默认 1024。</li></ul><p>// 初始化 table environment<br />TableEnvironment tEnv = …</p><p>// 获取 tableEnv 的配置对象<br />Configuration configuration = tEnv.getConfig().getConfiguration();</p><p>// 设置参数：<br />// 开启 Split Distinct<br />configuration.setString(“table.optimizer.distinct-agg.split.enabled”, “true”);<br />// 第一层打散的 bucket 数目<br />configuration.setString(“table.optimizer.distinct-agg.split.bucket-num”, “1024”);</p><ul><li>判断是否生效</li></ul><p>观察最终生成的拓扑图的节点名中是否包含 Expand 节点，或者原来一层的聚合变成了两层的聚合。</p><ul><li>适用场景</li></ul><p>使用 COUNT DISTINCT，但无法满足聚合节点性能要求。</p><ul><li><strong>注意事项：</strong></li></ul><p>1）目前不能在包含 UDAF 的 Flink SQL 中使用 Split Distinct 优化方法。<br />2）拆分出来的两个 GROUP 聚合还可参与 LocalGlobal 优化。<br />3）从 Flink1.9.0 版本开始，提供了 COUNT DISTINCT 自动打散功能，不需要手动重写（不用像上面的例子去手动实现）。</p><h3 id="514-改写为-agg-with-filter-语法提升大量-count-distinct-场景性能"><a class="markdownIt-Anchor" href="#514-改写为-agg-with-filter-语法提升大量-count-distinct-场景性能"></a> 5.1.4 改写为 AGG WITH FILTER 语法（提升大量 COUNT DISTINCT 场景性能）</h3><p>在某些场景下，可能需要从不同维度来统计 UV，如 Android 中的 UV，iPhone 中的 UV，Web 中的 UV 和总 UV，这时，可能会使用如下 CASE WHEN 语法。<br />SELECT<br />day,<br />COUNT(DISTINCT user_id) AS total_uv,<br />COUNT(DISTINCT CASE WHEN flag IN (‘android’, ‘iphone’) THEN user_id ELSE NULL END) AS app_uv,<br />COUNT(DISTINCT CASE WHEN flag IN (‘wap’, ‘other’) THEN user_id ELSE NULL END) AS web_uv<br />FROM T<br />GROUP BY day<br />在这种情况下，建议使用 FILTER 语法, 目前的 Flink SQL 优化器可以识别同一唯一键上的不同 FILTER 参数。如，在上面的示例中，三个 COUNT DISTINCT 都作用在 user_id 列上。此时，经过优化器识别后，Flink 可以只使用一个共享状态实例，而不是三个状态实例，可减少状态的大小和对状态的访问。<br /><strong>将上边的 CASE WHEN 替换成 FILTER 后</strong>，如下所示:<br />SELECT<br />day,<br />COUNT(DISTINCT user_id) AS total_uv,<br />COUNT(DISTINCT user_id) FILTER (WHERE flag IN (‘android’, ‘iphone’)) AS app_uv,<br />COUNT(DISTINCT user_id) FILTER (WHERE flag IN (‘wap’, ‘other’)) AS web_uv<br />FROM T<br />GROUP BY day</p><h2 id="52-topn-优化"><a class="markdownIt-Anchor" href="#52-topn-优化"></a> 5.2 TopN 优化</h2><h3 id="521-使用最优算法"><a class="markdownIt-Anchor" href="#521-使用最优算法"></a> 5.2.1 使用最优算法</h3><p>当 TopN 的输入是非更新流（例如 Source），TopN 只有一种算法 AppendRank。当 TopN 的输入是更新流时（例如经过了 AGG/JOIN 计算），TopN 有 2 种算法，性能从高到低分别是：UpdateFastRank 和 RetractRank。算法名字会显示在拓扑图的节点名字上。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954339430-3b6c5840-a25a-4ffc-b8e3-b9f47a135de0.png#" alt="" /><br />注意：apache 社区版的 Flink1.12 目前还没有 UnaryUpdateRank，阿里云实时计算版 Flink 才有<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954339765-932d1e53-ed38-4739-b7ef-98ab4ef3eed7.png#" alt="" /><br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954340109-13c71aaf-e570-45ef-827e-93dfcc6953d7.png#" alt="" /></p><ul><li><strong>UpdateFastRank ：最优算法</strong></li></ul><p>需要具备 2 个条件：<br />1）输入流有 PK（Primary Key）信息，例如 ORDER BY AVG。<br />2）排序字段的更新是单调的，且单调方向与排序方向相反。例如，ORDER BY COUNT/COUNT_DISTINCT/SUM（正数）DESC。<br />如果要获取到优化 Plan，则您需要在使用 ORDER BY SUM DESC 时，添加 SUM 为正数的过滤条件。</p><ul><li><strong>AppendFast：结果只追加，不更新</strong></li><li><strong>RetractRank：普通算法，性能差</strong></li></ul><p>不建议在生产环境使用该算法。请检查输入流是否存在 PK 信息，如果存在，则可进行 UpdateFastRank 优化。</p><h3 id="522-无排名优化解决数据膨胀问题"><a class="markdownIt-Anchor" href="#522-无排名优化解决数据膨胀问题"></a> 5.2.2 无排名优化（解决数据膨胀问题）</h3><ul><li><strong>TopN 语法：</strong></li></ul><p>SELECT _<br />FROM (<br />SELECT _,<br />ROW_NUMBER() OVER ([PARTITION BY col1[, col2…]]<br />ORDER BY col1 [asc|desc], col2 [asc|desc]…]) AS rownum<br />FROM table_name)<br />WHERE rownum &lt;= N [AND conditions]</p><ul><li><strong>数据膨胀问题：</strong></li></ul><p>根据 TopN 的语法，rownum 字段会作为结果表的主键字段之一写入结果表。但是这可能导致数据膨胀的问题。例如，收到一条原排名 9 的更新数据，更新后排名上升到 1，则从 1 到 9 的数据排名都发生变化了，需要将这些数据作为更新都写入结果表。这样就产生了数据膨胀，导致结果表因为收到了太多的数据而降低更新速度。</p><ul><li>**使用方式 **</li></ul><p>TopN 的输出结果无需要显示 rownum 值，仅需在最终前端显式时进行 1 次排序，极大地减少输入结果表的数据量。只需要在外层查询中将 rownum 字段裁剪掉即可<br />// 最外层的字段，不写 rownum<br />SELECT col1, col2, col3<br />FROM (<br />SELECT col1, col2, col3<br />ROW_NUMBER() OVER ([PARTITION BY col1[, col2…]]<br />ORDER BY col1 [asc|desc], col2 [asc|desc]…]) AS rownum<br />FROM table_name)<br />WHERE rownum &lt;= N [AND conditions]<br />在无 rownum 的场景中，对于结果表主键的定义需要特别小心。如果定义有误，会直接导致 TopN 结果的不正确。 无 rownum 场景中，主键应为 TopN 上游 GROUP BY 节点的 KEY 列表。</p><h3 id="523-增加-topn-的-cache-大小"><a class="markdownIt-Anchor" href="#523-增加-topn-的-cache-大小"></a> 5.2.3 增加 TopN 的 Cache 大小</h3><p>TopN 为了提升性能有一个 State Cache 层，Cache 层能提升对 State 的访问效率。TopN 的 Cache 命中率的计算公式为。<br />cache_hit = cache_size<em>parallelism/top_n/partition_key_num<br />例如，Top100 配置缓存 10000 条，并发 50，当 PatitionBy 的 key 维度较大时，例如 10 万级别时，Cache 命中率只有 10000</em>50/100/100000=5%，命中率会很低，导致大量的请求都会击中 State（磁盘），性能会大幅下降。因此当 PartitionKey 维度特别大时，可以适当加大 TopN 的 CacheS ize，相对应的也建议适当加大 TopN 节点的 Heap Memory。</p><ul><li><strong>使用方式</strong></li></ul><p>// 初始化 table environment<br />TableEnvironment tEnv = …</p><p>// 获取 tableEnv 的配置对象<br />Configuration configuration = tEnv.getConfig().getConfiguration();</p><p>// 设置参数：<br />// 默认 10000 条，调整 TopN cahce 到 20 万，那么理论命中率能达 200000*50/100/100000 = 100%<br />configuration.setString(“table.exec.topn.cache-size”, “200000”);<br />注意：目前源码中标记为实验项，官网中未列出该参数<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954340455-7cf6887f-ac44-4f7e-a05f-acb51172c3cd.png#" alt="" /></p><h3 id="524-partitionby-的字段中要有时间类字段"><a class="markdownIt-Anchor" href="#524-partitionby-的字段中要有时间类字段"></a> 5.2.4 PartitionBy 的字段中要有时间类字段</h3><p>例如每天的排名，要带上 Day 字段。否则 TopN 的结果到最后会由于 State ttl 有错乱。</p><h3 id="525-优化后的-sql-示例"><a class="markdownIt-Anchor" href="#525-优化后的-sql-示例"></a> 5.2.5 优化后的 SQL 示例</h3><p>insert<br />into print_test<br />SELECT<br />cate_id,<br />seller_id,<br />stat_date,<br />pay_ord_amt --不输出 rownum 字段，能减小结果表的输出量（无排名优化）<br />FROM (<br />SELECT<br />*,<br />ROW_NUMBER () OVER (<br />PARTITION BY cate_id,<br />stat_date --注意要有时间字段，否则 state 过期会导致数据错乱（分区字段优化）<br />ORDER<br />BY pay_ord_amt DESC --根据上游 sum 结果排序。排序字段的更新是单调的，且单调方向与排序方向相反（走最优算法）<br />) as rownum<br />FROM (<br />SELECT<br />cate_id,<br />seller_id,<br />stat_date,<br />–重点。声明 Sum 的参数都是正数，所以 Sum 的结果是单调递增的，因此 TopN 能使用优化算法，只获取前 100 个数据（走最优算法）<br />sum (total_fee) filter (<br />where<br />total_fee &gt;= 0<br />) as pay_ord_amt<br />FROM<br />random_test<br />WHERE<br />total_fee &gt;= 0<br />GROUP<br />BY cate_name,<br />seller_id,<br />stat_date<br />) a<br />WHERE<br />rownum &lt;= 100<br />);</p><h2 id="53-高效去重方案"><a class="markdownIt-Anchor" href="#53-高效去重方案"></a> 5.3 高效去重方案</h2><p>由于 SQL 上没有直接支持去重的语法，还要灵活的保留第一条或保留最后一条。因此我们使用了 SQL 的 ROW_NUMBER OVER WINDOW 功能来实现去重语法。去重本质上是一种特殊的 TopN。</p><h3 id="531-保留首行的去重策略deduplicate-keep-firstrow"><a class="markdownIt-Anchor" href="#531-保留首行的去重策略deduplicate-keep-firstrow"></a> 5.3.1 保留首行的去重策略（Deduplicate Keep FirstRow）</h3><p>保留 KEY 下第一条出现的数据，之后出现该 KEY 下的数据会被丢弃掉。因为 STATE 中只存储了 KEY 数据，所以性能较优，示例如下：<br />SELECT _<br />FROM (<br />SELECT _,<br />ROW_NUMBER() OVER (PARTITION BY b ORDER BY proctime) as rowNum<br />FROM T<br />)<br />WHERE rowNum = 1<br />以上示例是将 T 表按照 b 字段进行去重，并按照系统时间保留第一条数据。Proctime 在这里是源表 T 中的一个具有 Processing Time 属性的字段。如果按照系统时间去重，也可以将 Proctime 字段简化 PROCTIME()函数调用，可以省略 Proctime 字段的声明。</p><h3 id="532-保留末行的去重策略deduplicate-keep-lastrow"><a class="markdownIt-Anchor" href="#532-保留末行的去重策略deduplicate-keep-lastrow"></a> 5.3.2 保留末行的去重策略（Deduplicate Keep LastRow）</h3><p>保留 KEY 下最后一条出现的数据。保留末行的去重策略性能略优于 LAST_VALUE 函数，示例如下：<br />SELECT _<br />FROM (<br />SELECT _,<br />ROW_NUMBER() OVER (PARTITION BY b, d ORDER BY rowtime DESC) as rowNum<br />FROM T<br />)<br />WHERE rowNum = 1<br />以上示例是将 T 表按照 b 和 d 字段进行去重，并按照业务时间保留最后一条数据。Rowtime 在这里是源表 T 中的一个具有 Event Time 属性的字段。</p><h2 id="54-高效的内置函数"><a class="markdownIt-Anchor" href="#54-高效的内置函数"></a> 5.4 高效的内置函数</h2><h3 id="541-使用内置函数替换自定义函数"><a class="markdownIt-Anchor" href="#541-使用内置函数替换自定义函数"></a> 5.4.1 使用内置函数替换自定义函数</h3><p>Flink 的内置函数在持续的优化当中，请尽量使用内部函数替换自定义函数。使用内置函数好处：<br />1）优化数据序列化和反序列化的耗时。<br />2）新增直接对字节单位进行操作的功能。<br />支持的系统内置函数：<br /><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/functions/systemFunctions.html">https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/functions/systemFunctions.html</a></p><h3 id="542-like-操作注意事项"><a class="markdownIt-Anchor" href="#542-like-操作注意事项"></a> 5.4.2 LIKE 操作注意事项</h3><ul><li>如果需要进行 StartWith 操作，使用 LIKE ‘xxx%’。</li><li>如果需要进行 EndWith 操作，使用 LIKE ‘%xxx’。</li><li>如果需要进行 Contains 操作，使用 LIKE ‘%xxx%’。</li><li>如果需要进行 Equals 操作，使用 LIKE ‘xxx’，等价于 str = ‘xxx’。</li><li>如果需要匹配 * 字符，请注意要完成转义 LIKE ‘%seller/id%’ ESCAPE ‘/’。*在 SQL 中属于单字符通配符，能匹配任何字符。如果声明为 LIKE ‘%seller_id%’，则不单会匹配 seller_id 还会匹配 seller#id、sellerxid 或 seller1id 等，导致结果错误。</li></ul><h3 id="543-慎用正则函数regexp"><a class="markdownIt-Anchor" href="#543-慎用正则函数regexp"></a> 5.4.3 慎用正则函数（REGEXP）</h3><p>正则表达式是非常耗时的操作，对比加减乘除通常有百倍的性能开销，而且正则表达式在某些极端情况下可能会进入无限循环，导致作业阻塞。建议使用 LIKE。正则函数包括：</p><ul><li>REGEXP</li><li>REGEXP_EXTRACT</li><li>REGEXP_REPLACE</li></ul><h2 id="55-指定时区"><a class="markdownIt-Anchor" href="#55-指定时区"></a> 5.5 指定时区</h2><p>本地时区定义了当前会话时区 id。当本地时区的时间戳进行转换时使用。在内部，带有本地时区的时间戳总是以 UTC 时区表示。但是，当转换为不包含时区的数据类型时(例如 TIMESTAMP, TIME 或简单的 STRING)，会话时区在转换期间被使用。为了避免时区错乱的问题，可以参数指定时区。<br />// 初始化 table environment<br />TableEnvironment tEnv = …</p><p>// 获取 tableEnv 的配置对象<br />Configuration configuration = tEnv.getConfig().getConfiguration();</p><p>// 设置参数：<br />// 指定时区<br />configuration.setString(“table.local-time-zone”, “Asia/Shanghai”);</p><h2 id="56-设置参数总结"><a class="markdownIt-Anchor" href="#56-设置参数总结"></a> 5.6 设置参数总结</h2><p>总结以上的调优参数，代码如下：<br />// 初始化 table environment<br />TableEnvironment tEnv = …</p><p>// 获取 tableEnv 的配置对象<br />Configuration configuration = tEnv.getConfig().getConfiguration();</p><p>// 设置参数：<br />// 开启 miniBatch<br />configuration.setString(“table.exec.mini-batch.enabled”, “true”);<br />// 批量输出的间隔时间<br />configuration.setString(“table.exec.mini-batch.allow-latency”, “5 s”);<br />// 防止 OOM 设置每个批次最多缓存数据的条数，可以设为 2 万条<br />configuration.setString(“table.exec.mini-batch.size”, “20000”);<br />// 开启 LocalGlobal<br />configuration.setString(“table.optimizer.agg-phase-strategy”, “TWO_PHASE”);<br />// 开启 Split Distinct<br />configuration.setString(“table.optimizer.distinct-agg.split.enabled”, “true”);<br />// 第一层打散的 bucket 数目<br />configuration.setString(“table.optimizer.distinct-agg.split.bucket-num”, “1024”);<br />// TopN 的缓存条数<br />configuration.setString(“table.exec.topn.cache-size”, “200000”);<br />// 指定时区<br />configuration.setString(“table.local-time-zone”, “Asia/Shanghai”);</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="posts/0/"/>
      <url>posts/0/</url>
      
        <content type="html"><![CDATA[<h1 id="flink-的-checkpoint-机制"><a class="markdownIt-Anchor" href="#flink-的-checkpoint-机制"></a> Flink 的 checkpoint 机制</h1><h2 id="1-什么是-flink-的-checkpoint"><a class="markdownIt-Anchor" href="#1-什么是-flink-的-checkpoint"></a> 1 什么是 Flink 的 checkpoint</h2><p>简单的说就是 flink 为了达到容错和 exactly-once 语义的功能，定期把 state 持久化下来，而这一持久化的过程就叫做 checkpoint，它是 Flink Job 在某一时刻全局状态的快照。</p><h2 id="2-checkpoint-的流程"><a class="markdownIt-Anchor" href="#2-checkpoint-的流程"></a> 2 checkpoint 的流程</h2><h2 id="1jobmanager-的-checkpointcoordinator-会定期向所有-sourcetask-发送-checkpointtriggersource-task-会在数据流中安插-checkpoint-barrier"><a class="markdownIt-Anchor" href="#1jobmanager-的-checkpointcoordinator-会定期向所有-sourcetask-发送-checkpointtriggersource-task-会在数据流中安插-checkpoint-barrier"></a> 1）.JobManager 的 CheckPointCoordinator 会定期向所有 SourceTask 发送 CheckPointTrigger，Source Task 会在数据流中安插 Checkpoint barrier.</h2><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954356213-8b43b50d-27fd-4e1c-b789-958a004d7c65.png#" alt="" /><br />2）. 当 source Task 收到 barrier，开始向自己的下游算子继续传递 barrier，然后自身同步的进行快照,并将自身的状态的异步的写入持久化存储中。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954356680-a4f48cc9-aee0-4eb6-ae52-3ac917a1ed79.png#" alt="" /></p><p>3）. 当所有 task 将状态信息完成备份后，会讲备份数据的地址(state handle) 通知给 JobManager 的 CheckPoint，如果 Checkpoint 的持续时长超过了 Checkpoint 设定的超时时间 CheckPointCoordinator 还没有收集完所有的 State Handle，CheckPointCoordinator 就会认为本次 Checkpoint 失败，会把这次 Checkpoint 产生的所有状态数据全部删除。<br />4）. 如果 CheckPointCoordinator 收集完所有算子的 State Handle，CheckPointCoordinator 会把整个 StateHandle 封装成 completed Checkpoint Meta，写入到外部存储中，Checkpoint 结束。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954357371-31fbd013-54a6-4611-83b5-da135854320d.png#" alt="" /></p><p>总的流程图<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954357859-36996149-1e09-4705-b17a-4b157d1bf008.png#" alt="" /></p><h1 id="sparkcheckpoint-原理剖析与源码分析"><a class="markdownIt-Anchor" href="#sparkcheckpoint-原理剖析与源码分析"></a> <strong>Spark:Checkpoint 原理剖析与源码分析</strong></h1><p>****Spark Checkpoint 是什么？**<strong>Spark 在生产环境下经常会面临 Tranformations 的 RDD 非常多（例如一个 Job 中包含 1 万个 RDD）或者具体 Tranformation 产生的 RDD 本身计算特别复杂和耗时（例如计算时常超过 1~5 个小时），此时我们必须考虑对计算结果数据的持久化。如果采用 persist 把数据放在内存中的话，虽然是最快速的但是也是最不可靠的；如果放在磁盘上也不是完全可靠的！例如磁盘会损坏。那么也就是说，出现失败的时候，没有容错机制，所以当后面的 transformation 操作，又要使用到该 RDD 时，就会发现数据丢失了（CacheManager），此时如果没有进行容错处理的话，那么可能就又要重新计算一次数据。所以，针对上述的复杂 Spark 应用的问题（没有容错机制的问题）。就可以使用 checkponit 功能。 *</strong>*Spark Checkpoint 的功能****Checkpoint 的产生就是为了相对而言更加可靠的持久化数据，在 Checkpoint 可以指定把数据放在本地并且是多副本的方式，但是在正常生产环境下放在 HDFS 上，这就天然的借助 HDFS 高可靠的特征来完成最大化的可靠的持久化数据的方式<br />checkpoint，就是说，首先呢，要调用 SparkContext 的 setCheckpointDir()方法，设置一个容错的文件系统的目录，比如说 HDFS；然后，对 RDD 调用调用 checkpoint()方法。之后，在 RDD 所处的 job 运行结束之后，会启动一个单独的 job，来将 checkpoint 过的 RDD 的数据写入之前设置的文件系统，进行高可用、容错的类持久化操作。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954358351-7621829e-239e-4cd8-a1f7-50bb8936da84.png#" alt="" /></p><h1 id="flink-的被压机制"><a class="markdownIt-Anchor" href="#flink-的被压机制"></a> Flink 的被压机制</h1><p>Flink 的被压机制看这篇博客：<a href="https://blog.csdn.net/jiang7chengzi/article/details/107432099">https://blog.csdn.net/jiang7chengzi/article/details/107432099</a><br />Spark 的被压机制看这篇博客：<a href="https://blog.csdn.net/huzechen/article/details/105898149">https://blog.csdn.net/huzechen/article/details/105898149</a></p><h1 id="flink-的内存管理"><a class="markdownIt-Anchor" href="#flink-的内存管理"></a> Flink 的内存管理</h1><p>Flink 的内存分：堆内内存和堆外内存</p><h2 id="堆内内存组成如下"><a class="markdownIt-Anchor" href="#堆内内存组成如下"></a> 堆内内存组成如下：</h2><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954358784-5fb120d0-39db-43e1-b41b-9711b251f178.png#" alt="" /></p><ul><li>*<strong>*Network Buffers:**</strong>  一定数量的 32KB 大小的 buffer，主要用于数据的网络传输。在 TaskManager 启动的时候就会分配。默认数量是 2048 个，可以通过  taskmanager.network.numberOfBuffers 来配置。</li><li>*<strong>*Memory Manager Pool:**</strong>  这是一个由  MemoryManager  管理的，由众多 MemorySegment 组成的超大集合。Flink 中的算法（如 sort/shuffle/join）会向这个内存池申请 MemorySegment，将序列化后的数据存于其中，使用完后释放回内存池。默认情况下，池子占了堆内存的 70% 的大小。</li><li>*<strong>*Remaining (Free) Heap:**</strong>  这部分的内存是留给用户代码以及 TaskManager 的数据结构使用的。因为这些数据结构一般都很小，所以基本上这些内存都是给用户代码使用的。从 GC 的角度来看，可以把这里看成的新生代，也就是说这里主要都是由用户代码生成的短期对象。</li></ul><p>参考这篇博客：<a href="https://blog.csdn.net/qq_36421826/article/details/82494104">https://blog.csdn.net/qq_36421826/article/details/82494104</a></p><p>Spark 的内存管理：</p><h1 id="3-spark-内存管理"><a class="markdownIt-Anchor" href="#3-spark-内存管理"></a> 3 Spark 内存管理</h1><p>在执行 Spark 的应用程序时，Spark 集群会启动 Driver 和 Executor 两种 JVM 进程，前者为主控进程，负责创建 Spark 上下文，提交 Spark 作业（Job），并将作业转化为计算任务（Task），在各个 Executor 进程间协调任务的调度，后者负责在工作节点上执行具体的计算任务，并将结果返回给 Driver，同时为需要持久化的 RDD 提供存储功能。由于 Driver 的内存管理相对来说较为简单，本节主要对 Executor 的内存管理进行分析，下文中的 Spark 内存均特指 Executor 的内存。</p><h2 id="61-堆内和堆外内存规划"><a class="markdownIt-Anchor" href="#61-堆内和堆外内存规划"></a> 6.1 堆内和堆外内存规划</h2><p>作为一个 JVM 进程，Executor 的内存管理建立在 JVM 的内存管理之上，Spark 对 JVM 的堆内（On-heap）空间进行了更为详细的分配，以充分利用内存。同时，Spark 引入了堆外（Off-heap）内存，使之可以直接在工作节点的系统内存中开辟空间，进一步优化了内存的使用。<br />堆内内存受到 JVM 统一管理，堆外内存是直接向操作系统进行内存的申请和释放。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954359316-e968eaab-1b7a-4f61-9d69-a6be7e2d7803.png#" alt="" /><br />图 1-1 Executor 堆内与堆外内存</p><ol><li>堆内内存</li></ol><p>堆内内存的大小，由 Spark 应用程序启动时的 –executor-memory 或 spark.executor.memory 参数配置。Executor 内运行的并发任务共享 JVM 堆内内存，这些任务在缓存 RDD 数据和广播（Broadcast）数据时占用的内存被规划为存储（Storage）内存，而这些任务在执行 Shuffle 时占用的内存被规划为执行（Execution）内存，剩余的部分不做特殊规划，那些 Spark 内部的对象实例，或者用户定义的 Spark 应用程序中的对象实例，均占用剩余的空间。不同的管理模式下，这三部分占用的空间大小各不相同。<br />Spark 对堆内内存的管理是一种逻辑上的”<strong>规划式</strong>”的管理，因为对象实例占用内存的申请和释放都由 JVM 完成，Spark 只能在申请后和释放前记录这些内存，我们来看其具体流程：<br />申请内存流程如下：</p><ol><li>Spark 在代码中 new 一个对象实例；</li><li>JVM 从堆内内存分配空间，创建对象并返回对象引用；</li><li>Spark 保存该对象的引用，记录该对象占用的内存。</li></ol><p>释放内存流程如下：</p><ol><li><p>Spark 记录该对象释放的内存，删除该对象的引用；</p></li><li><p>等待 JVM 的垃圾回收机制释放该对象占用的堆内内存。<br />我们知道，JVM 的对象可以以序列化的方式存储，序列化的过程是将对象转换为二进制字节流，本质上可以理解为将非连续空间的链式存储转化为连续空间或块存储，在访问时则需要进行序列化的逆过程——反序列化，将字节流转化为对象，序列化的方式可以节省存储空间，但增加了存储和读取时候的计算开销。<br />对于 Spark 中序列化的对象，由于是字节流的形式，其占用的内存大小可直接计算，而对于非序列化的对象，其占用的内存是通过周期性地采样近似估算而得，即并不是每次新增的数据项都会计算一次占用的内存大小，这种方法降低了时间开销但是有可能误差较大，导致某一时刻的实际内存有可能远远超出预期。此外，在被 Spark 标记为释放的对象实例，很有可能在实际上并没有被 JVM 回收，导致实际可用的内存小于 Spark 记录的可用内存。所以 Spark 并不能准确记录实际可用的堆内内存，从而也就无法完全避免内存溢出（OOM, Out of Memory）的异常。<br />虽然不能精准控制堆内内存的申请和释放，但 Spark 通过对存储内存和执行内存各自独立的规划管理，可以决定是否要在存储内存里缓存新的 RDD，以及是否为新的任务分配执行内存，在一定程度上可以提升内存的利用率，减少异常的出现。</p></li><li><p>堆外内存</p></li></ol><p>为了进一步优化内存的使用以及提高 Shuffle 时排序的效率，Spark 引入了堆外（Off-heap）内存，使之可以直接在工作节点的系统内存中开辟空间，存储经过序列化的二进制数据。<br />堆外内存意味着把内存对象分配在 Java 虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。<br />利用 JDK Unsafe API（从 Spark 2.0 开始，在管理堆外的存储内存时不再基于 Tachyon，而是与堆外的执行内存一样，基于 JDK Unsafe API 实现），Spark 可以直接操作系统堆外内存，减少了不必要的内存开销，以及频繁的 GC 扫描和回收，提升了处理性能。堆外内存可以被精确地申请和释放（堆外内存之所以能够被精确的申请和释放，是由于内存的申请和释放不再通过 JVM 机制，而是直接向操作系统申请，JVM 对于内存的清理是无法准确指定时间点的，因此无法实现精确的释放），而且序列化的数据占用的空间可以被精确计算，所以相比堆内内存来说降低了管理的难度，也降低了误差。<br />在默认情况下堆外内存并不启用，可通过配置 spark.memory.offHeap.enabled 参数启用，并由 spark.memory.offHeap.size 参数设定堆外空间的大小。除了没有 other 空间，堆外内存与堆内内存的划分方式相同，所有运行中的并发任务共享存储内存和执行内存。<br />（*该部分内存主要用于程序的共享库、Perm Space、线程 Stack 和一些 Memory mapping 等, 或者类 C 方式 allocate object）</p><h2 id="62-内存空间分配"><a class="markdownIt-Anchor" href="#62-内存空间分配"></a> 6.2 内存空间分配</h2><ol><li><p>静态内存管理<br />在 Spark 最初采用的静态内存管理机制下，存储内存、执行内存和其他内存的大小在 Spark 应用程序运行期间均为固定的，但用户可以应用程序启动前进行配置，堆内内存的分配如图 2 所示：<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954359860-5e0ed955-9692-429a-a1d1-8e562552b5d4.png#" alt="" /><br />图 1-2 静态内存管理——堆内内存<br />可以看到，可用的堆内内存的大小需要按照代码清单 1-1 的方式计算：<br />代码清单 1-1 堆内内存计算公式<br />可用的存储内存 = systemMaxMemory _ spark.storage.memoryFraction _ spark.storage.safety Fraction<br />可用的执行内存 = systemMaxMemory _ spark.shuffle.memoryFraction _ spark.shuffle.safety Fraction<br />其中 systemMaxMemory 取决于当前 JVM 堆内内存的大小，最后可用的执行内存或者存储内存要在此基础上与各自的 memoryFraction 参数和 safetyFraction 参数相乘得出。上述计算公式中的两个 safetyFraction 参数，其意义在于在逻辑上预留出 1-safetyFraction 这么一块保险区域，降低因实际内存超出当前预设范围而导致 OOM 的风险（上文提到，对于非序列化对象的内存采样估算会产生误差）。值得注意的是，这个预留的保险区域仅仅是一种逻辑上的规划，在具体使用时 Spark 并没有区别对待，和”其它内存”一样交给了 JVM 去管理。<br />Storage 内存和 Execution 内存都有预留空间，目的是防止 OOM，因为 Spark 堆内内存大小的记录是不准确的，需要留出保险区域。<br />堆外的空间分配较为简单，只有存储内存和执行内存，如图 1-3 所示。可用的执行内存和存储内存占用的空间大小直接由参数 spark.memory.storageFraction 决定，由于堆外内存占用的空间可以被精确计算，所以无需再设定保险区域。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954360286-c44f94c7-1d7a-4a95-a9d5-0bc7890ee982.png#" alt="" /><br />图 1-3 静态内存管理<br />静态内存管理机制实现起来较为简单，但如果用户不熟悉 Spark 的存储机制，或没有根据具体的数据规模和计算任务或做相应的配置，很容易造成”一半海水，一半火焰”的局面，即存储内存和执行内存中的一方剩余大量的空间，而另一方却早早被占满，不得不淘汰或移出旧的内容以存储新的内容。由于新的内存管理机制的出现，这种方式目前已经很少有开发者使用，出于兼容旧版本的应用程序的目的，Spark 仍然保留了它的实现。</p></li><li><p>统一内存管理</p></li></ol><p>Spark 1.6 之后引入的统一内存管理机制，与静态内存管理的区别在于存储内存和执行内存共享同一块空间，可以动态占用对方的空闲区域，统一内存管理的堆内内存结构如图 1-4 所示：<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954360806-660d1c55-0f45-4156-b708-ab6859245b6b.png#" alt="" /><br />图 1-4 统一内存管理——堆内内存<br />统一内存管理的堆外内存结构如图 1-5 所示：<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954361353-1e7d94b6-ba1e-4cc1-9c57-4d57c45accd0.png#" alt="" /><br />图 1-5 统一内存管理——堆外内存<br />其中最重要的优化在于动态占用机制，其规则如下：</p><ol><li><p>设定基本的存储内存和执行内存区域（spark.storage.storageFraction 参数），该设定确定了双方各自拥有的空间的范围；</p></li><li><p>双方的空间都不足时，则存储到硬盘；若己方空间不足而对方空余时，可借用对方的空间;（存储空间不足是指不足以放下一个完整的 Block）</p></li><li><p>执行内存的空间被对方占用后，可让对方将占用的部分转存到硬盘，然后”归还”借用的空间；</p></li><li><p>存储内存的空间被对方占用后，无法让对方”归还”，因为需要考虑 Shuffle 过程中的很多因素，实现起来较为复杂。<br />统一内存管理的动态占用机制如图 1-6 所示：<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954361951-51a7ab21-9411-4abd-96e0-aac5a6819a87.png#" alt="" /><br />图 1-6 同一内存管理——动态占用机制<br />凭借统一内存管理机制，Spark 在一定程度上提高了堆内和堆外内存资源的利用率，降低了开发者维护 Spark 内存的难度，但并不意味着开发者可以高枕无忧。如果存储内存的空间太大或者说缓存的数据过多，反而会导致频繁的全量垃圾回收，降低任务执行时的性能，因为缓存的 RDD 数据通常都是长期驻留内存的。所以要想充分发挥 Spark 的性能，需要开发者进一步了解存储内存和执行内存各自的管理方式和实现原理。</p></li></ol><h2 id="63-存储内存管理"><a class="markdownIt-Anchor" href="#63-存储内存管理"></a> 6.3 存储内存管理</h2><ol><li>RDD 的持久化机制</li></ol><p>弹性分布式数据集（RDD）作为 Spark 最根本的数据抽象，是只读的分区记录（Partition）的集合，只能基于在稳定物理存储中的数据集上创建，或者在其他已有的 RDD 上执行转换（Transformation）操作产生一个新的 RDD。转换后的 RDD 与原始的 RDD 之间产生的依赖关系，构成了血统（Lineage）。<strong>凭借血统，Spark 保证了每一个 RDD 都可以被重新恢复</strong>。但 RDD 的所有转换都是惰性的，即只有当一个返回结果给 Driver 的行动（Action）发生时，Spark 才会创建任务读取 RDD，然后真正触发转换的执行。<br />Task 在启动之初读取一个分区时，会先判断这个分区是否已经被持久化，如果没有则需要检查 Checkpoint 或按照血统重新计算。所以如果一个 RDD 上要执行多次行动，可以在第一次行动中使用 persist 或 cache 方法，在内存或磁盘中持久化或缓存这个 RDD，从而在后面的行动时提升计算速度。<br />事实上，cache 方法是使用默认的 MEMORY_ONLY 的存储级别将 RDD 持久化到内存，故缓存是一种特殊的持久化。 <strong>堆内和堆外存储内存的设计，便可以对缓存 RDD 时使用的内存做统一的规划和管理</strong>。<br />RDD 的持久化由 Spark 的 Storage 模块负责，实现了 RDD 与物理存储的解耦合。Storage 模块负责管理 Spark 在计算过程中产生的数据，将那些在内存或磁盘、在本地或远程存取数据的功能封装了起来。在具体实现时 Driver 端和 Executor 端的 Storage 模块构成了主从式的架构，即 Driver 端的 BlockManager 为 Master，Executor 端的 BlockManager 为 Slave。<br />Storage 模块在逻辑上以 Block 为基本存储单位，<strong>RDD 的每个 Partition 经过处理后唯一对应一个 Block</strong>（BlockId 的格式为 rdd_RDD-ID_PARTITION-ID ）。Driver 端的 Master 负责整个 Spark 应用程序的 Block 的元数据信息的管理和维护，而 Executor 端的 Slave 需要将 Block 的更新等状态上报到 Master，同时接收 Master 的命令，例如新增或删除一个 RDD。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954362457-545129e5-7a8c-4f2a-8021-db488592db43.png#" alt="" /><br />图 5-1 Storage 模块示意图<br />在对 RDD 持久化时，Spark 规定了 MEMORY_ONLY、MEMORY_AND_DISK 等 7 种不同的[存储级别 ](<a href="http://spark.apache.org/docs/latest/programming-guide.html">http://spark.apache.org/docs/latest/programming-guide.html</a>&quot; \l &quot;rdd-persistence)，而存储级别是以下 5 个变量的组合：<br />代码清单 5-1 resourceOffer 代码<br />class StorageLevel private(<br />private var _useDisk: Boolean, //磁盘<br />private var _useMemory: Boolean, //这里其实是指堆内内存<br />private var _useOffHeap: Boolean, //堆外内存<br />private var _deserialized: Boolean, //是否为非序列化<br />private var _replication: Int = 1 //副本个数<br />)<br />Spark 中 7 种存储级别如下：<br />表 5-1 Spark 持久化级别</p><table><thead><tr><th><strong>持久化级别</strong></th><th><strong>含义</strong></th></tr></thead><tbody><tr><td><strong>MEMORY_ONLY</strong></td><td>以非序列化的 Java 对象的方式持久化在 JVM 内存中。如果内存无法完全存储 RDD 所有的 partition，那么那些没有持久化的 partition 就会在下一次需要使用它们的时候，重新被计算</td></tr><tr><td><strong>MEMORY_AND_DISK</strong></td><td>同上，但是当某些 partition 无法存储在内存中时，会持久化到磁盘中。下次需要使用这些 partition 时，需要从磁盘上读取</td></tr><tr><td><strong>MEMORY_ONLY_SER</strong></td><td>同 MEMORY_ONLY，但是会使用 Java 序列化方式，将 Java 对象序列化后进行持久化。可以减少内存开销，但是需要进行反序列化，因此会加大 CPU 开销</td></tr><tr><td><strong>MEMORY_AND_DISK_SER</strong></td><td>同 MEMORY_AND_DISK，但是使用序列化方式持久化 Java 对象</td></tr><tr><td><strong>DISK_ONLY</strong></td><td>使用非序列化 Java 对象的方式持久化，完全存储到磁盘上</td></tr></tbody></table><p>| <strong>MEMORY_ONLY_2</strong><br /><strong>MEMORY_AND_DISK_2</strong><br /><strong>等等</strong> | 如果是尾部加了 2 的持久化级别，表示将持久化数据复用一份，保存到其他节点，从而在数据丢失时，不需要再次计算，只需要使用备份数据即可 |</p><p>通过对数据结构的分析，可以看出存储级别从三个维度定义了 RDD 的 Partition（同时也就是 Block）的存储方式：</p><ol><li><strong>存储位置</strong>：磁盘／堆内内存／堆外内存。如 MEMORY_AND_DISK 是同时在磁盘和堆内内存上存储，实现了冗余备份。OFF_HEAP 则是只在堆外内存存储，目前选择堆外内存时不能同时存储到其他位置。</li><li><strong>存储形式</strong>：Block 缓存到存储内存后，是否为非序列化的形式。如 MEMORY_ONLY 是非序列化方式存储，OFF_HEAP 是序列化方式存储。</li><li><strong>副本数量</strong>：大于 1 时需要远程冗余备份到其他节点。如 DISK_ONLY_2 需要远程备份 1 个副本。</li><li>RDD 的缓存过程</li></ol><p>RDD 在缓存到存储内存之前，Partition 中的数据一般以迭代器（<a href="http://www.scala-lang.org/docu/files/collections-api/collections_43.html">Iterator</a>）的数据结构来访问，这是 Scala 语言中一种遍历数据集合的方法。通过 Iterator 可以获取分区中每一条序列化或者非序列化的数据项(Record)，这些 Record 的对象实例在逻辑上占用了 JVM 堆内内存的 other 部分的空间，<strong>同一 Partition 的不同 Record 的存储空间并不连续</strong>。<br />RDD 在缓存到存储内存之后，Partition 被转换成 Block，Record 在堆内或堆外存储内存中占用一块连续的空间。<strong>将 Partition 由不连续的存储空间转换为连续存储空间的过程，Spark 称之为&quot;展开&quot;（Unroll）</strong>。<br />Block 有序列化和非序列化两种存储格式，具体以哪种方式取决于该 RDD 的存储级别。非序列化的 Block 以一种 DeserializedMemoryEntry 的数据结构定义，用一个数组存储所有的对象实例，序列化的 Block 则以 SerializedMemoryEntry 的数据结构定义，用字节缓冲区（ByteBuffer）来存储二进制数据。每个 Executor 的 Storage 模块用一个链式 Map 结构（LinkedHashMap）来管理堆内和堆外存储内存中所有的 Block 对象的实例，对这个 LinkedHashMap 新增和删除间接记录了内存的申请和释放。<br /><strong>因为不能保证存储空间可以一次容纳 Iterator 中的所有数据，当前的计算任务在 Unroll 时要向 MemoryManager 申请足够的 Unroll 空间来临时占位，空间不足则 Unroll 失败，空间足够时可以继续进行</strong>。<br />对于序列化的 Partition，其所需的 Unroll 空间可以直接累加计算，一次申请。<br />对于非序列化的 Partition 则要在遍历 Record 的过程中依次申请，即每读取一条 Record，采样估算其所需的 Unroll 空间并进行申请，空间不足时可以中断，释放已占用的 Unroll 空间。<br />如果最终 Unroll 成功，当前 Partition 所占用的 Unroll 空间被转换为正常的缓存 RDD 的存储空间，如下图所示。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954362973-035672cb-e25d-4a76-9c5b-71a17de1ca8d.png#" alt="" /><br />图 5-2 Spark Unroll<br />在静态内存管理时，Spark 在存储内存中专门划分了一块 Unroll 空间，其大小是固定的，统一内存管理时则没有对 Unroll 空间进行特别区分，当存储空间不足时会根据动态占用机制进行处理。</p><ol><li>淘汰与落盘</li></ol><p><strong>由于同一个 Executor 的所有的计算任务共享有限的存储内存空间，当有新的 Block 需要缓存但是剩余空间不足且无法动态占用时，就要对 LinkedHashMap 中的旧 Block 进行淘汰（Eviction），而被淘汰的 Block 如果其存储级别中同时包含存储到磁盘的要求，则要对其进行落盘（Drop），否则直接删除该 Block</strong>。<br />存储内存的淘汰规则为：</p><ul><li>被淘汰的旧 Block 要与新 Block 的 MemoryMode 相同，即同属于堆外或堆内内存；</li><li>新旧 Block 不能属于同一个 RDD，避免循环淘汰；</li><li>旧 Block 所属 RDD 不能处于被读状态，避免引发一致性问题；</li><li>遍历 LinkedHashMap 中 Block，按照最近最少使用（LRU）的顺序淘汰，直到满足新 Block 所需的空间。其中 LRU 是 LinkedHashMap 的特性。</li></ul><p>落盘的流程则比较简单，如果其存储级别符合_useDisk 为 true 的条件，再根据其_deserialized 判断是否是非序列化的形式，若是则对其进行序列化，最后将数据存储到磁盘，在 Storage 模块中更新其信息。</p><h2 id="64-执行内存管理"><a class="markdownIt-Anchor" href="#64-执行内存管理"></a> 6.4 执行内存管理</h2><p>执行内存主要用来存储任务在执行 Shuffle 时占用的内存，Shuffle 是按照一定规则对 RDD 数据重新分区的过程，我们来看 Shuffle 的 Write 和 Read 两阶段对执行内存的使用：</p><ul><li>Shuffle Write</li></ul><ol><li>若在 map 端选择普通的排序方式，会采用 ExternalSorter 进行外排，在内存中存储数据时主要占用堆内执行空间。</li><li>若在 map 端选择 Tungsten 的排序方式，则采用 ShuffleExternalSorter 直接对以序列化形式存储的数据排序，在内存中存储数据时可以占用堆外或堆内执行空间，取决于用户是否开启了堆外内存以及堆外执行内存是否足够。</li></ol><ul><li>Shuffle Read</li></ul><ol start="3"><li>在对 reduce 端的数据进行聚合时，要将数据交给 Aggregator 处理，在内存中存储数据时占用堆内执行空间。</li><li>如果需要进行最终结果排序，则要将再次将数据交给 ExternalSorter 处理，占用堆内执行空间。</li></ol><p>在 ExternalSorter 和 Aggregator 中，Spark 会使用一种叫 AppendOnlyMap 的哈希表在堆内执行内存中存储数据，但在 Shuffle 过程中所有数据并不能都保存到该哈希表中，当这个哈希表占用的内存会进行周期性地采样估算，当其大到一定程度，无法再从 MemoryManager 申请到新的执行内存时，Spark 就会将其全部内容存储到磁盘文件中，这个过程被称为溢存(Spill)，溢存到磁盘的文件最后会被归并(Merge)。<br />Shuffle Write 阶段中用到的 Tungsten 是 Databricks 公司提出的对 Spark 优化内存和 CPU 使用的计划（钨丝计划），解决了一些 JVM 在性能上的限制和弊端。Spark 会根据 Shuffle 的情况来自动选择是否采用 Tungsten 排序。<br />Tungsten 采用的页式内存管理机制建立在 MemoryManager 之上，即 Tungsten 对执行内存的使用进行了一步的抽象，这样在 Shuffle 过程中无需关心数据具体存储在堆内还是堆外。<br />每个内存页用一个 MemoryBlock 来定义，并用 Object obj 和 long offset 这两个变量统一标识一个内存页在系统内存中的地址。<br />堆内的 MemoryBlock 是以 long 型数组的形式分配的内存，其 obj 的值为是这个数组的对象引用，offset 是 long 型数组的在 JVM 中的初始偏移地址，两者配合使用可以定位这个数组在堆内的绝对地址；堆外的 MemoryBlock 是直接申请到的内存块，其 obj 为 null，offset 是这个内存块在系统内存中的 64 位绝对地址。<strong>Spark 用 MemoryBlock 巧妙地将堆内和堆外内存页统一抽象封装，并用页表(pageTable)管理每个 Task 申请到的内存页</strong>。<br />Tungsten 页式管理下的所有内存用 64 位的逻辑地址表示，由页号和页内偏移量组成：</p><ul><li>页号：占 13 位，唯一标识一个内存页，Spark 在申请内存页之前要先申请空闲页号。</li><li>页内偏移量：占 51 位，是在使用内存页存储数据时，数据在页内的偏移地址。</li></ul><p>有了统一的寻址方式，Spark 可以用 64 位逻辑地址的指针定位到堆内或堆外的内存，整个 Shuffle Write 排序的过程只需要对指针进行排序，并且无需反序列化，整个过程非常高效，对于内存访问效率和 CPU 使用效率带来了明显的提升。<br />Spark 的存储内存和执行内存有着截然不同的管理方式：对于存储内存来说，Spark 用一个 LinkedHashMap 来集中管理所有的 Block，Block 由需要缓存的 RDD 的 Partition 转化而成；而对于执行内存，Spark 用 AppendOnlyMap 来存储 Shuffle 过程中的数据，在 Tungsten 排序中甚至抽象成为页式内存管理，开辟了全新的 JVM 内存管理机制。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="posts/0/"/>
      <url>posts/0/</url>
      
        <content type="html"><![CDATA[<p>尚硅谷大数据技术之 Spark 优化</p><p>版本：V3.0<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954371829-44ae6d36-6d00-461d-b09e-279b9bf8e6b6.png#" alt="" /></p><p>作者：尚硅谷大数据研发部</p><h1 id="第-1-章-spark-性能调优"><a class="markdownIt-Anchor" href="#第-1-章-spark-性能调优"></a> 第 1 章 Spark 性能调优</h1><h2 id="11-常规性能调优"><a class="markdownIt-Anchor" href="#11-常规性能调优"></a> 1.1 常规性能调优</h2><h3 id="111-常规性能调优一最优资源配置"><a class="markdownIt-Anchor" href="#111-常规性能调优一最优资源配置"></a> 1.1.1 常规性能调优一：最优资源配置</h3><p>Spark 性能调优的第一步，就是为任务分配更多的资源，在一定范围内，增加资源的分配与性能的提升是成正比的，实现了最优的资源配置后，在此基础上再考虑进行后面论述的性能调优策略。<br />资源的分配在使用脚本提交 Spark 任务时进行指定，标准的 Spark 任务提交脚本如下所示：<br />bin/spark-submit <br />–class com.atguigu.spark.Analysis <br />–master yarn<br />–deploy-mode cluster</p><p>–num-executors 80 \ 每台机器 3-12<br />–driver-memory 6g \<br />–executor-memory 6g \ 总内存给数据量的 8 倍以上<br />–executor-cores 3 \ 每个 exe 给 2-6<br />/usr/opt/modules/spark/jar/spark.jar <br />可以进行分配的资源如表所示：</p><table><thead><tr><th><strong>名称</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>–num-executors</td><td>置 Executor 的数量配</td></tr><tr><td>–driver-memory</td><td>配置 Driver 内存（影响不大）</td></tr><tr><td>–executor-memory</td><td>配置每个 Executor 的内存大小</td></tr><tr><td>–executor-cores</td><td>配置每个 Executor 的 CPU core 数量</td></tr></tbody></table><p>调节原则：尽量将任务分配的资源调节到可以使用的资源的最大限度。<br />对于具体资源的分配，我们分别讨论 Spark 的两种 Cluster 运行模式：</p><ul><li>第一种是 Spark Standalone 模式，你在提交任务前，一定知道或者可以从运维部门获取到你可以使用的资源情况，在编写 submit 脚本的时候，就根据可用的资源情况进行资源的分配，比如说集群有 15 台机器，每台机器为 8G 内存，2 个 CPU core，那么就指定 15 个 Executor，每个 Executor 分配 8G 内存，2 个 CPU core。</li><li>第二种是 Spark Yarn 模式，由于 Yarn 使用资源队列进行资源的分配和调度，在编写 submit 脚本的时候，就根据 Spark 作业要提交到的资源队列，进行资源的分配，比如资源队列有 400G 内存，100 个 CPU core，那么指定 50 个 Executor，每个 Executor 分配 8G 内存，2 个 CPU core。</li></ul><p>对各项资源进行了调节后，得到的性能提升会有如下表现：</p><table><thead><tr><th><strong>名称</strong></th><th><strong>解析</strong></th></tr></thead><tbody><tr><td>增加 Executor·个数</td><td>在资源允许的情况下，增加 Executor 的个数可以提高执行 task 的并行度。比如有 4 个 Executor，每个 Executor 有 2 个 CPU core，那么可以并行执行 8 个 task，如果将 Executor 的个数增加到 8 个（资源允许的情况下），那么可以并行执行 16 个 task，此时的并行能力提升了一倍。</td></tr><tr><td>增加每个 Executor 的 CPU core 个数</td><td>在资源允许的情况下，增加每个 Executor 的 Cpu core 个数，可以提高执行 task 的并行度。比如有 4 个 Executor，每个 Executor 有 2 个 CPU core，那么可以并行执行 8 个 task，如果将每个 Executor 的 CPU core 个数增加到 4 个（资源允许的情况下），那么可以并行执行 16 个 task，此时的并行能力提升了一倍。</td></tr><tr><td>增加每个 Executor 的内存量</td><td>在资源允许的情况下，增加每个 Executor 的内存量以后，对性能的提升有三点：</td></tr></tbody></table><ol><li>可以缓存更多的数据（即对 RDD 进行 cache），写入磁盘的数据相应减少，甚至可以不写入磁盘，减少了可能的磁盘 IO；</li><li>可以为 shuffle 操作提供更多内存，即有更多空间来存放 reduce 端拉取的数据，写入磁盘的数据相应减少，甚至可以不写入磁盘，减少了可能的磁盘 IO；</li><li>可以为 task 的执行提供更多内存，在 task 的执行过程中可能创建很多对象，内存较小时会引发频繁的 GC，增加内存后，可以避免频繁的 GC，提升整体性能。<br />|</li></ol><p>补充：生产环境 Spark submit 脚本配置<br />bin/spark-submit <br />–class com.atguigu.spark.WordCount <br />–master yarn<br />–deploy-mode cluster<br />**–num-executors 80 **<br />–driver-memory 6g <br />–executor-memory 6g <br />–executor-cores 3 <br />–queue root.default <br />–conf spark.yarn.executor.memoryOverhead=2048 <br />–conf spark.core.connection.ack.wait.timeout=300 <br />/usr/local/spark/spark.py<br />参数配置参考值：</p><ul><li>–num-executors：50~100</li><li>–driver-memory：1G~8G</li><li>–executor-memory：6G~10G</li><li>–executor-cores：3</li><li>–master：实际生产环境一定使用 yarn</li></ul><h3 id="112-常规性能调优二rdd-优化"><a class="markdownIt-Anchor" href="#112-常规性能调优二rdd-优化"></a> 1.1.2 常规性能调优二：RDD 优化</h3><ol><li>RDD 复用</li></ol><p>在对 RDD 进行算子时，要避免相同的算子和计算逻辑之下对 RDD 进行重复的计算<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954372370-5b37e172-51a6-4586-b9b8-d64f36877218.png#" alt="" /><br />对上图中的 RDD 计算架构进行修改，得到如下图所示的优化结果：<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954372916-2f2fc253-d241-482a-b222-1bafabd0714c.png#" alt="" /></p><ol><li>RDD 持久化</li></ol><p>在 Spark 中，当多次对同一个 RDD 执行算子操作时，每一次都会对这个 RDD 以之前的父 RDD 重新计算一次，这种情况是必须要避免的，对同一个 RDD 的重复计算是对资源的极大浪费，因此，必须对多次使用的 RDD 进行持久化，通过持久化将公共 RDD 的数据缓存到内存/磁盘中，之后对于公共 RDD 的计算都会从内存/磁盘中直接获取 RDD 数据。<br />对于 RDD 的持久化，有两点需要说明：</p><ul><li>RDD 的持久化是可以进行序列化的，当内存无法将 RDD 的数据完整的进行存放的时候，可以考虑使用序列化的方式减小数据体积，将数据完整存储在内存中。</li><li>如果对于数据的可靠性要求很高，并且内存充足，可以使用副本机制，对 RDD 数据进行持久化。当持久化启用了复本机制时，对于持久化的每个数据单元都存储一个副本，放在其他节点上面，由此实现数据的容错，一旦一个副本数据丢失，不需要重新计算，还可以使用另外一个副本。</li></ul><ol><li>RDD 尽可能早的 filter 操作</li></ol><p>获取到初始 RDD 后，应该考虑尽早地过滤掉不需要的数据，进而减少对内存的占用，从而提升 Spark 作业的运行效率。</p><h3 id="113-常规性能调优三并行度调节"><a class="markdownIt-Anchor" href="#113-常规性能调优三并行度调节"></a> 1.1.3 常规性能调优三：并行度调节</h3><p>Spark 作业中的并行度指各个 stage 的 task 的数量。<br />如果并行度设置不合理而导致并行度过低，会导致资源的极大浪费，例如，20 个 Executor，每个 Executor 分配 3 个 CPU core，而 Spark 作业有 40 个 task，这样每个 Executor 分配到的 task 个数是 2 个，这就使得每个 Executor 有一个 CPU core 空闲，导致资源的浪费。<br />理想的并行度设置，应该是让并行度与资源相匹配，简单来说就是在资源允许的前提下，并行度要设置的尽可能大，达到可以充分利用集群资源。合理的设置并行度，可以提升整个 Spark 作业的性能和运行速度。<br />Spark 官方推荐，task 并行度数量应该设置为 Spark 作业总 CPU core 数量的 2~3 倍。之所以没有推荐 task 数量与 CPU core 总数相等，是因为 task 的执行时间不同，有的 task 执行速度快而有的 task 执行速度慢，如果 task 数量与 CPU core 总数相等，那么执行快的 task 执行完成后，会出现 CPU core 空闲的情况。如果 task 数量设置为 CPU core 总数的 2~3 倍，那么一个 task 执行完毕后，CPU core 会立刻执行下一个 task，降低了资源的浪费，同时提升了 Spark 作业运行的效率。<br />Spark 作业并行度的设置如下所示：<br />val conf = new SparkConf()<br />.set(“spark.default.parallelism”, “500”)</p><h3 id="114-常规性能调优四广播大变量"><a class="markdownIt-Anchor" href="#114-常规性能调优四广播大变量"></a> 1.1.4 常规性能调优四：广播大变量</h3><p>默认情况下，task 中的算子中如果使用了外部的变量，每个 task 都会获取一份变量的复本，这就造成了内存的极大消耗。一方面，如果后续对 RDD 进行持久化，可能就无法将 RDD 数据存入内存，只能写入磁盘，磁盘 IO 将会严重消耗性能；另一方面，task 在创建对象的时候，也许会发现堆内存无法存放新创建的对象，这就会导致频繁的 GC，GC 会导致工作线程停止，进而导致 Spark 暂停工作一段时间，严重影响 Spark 性能。<br />假设当前任务配置了 20 个 Executor，指定 500 个 task，有一个 20M 的变量被所有 task 共用，此时会在 500 个 task 中产生 500 个副本，耗费集群 10G 的内存，如果使用了广播变量， 那么每个 Executor 保存一个副本，一共消耗 400M 内存，内存消耗减少了 5 倍。<br />广播变量在每个 Executor 保存一个副本，此 Executor 的所有 task 共用此广播变量，这让变量产生的副本数量大大减少。<br />在初始阶段，广播变量只在 Driver 中有一份副本。task 在运行的时候，想要使用广播变量中的数据，此时首先会在自己本地的 Executor 对应的 BlockManager 中尝试获取变量，如果本地没有，BlockManager 就会从 Driver 或者其他节点的 BlockManager 上远程拉取变量的复本，并由本地的 BlockManager 进行管理；之后此 Executor 的所有 task 都会直接从本地的 BlockManager 中获取变量。</p><h3 id="115-常规性能调优五kryo-序列化"><a class="markdownIt-Anchor" href="#115-常规性能调优五kryo-序列化"></a> 1.1.5 常规性能调优五：Kryo 序列化</h3><p>默认情况下，Spark 使用 Java 的序列化机制。Java 的序列化机制使用方便，不需要额外的配置，在算子中使用的变量实现 Serializable 接口即可，但是，Java 序列化机制的效率不高，序列化速度慢并且序列化后的数据所占用的空间依然较大。<br />Kryo 序列化机制比 Java 序列化机制性能提高 10 倍左右，Spark 之所以没有默认使用 Kryo 作为序列化类库，是因为它不支持所有对象的序列化，同时 Kryo 需要用户在使用前注册需要序列化的类型，不够方便，但从 Spark 2.0.0 版本开始，简单类型、简单类型数组、字符串类型的 Shuffling RDDs 已经默认使用 Kryo 序列化方式了。<br />public class MyKryoRegistrator implements KryoRegistrator<br />{<br />@Override<br />public void registerClasses(Kryo kryo)<br />{<br />kryo.register(StartupReportLogs.class);<br />}<br />}<br />配置 Kryo 序列化方式的实例代码：<br />//创建 SparkConf 对象<br />val conf = new SparkConf().setMaster(…).setAppName(…)<br />//使用 Kryo 序列化库，如果要使用 Java 序列化库，需要把该行屏蔽掉<br />conf.set(“spark.serializer”, “org.apache.spark.serializer.KryoSerializer”);<br />//在 Kryo 序列化库中注册自定义的类集合，如果要使用 Java 序列化库，需要把该行屏蔽掉<br />conf.set(“spark.kryo.registrator”, “atguigu.com.MyKryoRegistrator”);</p><h3 id="116-常规性能调优六调节本地化等待时长"><a class="markdownIt-Anchor" href="#116-常规性能调优六调节本地化等待时长"></a> 1.1.6 常规性能调优六：调节本地化等待时长</h3><p>Spark 作业运行过程中，Driver 会对每一个 stage 的 task 进行分配。根据 Spark 的 task 分配算法，Spark 希望 task 能够运行在它要计算的数据算在的节点（数据本地化思想），这样就可以避免数据的网络传输。通常来说，task 可能不会被分配到它处理的数据所在的节点，因为这些节点可用的资源可能已经用尽，此时，Spark 会等待一段时间，默认 3s，如果等待指定时间后仍然无法在指定节点运行，那么会自动降级，尝试将 task 分配到比较差的本地化级别所对应的节点上，比如将 task 分配到离它要计算的数据比较近的一个节点，然后进行计算，如果当前级别仍然不行，那么继续降级。<br />当 task 要处理的数据不在 task 所在节点上时，会发生数据的传输。task 会通过所在节点的 BlockManager 获取数据，BlockManager 发现数据不在本地时，户通过网络传输组件从数据所在节点的 BlockManager 处获取数据。<br />网络传输数据的情况是我们不愿意看到的，大量的网络传输会严重影响性能，因此，我们希望通过调节本地化等待时长，如果在等待时长这段时间内，目标节点处理完成了一部分 task，那么当前的 task 将有机会得到执行，这样就能够改善 Spark 作业的整体性能。<br />Spark 的本地化等级如表所示：</p><table><thead><tr><th>名称</th><th>解析</th></tr></thead><tbody><tr><td>PROCESS_LOCAL</td><td>进程本地化，task 和数据在同一个 Executor 中，性能最好。</td></tr><tr><td>NODE_LOCAL</td><td>节点本地化，task 和数据在同一个节点中，但是 task 和数据不在同一个 Executor 中，数据需要在进程间进行传输。</td></tr><tr><td>RACK_LOCAL</td><td>机架本地化，task 和数据在同一个机架的两个节点上，数据需要通过网络在节点之间进行传输。</td></tr><tr><td>NO_PREF</td><td>对于 task 来说，从哪里获取都一样，没有好坏之分。</td></tr><tr><td>ANY</td><td>task 和数据可以在集群的任何地方，而且不在一个机架中，性能最差。</td></tr></tbody></table><p>在 Spark 项目开发阶段，可以使用 client 模式对程序进行测试，此时，可以在本地看到比较全的日志信息，日志信息中有明确的 task 数据本地化的级别，如果大部分都是 PROCESS_LOCAL，那么就无需进行调节，但是如果发现很多的级别都是 NODE_LOCAL、ANY，那么需要对本地化的等待时长进行调节，通过延长本地化等待时长，看看 task 的本地化级别有没有提升，并观察 Spark 作业的运行时间有没有缩短。<br />注意，过犹不及，不要将本地化等待时长延长地过长，导致因为大量的等待时长，使得 Spark 作业的运行时间反而增加了。<br />Spark 本地化等待时长的设置如代码所示：<br />val conf = new SparkConf()<br />.set(“spark.locality.wait”, “6”)</p><h2 id="12-算子调优"><a class="markdownIt-Anchor" href="#12-算子调优"></a> 1.2 算子调优</h2><h3 id="121-算子调优一mappartitions"><a class="markdownIt-Anchor" href="#121-算子调优一mappartitions"></a> 1.2.1 算子调优一：mapPartitions</h3><p>普通的 map 算子对 RDD 中的每一个元素进行操作，而 mapPartitions 算子对 RDD 中每一个分区进行操作。如果是普通的 map 算子，假设一个 partition 有 1 万条数据，那么 map 算子中的 function 要执行 1 万次，也就是对每个元素进行操作。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954373413-2d72ee23-1600-4795-a3b7-33b4d255b59d.png#" alt="" /><br />如果是 mapPartition 算子，由于一个 task 处理一个 RDD 的 partition，那么一个 task 只会执行一次 function，function 一次接收所有的 partition 数据，效率比较高。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954373913-48615c9c-a6e6-4005-8551-378422099873.png#" alt="" /><br />比如，当要把 RDD 中的所有数据通过 JDBC 写入数据，如果使用 map 算子，那么需要对 RDD 中的每一个元素都创建一个数据库连接，这样对资源的消耗很大，如果使用 mapPartitions 算子，那么针对一个分区的数据，只需要建立一个数据库连接。<br />mapPartitions 算子也存在一些缺点：对于普通的 map 操作，一次处理一条数据，如果在处理了 2000 条数据后内存不足，那么可以将已经处理完的 2000 条数据从内存中垃圾回收掉；但是如果使用 mapPartitions 算子，但数据量非常大时，function 一次处理一个分区的数据，如果一旦内存不足，此时无法回收内存，就可能会 OOM，即内存溢出。<br />因此，mapPartitions 算子适用于数据量不是特别大的时候，此时使用 mapPartitions 算子对性能的提升效果还是不错的。（当数据量很大的时候，一旦使用 mapPartitions 算子，就会直接 OOM）<br />在项目中，应该首先估算一下 RDD 的数据量、每个 partition 的数据量，以及分配给每个 Executor 的内存资源，如果资源允许，可以考虑使用 mapPartitions 算子代替 map。</p><h3 id="122-算子调优二foreachpartition-优化数据库操作"><a class="markdownIt-Anchor" href="#122-算子调优二foreachpartition-优化数据库操作"></a> 1.2.2 算子调优二：foreachPartition 优化数据库操作</h3><p>在生产环境中，通常使用 foreachPartition 算子来完成数据库的写入，通过 foreachPartition 算子的特性，可以优化写数据库的性能。<br />如果使用 foreach 算子完成数据库的操作，由于 foreach 算子是遍历 RDD 的每条数据，因此，每条数据都会建立一个数据库连接，这是对资源的极大浪费，因此，对于写数据库操作，我们应当使用 foreachPartition 算子。<br />与 mapPartitions 算子非常相似，foreachPartition 是将 RDD 的每个分区作为遍历对象，一次处理一个分区的数据，也就是说，如果涉及数据库的相关操作，一个分区的数据只需要创建一次数据库连接，如图所示：<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954374395-a7708c2e-13ec-485e-9c55-ea88bbc1a792.png#" alt="" /><br />使用了 foreachPartition 算子后，可以获得以下的性能提升：</p><ul><li>对于我们写的 function 函数，一次处理一整个分区的数据；</li><li>对于一个分区内的数据，创建唯一的数据库连接；</li><li>只需要向数据库发送一次 SQL 语句和多组参数；</li></ul><p>在生产环境中，全部都会使用 foreachPartition 算子完成数据库操作。foreachPartition 算子存在一个问题，与 mapPartitions 算子类似，如果一个分区的数据量特别大，可能会造成 OOM，即内存溢出。</p><h3 id="123-算子调优三filter-与-coalesce-的配合使用"><a class="markdownIt-Anchor" href="#123-算子调优三filter-与-coalesce-的配合使用"></a> 1.2.3 算子调优三：filter 与 coalesce 的配合使用</h3><p>在 Spark 任务中我们经常会使用 filter 算子完成 RDD 中数据的过滤，在任务初始阶段，从各个分区中加载到的数据量是相近的，但是一旦进过 filter 过滤后，每个分区的数据量有可能会存在较大差异，如图所示：<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954374912-63340952-a267-4f7a-8090-86417395c36f.png#" alt="" /><br />根据图中信息我们可以发现两个问题：</p><ul><li>每个 partition 的数据量变小了，如果还按照之前与 partition 相等的 task 个数去处理当前数据，有点浪费 task 的计算资源；</li><li>每个 partition 的数据量不一样，会导致后面的每个 task 处理每个 partition 数据的时候，每个 task 要处理的数据量不同，这很有可能导致数据倾斜问题。</li></ul><p>如上图所示，第二个分区的数据过滤后只剩 100 条，而第三个分区的数据过滤后剩下 800 条，在相同的处理逻辑下，第二个分区对应的 task 处理的数据量与第三个分区对应的 task 处理的数据量差距达到了 8 倍，这也会导致运行速度可能存在数倍的差距，这也就是数据倾斜问题。<br />针对上述的两个问题，我们分别进行分析：</p><ul><li>针对第一个问题，既然分区的数据量变小了，我们希望可以对分区数据进行重新分配，比如将原来 4 个分区的数据转化到 2 个分区中，这样只需要用后面的两个 task 进行处理即可，避免了资源的浪费。</li><li>针对第二个问题，解决方法和第一个问题的解决方法非常相似，对分区数据重新分配，让每个 partition 中的数据量差不多，这就避免了数据倾斜问题。</li></ul><p>那么具体应该如何实现上面的解决思路？我们需要 coalesce 算子。<br />repartition 与 coalesce 都可以用来进行重分区，其中 repartition 只是 coalesce 接口中 shuffle 为 true 的简易实现，coalesce 默认情况下不进行 shuffle，但是可以通过参数进行设置。<br />假设我们希望将原本的分区个数 A 通过重新分区变为 B，那么有以下几种情况：</p><ul><li><strong>A &gt; B（多数分区合并为少数分区）</strong></li></ul><ol><li>A 与 B 相差值不大</li></ol><p>此时使用 coalesce 即可，无需 shuffle 过程。</p><ol><li>A 与 B 相差值很大</li></ol><p>此时可以使用 coalesce 并且不启用 shuffle 过程，但是会导致合并过程性能低下，所以推荐设置 coalesce 的第二个参数为 true，即启动 shuffle 过程。</p><ul><li><strong>A &lt; B（少数分区分解为多数分区）</strong></li></ul><p>此时使用 repartition 即可，如果使用 coalesce 需要将 shuffle 设置为 true，否则 coalesce 无效。<br />我们可以在 filter 操作之后，使用 coalesce 算子针对每个 partition 的数据量各不相同的情况，压缩 partition 的数量，而且让每个 partition 的数据量尽量均匀紧凑，以便于后面的 task 进行计算操作，在某种程度上能够在一定程度上提升性能。<br />注意：local 模式是进程内模拟集群运行，已经对并行度和分区数量有了一定的内部优化，因此不用去设置并行度和分区数量。</p><h3 id="124-算子调优四repartition-解决-sparksql-低并行度问题"><a class="markdownIt-Anchor" href="#124-算子调优四repartition-解决-sparksql-低并行度问题"></a> 1.2.4 算子调优四：repartition 解决 SparkSQL 低并行度问题</h3><p>在第一节的常规性能调优中我们讲解了并行度的调节策略，但是，并行度的设置对于 Spark SQL 是不生效的，用户设置的并行度只对于 Spark SQL 以外的所有 Spark 的 stage 生效。<br />Spark SQL 的并行度不允许用户自己指定，Spark SQL 自己会默认根据 hive 表对应的 HDFS 文件的 split 个数自动设置 Spark SQL 所在的那个 stage 的并行度，用户自己通 spark.default.parallelism 参数指定的并行度，只会在没 Spark SQL 的 stage 中生效。<br />由于 Spark SQL 所在 stage 的并行度无法手动设置，如果数据量较大，并且此 stage 中后续的 transformation 操作有着复杂的业务逻辑，而 Spark SQL 自动设置的 task 数量很少，这就意味着每个 task 要处理为数不少的数据量，然后还要执行非常复杂的处理逻辑，这就可能表现为第一个有 Spark SQL 的 stage 速度很慢，而后续的没有 Spark SQL 的 stage 运行速度非常快。<br />为了解决 Spark SQL 无法设置并行度和 task 数量的问题，我们可以使用 repartition 算子。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954375479-dc004e37-7ee6-4b2a-8a66-92d1a7d3a724.png#" alt="" /><br />Spark SQL 这一步的并行度和 task 数量肯定是没有办法去改变了，但是，对于 Spark SQL 查询出来的 RDD，立即使用 repartition 算子，去重新进行分区，这样可以重新分区为多个 partition，从 repartition 之后的 RDD 操作，由于不再设计 Spark SQL，因此 stage 的并行度就会等于你手动设置的值，这样就避免了 Spark SQL 所在的 stage 只能用少量的 task 去处理大量数据并执行复杂的算法逻辑。</p><h3 id="125-算子调优五reducebykey-预聚合"><a class="markdownIt-Anchor" href="#125-算子调优五reducebykey-预聚合"></a> 1.2.5 算子调优五：reduceByKey 预聚合</h3><p>reduceByKey 相较于普通的 shuffle 操作一个显著的特点就是会进行 map 端的本地聚合，map 端会先对本地的数据进行 combine 操作，然后将数据写入给下个 stage 的每个 task 创建的文件中，也就是在 map 端，对每一个 key 对应的 value，执行 reduceByKey 算子函数。reduceByKey 算子的执行过程如图所示：<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954376007-bce54b1c-9bc1-4ecb-b2e0-6d5094ad4155.png#" alt="" /><br />使用 reduceByKey 对性能的提升如下：</p><ul><li>本地聚合后，在 map 端的数据量变少，减少了磁盘 IO，也减少了对磁盘空间的占用；</li><li>本地聚合后，下一个 stage 拉取的数据量变少，减少了网络传输的数据量；</li><li>本地聚合后，在 reduce 端进行数据缓存的内存占用减少；</li><li>本地聚合后，在 reduce 端进行聚合的数据量减少。</li></ul><p>基于 reduceByKey 的本地聚合特征，我们应该考虑使用 reduceByKey 代替其他的 shuffle 算子，例如 groupByKey。reduceByKey 与 groupByKey 的运行原理如图所示：<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954376502-4013d4e2-1ce3-4fb8-b23f-340ba7a61f76.png#" alt="" /><br />groupByKey 原理<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954377035-cb56a059-4845-41a8-8bc8-881ac95fa05e.png#" alt="" /><br />reduceByKey 原理<br />根据上图可知，groupByKey 不会进行 map 端的聚合，而是将所有 map 端的数据 shuffle 到 reduce 端，然后在 reduce 端进行数据的聚合操作。由于 reduceByKey 有 map 端聚合的特性，使得网络传输的数据量减小，因此效率要明显高于 groupByKey。</p><h2 id="13-shuffle-调优"><a class="markdownIt-Anchor" href="#13-shuffle-调优"></a> 1.3 Shuffle 调优</h2><h3 id="131-shuffle-调优一调节-map-端缓冲区大小"><a class="markdownIt-Anchor" href="#131-shuffle-调优一调节-map-端缓冲区大小"></a> 1.3.1 Shuffle 调优一：调节 map 端缓冲区大小</h3><p>在 Spark 任务运行过程中，如果 shuffle 的 map 端处理的数据量比较大，但是 map 端缓冲的大小是固定的，可能会出现 map 端缓冲数据频繁 spill 溢写到磁盘文件中的情况，使得性能非常低下，通过调节 map 端缓冲的大小，可以避免频繁的磁盘 IO 操作，进而提升 Spark 任务的整体性能。<br />map 端缓冲的默认配置是 32KB，如果每个 task 处理 640KB 的数据，那么会发生 640/32 = 20 次溢写，如果每个 task 处理 64000KB 的数据，机会发生 64000/32=2000 此溢写，这对于性能的影响是非常严重的。<br />map 端缓冲的配置方法如代码清单所示：<br />val conf = new SparkConf()<br />.set(“spark.shuffle.file.buffer”, “64”)</p><h3 id="132-shuffle-调优二调节-reduce-端拉取数据缓冲区大小"><a class="markdownIt-Anchor" href="#132-shuffle-调优二调节-reduce-端拉取数据缓冲区大小"></a> 1.3.2 Shuffle 调优二：调节 reduce 端拉取数据缓冲区大小</h3><p>Spark Shuffle 过程中，shuffle reduce task 的 buffer 缓冲区大小决定了 reduce task 每次能够缓冲的数据量，也就是每次能够拉取的数据量，如果内存资源较为充足，适当增加拉取数据缓冲区的大小，可以减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。<br />reduce 端数据拉取缓冲区的大小可以通过 spark.reducer.maxSizeInFlight 参数进行设置，默认为 48MB，该参数的设置方法如代码清单所示：<br />val conf = new SparkConf()<br />.set(“spark.reducer.maxSizeInFlight”, “96”)</p><h3 id="133-shuffle-调优三调节-reduce-端拉取数据重试次数"><a class="markdownIt-Anchor" href="#133-shuffle-调优三调节-reduce-端拉取数据重试次数"></a> 1.3.3 Shuffle 调优三：调节 reduce 端拉取数据重试次数</h3><p>Spark Shuffle 过程中，reduce task 拉取属于自己的数据时，如果因为网络异常等原因导致失败会自动进行重试。对于那些包含了特别耗时的 shuffle 操作的作业，建议增加重试最大次数（比如 60 次），以避免由于 JVM 的 full gc 或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的 shuffle 过程，调节该参数可以大幅度提升稳定性。<br />reduce 端拉取数据重试次数可以通过 spark.shuffle.io.maxRetries 参数进行设置，该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败，默认为 3，该参数的设置方法如代码清单所示：<br />val conf = new SparkConf()<br />.set(“spark.shuffle.io.maxRetries”, “6”)</p><h3 id="134-shuffle-调优四调节-reduce-端拉取数据等待间隔"><a class="markdownIt-Anchor" href="#134-shuffle-调优四调节-reduce-端拉取数据等待间隔"></a> 1.3.4 Shuffle 调优四：调节 reduce 端拉取数据等待间隔</h3><p>Spark Shuffle 过程中，reduce task 拉取属于自己的数据时，如果因为网络异常等原因导致失败会自动进行重试，在一次失败后，会等待一定的时间间隔再进行重试，可以通过加大间隔时长（比如 60s），以增加 shuffle 操作的稳定性。<br />reduce 端拉取数据等待间隔可以通过 spark.shuffle.io.retryWait 参数进行设置，默认值为 5s，该参数的设置方法如代码清单所示：<br />val conf = new SparkConf()<br />.set(“spark.shuffle.io.retryWait”, &quot;20s”</p><h3 id="135-shuffle-调优五调节-sortshuffle-排序操作阈值"><a class="markdownIt-Anchor" href="#135-shuffle-调优五调节-sortshuffle-排序操作阈值"></a> 1.3.5 Shuffle 调优五：调节 SortShuffle 排序操作阈值</h3><p>对于 SortShuffleManager，如果 shuffle reduce task 的数量小于某一阈值则 shuffle write 过程中不会进行排序操作，而是直接按照未经优化的 HashShuffleManager 的方式去写数据，但是最后会将每个 task 产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。<br />当你使用 SortShuffleManager 时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于 shuffle read task 的数量，那么此时 map-side 就不会进行排序了，减少了排序的性能开销，但是这种方式下，依然会产生大量的磁盘文件，因此 shuffle write 性能有待提高。<br />SortShuffleManager 排序操作阈值的设置可以通过 spark.shuffle.sort. bypassMergeThreshold 这一参数进行设置，默认值为 200，该参数的设置方法如代码清单所示：<br />val conf = new SparkConf()<br />.set(“spark.shuffle.sort.bypassMergeThreshold”, “500”)</p><h2 id="14-jvm-调优"><a class="markdownIt-Anchor" href="#14-jvm-调优"></a> 1.4 JVM 调优</h2><p>对于 JVM 调优，首先应该明确，full gc/minor gc，都会导致 JVM 的工作线程停止工作，即 stop the world。</p><h3 id="141-jvm-调优一降低-cache-操作的内存占比"><a class="markdownIt-Anchor" href="#141-jvm-调优一降低-cache-操作的内存占比"></a> 1.4.1 JVM 调优一：降低 cache 操作的内存占比</h3><ol><li><pre><code> 静态内存管理机制</code></pre>根据 Spark 静态内存管理机制，堆内存被划分为了两块，Storage 和 Execution。Storage 主要用于缓存 RDD 数据和 broadcast 数据，Execution 主要用于缓存在 shuffle 过程中产生的中间数据，Storage 占系统内存的 60%，Execution 占系统内存的 20%，并且两者完全独立。<br />在一般情况下，Storage 的内存都提供给了 cache 操作，但是如果在某些情况下 cache 操作内存不是很紧张，而 task 的算子中创建的对象很多，Execution 内存又相对较小，这回导致频繁的 minor gc，甚至于频繁的 full gc，进而导致 Spark 频繁的停止工作，性能影响会很大。<br />在 Spark UI 中可以查看每个 stage 的运行情况，包括每个 task 的运行时间、gc 时间等等，如果发现 gc 太频繁，时间太长，就可以考虑调节 Storage 的内存占比，让 task 执行算子函数式，有更多的内存可以使用。<br />Storage 内存区域可以通过 spark.storage.memoryFraction 参数进行指定，默认为 0.6，即 60%，可以逐级向下递减，如代码清单所示：<br />val conf = new SparkConf()<br />.set(“spark.storage.memoryFraction”, “0.4”)</li><li><pre><code> 统一内存管理机制</code></pre>根据 Spark 统一内存管理机制，堆内存被划分为了两块，Storage 和 Execution。Storage 主要用于缓存数据，Execution 主要用于缓存在 shuffle 过程中产生的中间数据，两者所组成的内存部分称为统一内存，Storage 和 Execution 各占统一内存的 50%，由于动态占用机制的实现，shuffle 过程需要的内存过大时，会自动占用 Storage 的内存区域，因此无需手动进行调节。</li></ol><h3 id="142-jvm-调优二调节-executor-堆外内存"><a class="markdownIt-Anchor" href="#142-jvm-调优二调节-executor-堆外内存"></a> 1.4.2 JVM 调优二：调节 Executor 堆外内存</h3><p>Executor 的堆外内存主要用于程序的共享库、Perm Space、 线程 Stack 和一些 Memory mapping 等, 或者类 C 方式 allocate object。<br />有时，如果你的 Spark 作业处理的数据量非常大，达到几亿的数据量，此时运行 Spark 作业会时不时地报错，例如 shuffle output file cannot find，executor lost，task lost，out of memory 等，这可能是 Executor 的堆外内存不太够用，导致 Executor 在运行的过程中内存溢出。<br />stage 的 task 在运行的时候，可能要从一些 Executor 中去拉取 shuffle map output 文件，但是 Executor 可能已经由于内存溢出挂掉了，其关联的 BlockManager 也没有了，这就可能会报出 shuffle output file cannot find，executor lost，task lost，out of memory 等错误，此时，就可以考虑调节一下 Executor 的堆外内存，也就可以避免报错，与此同时，堆外内存调节的比较大的时候，对于性能来讲，也会带来一定的提升。<br />默认情况下，Executor 堆外内存上限大概为 300 多 MB，在实际的生产环境下，对海量数据进行处理的时候，这里都会出现问题，导致 Spark 作业反复崩溃，无法运行，此时就会去调节这个参数，到至少 1G，甚至于 2G、4G。<br />Executor 堆外内存的配置需要在 spark-submit 脚本里配置，如代码清单所示：<br />–conf spark.yarn.executor.memoryOverhead=2048<br />以上参数配置完成后，会避免掉某些 JVM OOM 的异常问题，同时，可以提升整体 Spark 作业的性能。</p><h3 id="143-jvm-调优三调节连接等待时长"><a class="markdownIt-Anchor" href="#143-jvm-调优三调节连接等待时长"></a> 1.4.3 JVM 调优三：调节连接等待时长</h3><p>在 Spark 作业运行过程中，Executor 优先从自己本地关联的 BlockManager 中获取某份数据，如果本地 BlockManager 没有的话，会通过 TransferService 远程连接其他节点上 Executor 的 BlockManager 来获取数据。<br />如果 task 在运行过程中创建大量对象或者创建的对象较大，会占用大量的内存，这回导致频繁的垃圾回收，但是垃圾回收会导致工作线程全部停止，也就是说，垃圾回收一旦执行，Spark 的 Executor 进程就会停止工作，无法提供相应，此时，由于没有响应，无法建立网络连接，会导致网络连接超时。<br />在生产环境下，有时会遇到 file not found、file lost 这类错误，在这种情况下，很有可能是 Executor 的 BlockManager 在拉取数据的时候，无法建立连接，然后超过默认的连接等待时长 60s 后，宣告数据拉取失败，如果反复尝试都拉取不到数据，可能会导致 Spark 作业的崩溃。这种情况也可能会导致 DAGScheduler 反复提交几次 stage，TaskScheduler 返回提交几次 task，大大延长了我们的 Spark 作业的运行时间。<br />此时，可以考虑调节连接的超时时长，连接等待时长需要在 spark-submit 脚本中进行设置，设置方式如代码清单所示：<br />–conf spark.core.connection.ack.wait.timeout=300<br />调节连接等待时长后，通常可以避免部分的 XX 文件拉取失败、XX 文件 lost 等报错。</p><h1 id="第-2-章-spark-数据倾斜"><a class="markdownIt-Anchor" href="#第-2-章-spark-数据倾斜"></a> 第 2 章 Spark 数据倾斜</h1><p>Spark 中的数据倾斜问题主要指 shuffle 过程中出现的数据倾斜问题，是由于不同的 key 对应的数据量不同导致的不同 task 所处理的数据量不同的问题。<br />例如，reduce 点一共要处理 100 万条数据，第一个和第二个 task 分别被分配到了 1 万条数据，计算 5 分钟内完成，第三个 task 分配到了 98 万数据，此时第三个 task 可能需要 10 个小时完成，这使得整个 Spark 作业需要 10 个小时才能运行完成，这就是数据倾斜所带来的后果。<br />注意，要区分开数据倾斜与数据量过量这两种情况，数据倾斜是指少数 task 被分配了绝大多数的数据，因此少数 task 运行缓慢；数据过量是指所有 task 被分配的数据量都很大，相差不多，所有 task 都运行缓慢。<br /><strong>数据倾斜的表现：</strong></p><ul><li>Spark 作业的大部分 task 都执行迅速，只有有限的几个 task 执行的非常慢，此时可能出现了数据倾斜，作业可以运行，但是运行得非常慢；</li><li>Spark 作业的大部分 task 都执行迅速，但是有的 task 在运行过程中会突然报出 OOM，反复执行几次都在某一个 task 报出 OOM 错误，此时可能出现了数据倾斜，作业无法正常运行。</li></ul><p><strong>定位数据倾斜问题：</strong></p><ul><li>查阅代码中的 shuffle 算子，例如 reduceByKey、countByKey、groupByKey、join 等算子，根据代码逻辑判断此处是否会出现数据倾斜；</li><li>查看 Spark 作业的 log 文件，log 文件对于错误的记录会精确到代码的某一行，可以根据异常定位到的代码位置来明确错误发生在第几个 stage，对应的 shuffle 算子是哪一个；</li></ul><h2 id="21-解决方案一聚合原数据"><a class="markdownIt-Anchor" href="#21-解决方案一聚合原数据"></a> 2.1 解决方案一：聚合原数据</h2><ol><li>避免 shuffle 过程</li></ol><p>绝大多数情况下，Spark 作业的数据来源都是 Hive 表，这些 Hive 表基本都是经过 ETL 之后的昨天的数据。为了避免数据倾斜，我们可以考虑避免 shuffle 过程，如果避免了 shuffle 过程，那么从根本上就消除了发生数据倾斜问题的可能。<br />如果 Spark 作业的数据来源于 Hive 表，那么可以先在 Hive 表中对数据进行聚合，例如按照 key 进行分组，将同一 key 对应的所有 value 用一种特殊的格式拼接到一个字符串里去，这样，一个 key 就只有一条数据了；之后，对一个 key 的所有 value 进行处理时，只需要进行 map 操作即可，无需再进行任何的 shuffle 操作。通过上述方式就避免了执行 shuffle 操作，也就不可能会发生任何的数据倾斜问题。<br />对于 Hive 表中数据的操作，不一定是拼接成一个字符串，也可以是直接对 key 的每一条数据进行累计计算。<br />要区分开，处理的数据量大和数据倾斜的区别。</p><ol><li>缩小 key 粒度（增大数据倾斜可能性，降低每个 task 的数据量）</li></ol><p>key 的数量增加，可能使数据倾斜更严重。</p><ol><li>增大 key 粒度（减小数据倾斜可能性，增大每个 task 的数据量）</li></ol><p>如果没有办法对每个 key 聚合出来一条数据，在特定场景下，可以考虑扩大 key 的聚合粒度。<br />例如，目前有 10 万条用户数据，当前 key 的粒度是（省，城市，区，日期），现在我们考虑扩大粒度，将 key 的粒度扩大为（省，城市，日期），这样的话，key 的数量会减少，key 之间的数据量差异也有可能会减少，由此可以减轻数据倾斜的现象和问题。（此方法只针对特定类型的数据有效，当应用场景不适宜时，会加重数据倾斜）</p><h2 id="22-解决方案二过滤导致倾斜的-key"><a class="markdownIt-Anchor" href="#22-解决方案二过滤导致倾斜的-key"></a> 2.2 解决方案二：过滤导致倾斜的 key</h2><p>如果在 Spark 作业中允许丢弃某些数据，那么可以考虑将可能导致数据倾斜的 key 进行过滤，滤除可能导致数据倾斜的 key 对应的数据，这样，在 Spark 作业中就不会发生数据倾斜了。</p><h2 id="23-解决方案三提高-shuffle-操作中的-reduce-并行度"><a class="markdownIt-Anchor" href="#23-解决方案三提高-shuffle-操作中的-reduce-并行度"></a> 2.3 解决方案三：提高 shuffle 操作中的 reduce 并行度</h2><p>当方案一和方案二对于数据倾斜的处理没有很好的效果时，可以考虑提高 shuffle 过程中的 reduce 端并行度，reduce 端并行度的提高就增加了 reduce 端 task 的数量，那么每个 task 分配到的数据量就会相应减少，由此缓解数据倾斜问题。</p><ul><li><strong>reduce 端并行度的设置</strong></li></ul><p>在大部分的 shuffle 算子中，都可以传入一个并行度的设置参数，比如 reduceByKey(500)，这个参数会决定 shuffle 过程中 reduce 端的并行度，在进行 shuffle 操作的时候，就会对应着创建指定数量的 reduce task。对于 Spark SQL 中的 shuffle 类语句，比如 group by、join 等，需要设置一个参数，即 spark.sql.shuffle.partitions，该参数代表了 shuffle read task 的并行度，该值默认是 200，对于很多场景来说都有点过小。<br />增加 shuffle read task 的数量，可以让原本分配给一个 task 的多个 key 分配给多个 task，从而让每个 task 处理比原来更少的数据。举例来说，如果原本有 5 个 key，每个 key 对应 10 条数据，这 5 个 key 都是分配给一个 task 的，那么这个 task 就要处理 50 条数据。而增加了 shuffle read task 以后，每个 task 就分配到一个 key，即每个 task 就处理 10 条数据，那么自然每个 task 的执行时间都会变短了。</p><ul><li><strong>reduce 端并行度设置存在的缺陷</strong></li></ul><p>提高 reduce 端并行度并没有从根本上改变数据倾斜的本质和问题（方案一和方案二从根本上避免了数据倾斜的发生），只是尽可能地去缓解和减轻 shuffle reduce task 的数据压力，以及数据倾斜的问题，适用于有较多 key 对应的数据量都比较大的情况。<br />该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个 key 对应的数据量有 100 万，那么无论你的 task 数量增加到多少，这个对应着 100 万数据的 key 肯定还是会分配到一个 task 中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用嘴简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。<br />在理想情况下，reduce 端并行度提升后，会在一定程度上减轻数据倾斜的问题，甚至基本消除数据倾斜；但是，在一些情况下，只会让原来由于数据倾斜而运行缓慢的 task 运行速度稍有提升，或者避免了某些 task 的 OOM 问题，但是，仍然运行缓慢，此时，要及时放弃方案三，开始尝试后面的方案。</p><h2 id="24-解决方案四使用随机-key-实现双重聚合"><a class="markdownIt-Anchor" href="#24-解决方案四使用随机-key-实现双重聚合"></a> 2.4 解决方案四：使用随机 key 实现双重聚合</h2><p>当使用了类似于 groupByKey、reduceByKey 这样的算子时，可以考虑使用随机 key 实现双重聚合，如图所示：<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954377574-9010b62f-82e8-4761-bc3d-1e5490210742.png#" alt="" /><br />首先，通过 map 算子给每个数据的 key 添加随机数前缀，对 key 进行打散，将原先一样的 key 变成不一样的 key，然后进行第一次聚合，这样就可以让原本被一个 task 处理的数据分散到多个 task 上去做局部聚合；随后，去除掉每个 key 的前缀，再次进行聚合。<br />此方法对于由 groupByKey、reduceByKey 这类算子造成的数据倾斜由比较好的效果，仅仅适用于聚合类的 shuffle 操作，适用范围相对较窄。如果是 join 类的 shuffle 操作，还得用其他的解决方案。<br />此方法也是前几种方案没有比较好的效果时要尝试的解决方案。</p><h2 id="25-解决方案五将-reduce-join-转换为-map-join"><a class="markdownIt-Anchor" href="#25-解决方案五将-reduce-join-转换为-map-join"></a> 2.5 解决方案五：将 reduce join 转换为 map join</h2><p>正常情况下，join 操作都会执行 shuffle 过程，并且执行的是 reduce join，也就是先将所有相同的 key 和对应的 value 汇聚到一个 reduce task 中，然后再进行 join。普通 join 的过程如下图所示：<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954378098-d3482c0e-5af8-48e2-bcb6-19ff21d888d8.png#" alt="" /><br />普通的 join 是会走 shuffle 过程的，而一旦 shuffle，就相当于会将相同 key 的数据拉取到一个 shuffle read task 中再进行 join，此时就是 reduce join。但是如果一个 RDD 是比较小的，则可以采用广播小 RDD 全量数据+map 算子来实现与 join 同样的效果，也就是 map join，此时就不会发生 shuffle 操作，也就不会发生数据倾斜。<br />（注意，RDD 是并不能进行广播的，只能将 RDD 内部的数据通过 collect 拉取到 Driver 内存然后再进行广播）<br /><strong>核心思路：</strong><br />不使用 join 算子进行连接操作，而使用 Broadcast 变量与 map 类算子实现 join 操作，进而完全规避掉 shuffle 类的操作，彻底避免数据倾斜的发生和出现。将较小 RDD 中的数据直接通过 collect 算子拉取到 Driver 端的内存中来，然后对其创建一个 Broadcast 变量；接着对另外一个 RDD 执行 map 类算子，在算子函数内，从 Broadcast 变量中获取较小 RDD 的全量数据，与当前 RDD 的每一条数据按照连接 key 进行比对，如果连接 key 相同的话，那么就将两个 RDD 的数据用你需要的方式连接起来。<br />根据上述思路，根本不会发生 shuffle 操作，从根本上杜绝了 join 操作可能导致的数据倾斜问题。<br />当 join 操作有数据倾斜问题并且其中一个 RDD 的数据量较小时，可以优先考虑这种方式，效果非常好。map join 的过程如图所示：<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954378604-5a455900-c756-409a-9664-7f44abf34817.png#" alt="" /><br /><strong>不适用场景分析：</strong><br />由于 Spark 的广播变量是在每个 Executor 中保存一个副本，如果两个 RDD 数据量都比较大，那么如果将一个数据量比较大的 RDD 做成广播变量，那么很有可能会造成内存溢出。</p><h2 id="26-解决方案六sample-采样对倾斜-key-单独进行-join"><a class="markdownIt-Anchor" href="#26-解决方案六sample-采样对倾斜-key-单独进行-join"></a> 2.6 解决方案六：sample 采样对倾斜 key 单独进行 join</h2><p>在 Spark 中，如果某个 RDD 只有一个 key，那么在 shuffle 过程中会默认将此 key 对应的数据打散，由不同的 reduce 端 task 进行处理。<br />当由单个 key 导致数据倾斜时，可有将发生数据倾斜的 key 单独提取出来，组成一个 RDD，然后用这个原本会导致倾斜的 key 组成的 RDD 根其他 RDD 单独 join，此时，根据 Spark 的运行机制，此 RDD 中的数据会在 shuffle 阶段被分散到多个 task 中去进行 join 操作。倾斜 key 单独 join 的流程如图所示：<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954379146-204c1d66-35d6-4605-80f0-b01a302a05d7.png#" alt="" /></p><ol><li>适用场景分析：</li></ol><p>对于 RDD 中的数据，可以将其转换为一个中间表，或者是直接使用 countByKey()的方式，看一个这个 RDD 中各个 key 对应的数据量，此时如果你发现整个 RDD 就一个 key 的数据量特别多，那么就可以考虑使用这种方法。<br />当数据量非常大时，可以考虑使用 sample 采样获取 10%的数据，然后分析这 10%的数据中哪个 key 可能会导致数据倾斜，然后将这个 key 对应的数据单独提取出来。</p><ol><li>不适用场景分析：</li></ol><p>如果一个 RDD 中导致数据倾斜的 key 很多，那么此方案不适用。</p><h2 id="27-解决方案七使用随机数扩容进行-join"><a class="markdownIt-Anchor" href="#27-解决方案七使用随机数扩容进行-join"></a> 2.7 解决方案七：使用随机数扩容进行 join</h2><p>如果在进行 join 操作时，RDD 中有大量的 key 导致数据倾斜，那么进行分拆 key 也没什么意义，此时就只能使用最后一种方案来解决问题了，对于 join 操作，我们可以考虑对其中一个 RDD 数据进行扩容，另一个 RDD 进行稀释后再 join。<br />我们会将原先一样的 key 通过附加随机前缀变成不一样的 key，然后就可以将这些处理后的“不同 key”分散到多个 task 中去处理，而不是让一个 task 处理大量的相同 key。这一种方案是针对有大量倾斜 key 的情况，没法将部分 key 拆分出来进行单独处理，需要对整个 RDD 进行数据扩容，对内存资源要求很高。<br />1**. 核心思想：**<br />选择一个 RDD，使用 flatMap 进行扩容，对每条数据的 key 添加数值前缀（1~N 的数值），将一条数据映射为多条数据；（扩容）<br />选择另外一个 RDD，进行 map 映射操作，每条数据的 key 都打上一个随机数作为前缀（1~N 的随机数）；（稀释）<br />将两个处理后的 RDD，进行 join 操作。<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954379608-493c083d-4d33-452b-80c0-891a45fdc0ed.png#" alt="" /><br /><strong>2. 局限性：</strong><br />如果两个 RDD 都很大，那么将 RDD 进行 N 倍的扩容显然行不通；<br />使用扩容的方式只能缓解数据倾斜，不能彻底解决数据倾斜问题。<br />使用方案七对方案六进一步优化分析：<br />当 RDD 中有几个 key 导致数据倾斜时，方案六不再适用，而方案七又非常消耗资源，此时可以引入方案七的思想完善方案六：</p><ul><li>对包含少数几个数据量过大的 key 的那个 RDD，通过 sample 算子采样出一份样本来，然后统计一下每个 key 的数量，计算出来数据量最大的是哪几个 key。</li><li>然后将这几个 key 对应的数据从原来的 RDD 中拆分出来，形成一个单独的 RDD，并给每个 key 都打上 n 以内的随机数作为前缀，而不会导致倾斜的大部分 key 形成另外一个 RDD。</li><li>接着将需要 join 的另一个 RDD，也过滤出来那几个倾斜 key 对应的数据并形成一个单独的 RDD，将每条数据膨胀成 n 条数据，这 n 条数据都按顺序附加一个 0~n 的前缀，不会导致倾斜的大部分 key 也形成另外一个 RDD。</li><li>再将附加了随机前缀的独立 RDD 与另一个膨胀 n 倍的独立 RDD 进行 join，此时就可以将原先相同的 key 打散成 n 份，分散到多个 task 中去进行 join 了。</li><li>而另外两个普通的 RDD 就照常 join 即可。</li><li>最后将两次 join 的结果使用 union 算子合并起来即可，就是最终的 join 结果。</li></ul><h1 id="第-3-章-spark-故障排除"><a class="markdownIt-Anchor" href="#第-3-章-spark-故障排除"></a> 第 3 章 Spark 故障排除</h1><h2 id="31-故障排除一控制-reduce-端缓冲大小以避免-oom"><a class="markdownIt-Anchor" href="#31-故障排除一控制-reduce-端缓冲大小以避免-oom"></a> 3.1 故障排除一：控制 reduce 端缓冲大小以避免 OOM</h2><p>在 Shuffle 过程，reduce 端 task 并不是等到 map 端 task 将其数据全部写入磁盘后再去拉取，而是 map 端写一点数据，reduce 端 task 就会拉取一小部分数据，然后立即进行后面的聚合、算子函数的使用等操作。<br />reduce 端 task 能够拉取多少数据，由 reduce 拉取数据的缓冲区 buffer 来决定，因为拉取过来的数据都是先放在 buffer 中，然后再进行后续的处理，buffer 的默认大小为 48MB。<br />reduce 端 task 会一边拉取一边计算，不一定每次都会拉满 48MB 的数据，可能大多数时候拉取一部分数据就处理掉了。<br />虽然说增大 reduce 端缓冲区大小可以减少拉取次数，提升 Shuffle 性能，但是有时 map 端的数据量非常大，写出的速度非常快，此时 reduce 端的所有 task 在拉取的时候，有可能全部达到自己缓冲的最大极限值，即 48MB，此时，再加上 reduce 端执行的聚合函数的代码，可能会创建大量的对象，这可难会导致内存溢出，即 OOM。<br />如果一旦出现 reduce 端内存溢出的问题，我们可以考虑减小 reduce 端拉取数据缓冲区的大小，例如减少为 12MB。<br />在实际生产环境中是出现过这种问题的，这是典型的以性能换执行的原理。reduce 端拉取数据的缓冲区减小，不容易导致 OOM，但是相应的，reudce 端的拉取次数增加，造成更多的网络传输开销，造成性能的下降。<br />注意，要保证任务能够运行，再考虑性能的优化。</p><h2 id="32-故障排除二jvm-gc-导致的-shuffle-文件拉取失败"><a class="markdownIt-Anchor" href="#32-故障排除二jvm-gc-导致的-shuffle-文件拉取失败"></a> 3.2 故障排除二：JVM GC 导致的 shuffle 文件拉取失败</h2><pre><code>在Spark作业中，有时会出现shuffle file not found的错误，这是非常常见的一个报错，有时出现这种错误以后，选择重新执行一遍，就不再报出这种错误。出现上述问题可能的原因是Shuffle操作中，后面stage的task想要去上一个stage的task所在的Executor拉取数据，结果对方正在执行GC，执行GC会导致Executor内所有的工作线程全部停止，比如BlockManager、基于netty的网络通信等，这就会导致后面的task拉取数据拉取了半天都没有拉取到，就会报出shuffle file not found的错误，而第二次再次执行就不会再出现这种错误。可以通过调整reduce端拉取数据重试次数和reduce端拉取数据时间间隔这两个参数来对Shuffle性能进行调整，增大参数值，使得reduce端拉取数据的重试次数增加，并且每次失败后等待的时间间隔加长。</code></pre><p>val conf = new SparkConf()<br />.set(“spark.shuffle.io.maxRetries”, “60”)<br />.set(“spark.shuffle.io.retryWait”, “60s”)</p><h2 id="33-故障排除三解决各种序列化导致的报错"><a class="markdownIt-Anchor" href="#33-故障排除三解决各种序列化导致的报错"></a> 3.3 故障排除三：解决各种序列化导致的报错</h2><pre><code>当Spark作业在运行过程中报错，而且报错信息中含有Serializable等类似词汇，那么可能是序列化问题导致的报错。序列化问题要注意以下三点：</code></pre><ul><li>作为 RDD 的元素类型的自定义类，必须是可以序列化的；</li><li>算子函数里可以使用的外部的自定义变量，必须是可以序列化的；</li><li>不可以在 RDD 的元素类型、算子函数里使用第三方的不支持序列化的类型，例如 Connection。</li></ul><h2 id="34-故障排除四解决算子函数返回-null-导致的问题"><a class="markdownIt-Anchor" href="#34-故障排除四解决算子函数返回-null-导致的问题"></a> 3.4 故障排除四：解决算子函数返回 NULL 导致的问题</h2><p>在一些算子函数里，需要我们有一个返回值，但是在一些情况下我们不希望有返回值，此时我们如果直接返回 NULL，会报错，例如 Scala.Math(NULL)异常。<br />如果你遇到某些情况，不希望有返回值，那么可以通过下述方式解决：</p><ul><li>返回特殊值，不返回 NULL，例如“-1”；</li><li>在通过算子获取到了一个 RDD 之后，可以对这个 RDD 执行 filter 操作，进行数据过滤，将数值为-1 的数据给过滤掉；</li><li>在使用完 filter 算子后，继续调用 coalesce 算子进行优化。</li></ul><h2 id="35-故障排除五解决-yarn-client-模式导致的网卡流量激增问题"><a class="markdownIt-Anchor" href="#35-故障排除五解决-yarn-client-模式导致的网卡流量激增问题"></a> 3.5 故障排除五：解决 YARN-CLIENT 模式导致的网卡流量激增问题</h2><p>YARN-client 模式的运行原理如下图所示：<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954380314-95ddbc7e-5e4f-4971-ba0e-cf2fdfa7d105.png#" alt="" /><br />在 YARN-client 模式下，Driver 启动在本地机器上，而 Driver 负责所有的任务调度，需要与 YARN 集群上的多个 Executor 进行频繁的通信。<br />假设有 100 个 Executor， 1000 个 task，那么每个 Executor 分配到 10 个 task，之后，Driver 要频繁地跟 Executor 上运行的 1000 个 task 进行通信，通信数据非常多，并且通信品类特别高。这就导致有可能在 Spark 任务运行过程中，由于频繁大量的网络通讯，本地机器的网卡流量会激增。<br />注意，YARN-client 模式只会在测试环境中使用，而之所以使用 YARN-client 模式，是由于可以看到详细全面的 log 信息，通过查看 log，可以锁定程序中存在的问题，避免在生产环境下发生故障。<br />在生产环境下，使用的一定是 YARN-cluster 模式。在 YARN-cluster 模式下，就不会造成本地机器网卡流量激增问题，如果 YARN-cluster 模式下存在网络通信的问题，需要运维团队进行解决。</p><h2 id="36-故障排除六解决-yarn-cluster-模式的-jvm-栈内存溢出无法执行问题"><a class="markdownIt-Anchor" href="#36-故障排除六解决-yarn-cluster-模式的-jvm-栈内存溢出无法执行问题"></a> 3.6 故障排除六：解决 YARN-CLUSTER 模式的 JVM 栈内存溢出无法执行问题</h2><p>YARN-cluster 模式的运行原理如下图所示：<br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1620954380695-72e0eaf6-db21-4de4-8801-c9eaf2ecd6ff.png#" alt="" /><br />当 Spark 作业中包含 SparkSQL 的内容时，可能会碰到 YARN-client 模式下可以运行，但是 YARN-cluster 模式下无法提交运行（报出 OOM 错误）的情况。<br />YARN-client 模式下，Driver 是运行在本地机器上的，Spark 使用的 JVM 的 PermGen 的配置，是本地机器上的 spark-class 文件，JVM 永久代的大小是 128MB，这个是没有问题的，但是在 YARN-cluster 模式下，Driver 运行在 YARN 集群的某个节点上，使用的是没有经过配置的默认设置，PermGen 永久代大小为 82MB。<br />SparkSQL 的内部要进行很复杂的 SQL 的语义解析、语法树转换等等，非常复杂，如果 sql 语句本身就非常复杂，那么很有可能会导致性能的损耗和内存的占用，特别是对 PermGen 的占用会比较大。<br />所以，此时如果 PermGen 的占用好过了 82MB，但是又小于 128MB，就会出现 YARN-client 模式下可以运行，YARN-cluster 模式下无法运行的情况。<br />解决上述问题的方法时增加 PermGen 的容量，需要在 spark-submit 脚本中对相关参数进行设置，设置方法如代码清单所示。<br />–conf spark.driver.extraJavaOptions=&quot;-XX:PermSize=128M -XX:MaxPermSize=256M&quot;<br />通过上述方法就设置了 Driver 永久代的大小，默认为 128MB，最大 256MB，这样就可以避免上面所说的问题。</p><h2 id="37-故障排除七解决-sparksql-导致的-jvm-栈内存溢出"><a class="markdownIt-Anchor" href="#37-故障排除七解决-sparksql-导致的-jvm-栈内存溢出"></a> 3.7 故障排除七：解决 SparkSQL 导致的 JVM 栈内存溢出</h2><pre><code>当SparkSQL的sql语句有成百上千的or关键字时，就可能会出现Driver端的JVM栈内存溢出。JVM栈内存溢出基本上就是由于调用的方法层级过多，产生了大量的，非常深的，超出了JVM栈深度限制的递归。（我们猜测SparkSQL有大量or语句的时候，在解析SQL时，例如转换为语法树或者进行执行计划的生成的时候，对于or的处理是递归，or非常多时，会发生大量的递归）此时，建议将一条sql语句拆分为多条sql语句来执行，每条sql语句尽量保证100个以内的子句。根据实际的生产环境试验，一条sql语句的or关键字控制在100个以内，通常不会导致JVM栈内存溢出。</code></pre><h2 id="38-故障排除八持久化与-checkpoint-的使用"><a class="markdownIt-Anchor" href="#38-故障排除八持久化与-checkpoint-的使用"></a> 3.8 故障排除八：持久化与 checkpoint 的使用</h2><p>Spark 持久化在大部分情况下是没有问题的，但是有时数据可能会丢失，如果数据一旦丢失，就需要对丢失的数据重新进行计算，计算完后再缓存和使用，为了避免数据的丢失，可以选择对这个 RDD 进行 checkpoint，也就是将数据持久化一份到容错的文件系统上（比如 HDFS）。<br />一个 RDD 缓存并 checkpoint 后，如果一旦发现缓存丢失，就会优先查看 checkpoint 数据存不存在，如果有，就会使用 checkpoint 数据，而不用重新计算。也即是说，checkpoint 可以视为 cache 的保障机制，如果 cache 失败，就使用 checkpoint 的数据。<br />使用 checkpoint 的优点在于提高了 Spark 作业的可靠性，一旦缓存出现问题，不需要重新计算数据，缺点在于，checkpoint 时需要将数据写入 HDFS 等文件系统，对性能的消耗较大。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="posts/0/"/>
      <url>posts/0/</url>
      
        <content type="html"><![CDATA[<p>我来介绍一下我做的离线数仓项目，我们的离线数仓有两个数据来源，分为日志行为数据和业务数据。</p><h2 id="日志数据采集"><a class="markdownIt-Anchor" href="#日志数据采集"></a> 日志数据采集</h2><p>我先来介绍一下日志数据的采集，我们通过采用前端埋点的方式，将数据通过 Nginx 负载均衡转发到日志服务器，并将数据进行落盘成日志文件。接下来，我们调研了一下对于文件数据的采集，最后选择了使用 flume 来对日志数据进行采集，因为 flume 本身就擅长对文件数据进行采集，而且它的效率性能也不错。flume 主要分为三个部分，source，channel，sink。</p><h3 id="1第一层-flume"><a class="markdownIt-Anchor" href="#1第一层-flume"></a> <strong>（1）第一层 flume，</strong></h3><p>对于 source，我们选择的是 taildir source，它支持断点续传和多目录，在采集到数据后，会将 offset 持久化到磁盘，下一次再进行读取的时间，就会先读取 offset，继续上一次读取。但是使用这个会存在一个问题，就是说当采集到数据但还没来得及将 offset 进行落盘，flume 就挂掉了，再一次重启就会继续从上一次的 offset 开始进行读取，读取的数据就会出现重复。我们也思考了到底要不要对这个重复数据进行处理，如果处理的话就需要使用到事务，但是这样会降低性能，所以我们还是没有对其进行处理，因为我们可以在下游进行数据清洗的时候对重复数据进行处理（比如：分组开窗取第一条）。</p><p>对于 channel，channel 主要分为 file channel ,memory channel，以及 kafka channel。file channel 的性能低，但是可靠性高，最多有 100 万个 Event。memory channel 可靠性低，但是性能高，最多有 100 个 Event。而 Kafka channel 的性能比 memory channel+kafka sink 的高，因为其省去了 sink 阶段。并且我的下游会使用到 Kafka，因为我们采用了 Kafka chanel。</p><p>在 source 和 channel 中间会有拦截器以及选择器，我们定义了 ETL 拦截器用来对 json 字符串不完整的数据进行过滤掉，并且我们这里只是轻度的聚合，而且可以防止这些数据从源头往下进行传递。在自定义拦截器的时候，通过继承 intercepetor 接口，然后重写初始化，单 Event，多 Event，关闭方法。对于选择器，flume 有两种类型的选择器，默认的是 Replacing（可以将数据发送到多个 channel），还有一个就是 Muteplexing（将数据发送到指定的 channel）。由于我们只需要将数据写入 Kafka 的一个 Topic，因此只需要使用默认的选择器就可以了。</p><p>并且我们采用了 gangalia 监控器来进行监控，有一次出现问题，发现提交的次数远大于成功的次数，然后就查询官网逛论坛，发现可以对 flume 进行优化，调大 flume 的内存大小，默认的大小是 2000m，我们将其调至 6 个 G（最大最小都设置为 6G，防止频繁 GC 导致内存抖动），之后一直能正常运行。后面搞活动的时候，由于数据洪峰，导致 flume 挂掉了。然后就立即进行了重启，但是问题不大，我们使用的是 tail dirsource，不会丢失数据只是有可能造成数据重复，并且我们的原始日志数据一般会保存 30 天，但是这个问题必须进行解决，我们就添加了服务器的台数，并将其配置到 Ningx 中进行转发，并同样使用 flume 进行数据采集，最终解决了问题。得到活动结束之后，对该服务器进行下线处理。</p><h3 id="2接下来我们使用的-kafka"><a class="markdownIt-Anchor" href="#2接下来我们使用的-kafka"></a> <strong>（2）接下来，我们使用的 Kafka，</strong></h3><p>kafka 是高吞吐低延迟的消息队列，并且能够起到削峰作用。在搭建 Kafka 的时候，我们使用老前辈给出的经验公式，搭建的 Kafka 台数为 2n+1，就是 2*（峰值生产速度 * 副本数/100）+1 台，我们使用 Kafka 自带的压测脚本测出峰值生产数据为 50M/s，我们使用的副本数为 2，因此算出 Kafka 的台数为 2_（50 _2/100）+1=3 台。还有就是对 Kafka 的分区进行设置，我们通过对于单分区的 tpoic 进行压测处理，测出峰值生产速度为 50M/s，而峰值消费速率为 20M/s，我们期望的速率为 100M/s，因此使用 100 /min(峰值生产速度，峰值消费速度)，最后算出分区数为（100/20）=5,最终我们设置的分区数为 5 个。在 Kafka 中，我们在需要考虑到数据的一致性问题，Kafka 的 ack 响应机制中，如果 ack=0，生产者发送完数据之后，就进行了响应。而 ack=1 的时候，当 leader 接收到之后才会进行响应，而 ack=-1 时，当 leader 和所有的 follower 接收到数据之后才会进行响应。我们选择的是 ack=-1，这样有可能会有少量数据的丢失，但是考虑到数据的一致性以及性能最终还是选择了使用 ack=-1。如果需要保证数据精准一次，我们就需要开启 Kafka 的幂等性+事务+ack=-1。</p><p>我们当时考虑了如果 Kafka 挂掉了怎么办？我们当时评估不会有太大问题，由于 Kafka 前面是 flume，如果 Kafka 挂掉，flume 就无法将数据灌入 Kafka 集群，那么 flume 就会停止工作，然后等待 Kafka 集群重启，因此不会有数据丢失的问题。</p><p>我们通过查询官网也对 Kafka 进行了一定的优化。对配置文件进行了修改，我们设置了 Kafka 的默认分区数为 2，并且修改了日志数据默认的保存事件为 3 天，默认为 7 天。并且我们增大网络通信时的最大延迟时间，由于有时候可能出现网络通信延迟，导致副本失败从而导致频繁的副本复制，严重影响集群性能。我们设置了 Kafka 的压缩格式，由于 Kafka 自身对压缩处理很好，因此只需要设置压缩格式就可以了，Kafka 会自动对数据进行压缩解压。由于前端埋点的问题，导致传输过来的数据过大，超过 Kafka 单次能够传输的最大数据量 1MB，我们后面才发现并调大了 Kafka 单次传输的最大数据量，并且与前端进行协调处理，确保了单次传输的数据不会超过 1MB。</p><p>我们使用了 Kafka eagle 来对 Kafka 进行监控</p><h3 id="3接下来就是第二层-flume"><a class="markdownIt-Anchor" href="#3接下来就是第二层-flume"></a> <strong>（3）接下来就是第二层 flume，</strong></h3><p>我们需要从 kakfa 中读取数据，本来是可以选择 Kafka chanel 的，但是我们考虑到对于当天 23：59：59 秒的数据，由于网络延迟等原因最终达到 hafs 的时候，已经是另一天了，那么这条数据就成了另一天的数据了，就会出现零点漂移的问题。我们就选择了 Kafka source，因此我们可以制定时间拦截器，通过将数据自身携带的时间作为写入 hdfs 时的文件名称。中间的话我们使用的是 Memory Channel，因此 Memory Channel 的速率快，并且最多就丢失 100 个 Event，而且由于我们的是日志数据，因此丢失一点也不要紧。最后 Sink 的时候，我们就选择了 hdfs Sink。由于我们这一层 flume 拉取数据的时候，拉取数据的速度太慢，而生产速度比较快，从而造成 Kafka 中数据堆积，我们设置 flume 每批次从 Kafka 拉取数据量的大小，通过设置 batch.size，并且这样也可以防止 flume 在写入 hdfs 时写入过快，从而导致 nameNode 宕机。在写入 hdfs 之后，我们发现出现了大量的小文件，为了解决小文件我们可以使用 har 进行归档，或者设置使用 combineTextInputformat，也可以设置开始 JVM 重用（使得 JVM 实例在同一个 job 中重新使用 N 次）。后面查阅 flume 官网，发现可以通过设置文件滚动的配置来尽量防止小文件的产生，可以设置文件的滚动时长为 1 个小时，设置文件的滚动大小为 128M，设置文件的滚出事务为 0（不生效），因为每个事务的大小不一。到这一步我们的用户日志数据采集就完成了。</p><h2 id="业务数据采集"><a class="markdownIt-Anchor" href="#业务数据采集"></a> 业务数据采集</h2><p>接下来就是业务数据的采集。对于业务数据的采集，使用 sqoop 工具将 mysql 数据导入到 Hadoop 生态圈。在导入数据的时候，我们需要考虑到 sqoop 的两个一致性问题，一个是存储一致性问题，一个是数据导出一致性问题。存储一致性问题就是 msyql 中的 null 值在导入 hive 后就变成了字符串类型的’null’，而 hive 中的空值是以\N 来进行存储的。因此我们将 sqoop 参数的时候需要设置 null-string 和 null-non-string 为’ \ \N’。另一个是导出一致性问题，这个问题是在将 Hadoop 生态圈的数据导出到 mysql 时，由于 sqoop 其实就是 4 个 MapTask 组成，因此运行时很可能会有 task 会挂掉，这样导出的数据就有可能出现一致性问题，那么我们可以设置参数 staging-table 和 clear-staging-table，在数据导出时先将数据导入到一张临时表，如果都成功了，就以事务的方式提交到正式表。我曾经挺朋友说过，他们在使用 sqoop 导入业务数据的时候，发生了数据倾斜的问题，经过讨论我们得出结论，由于我们的 mysql 中的表是设置有主键的，因此我们在导入数据的时候可以设置参数 split-by 指定主键自增来切分表，为了进一步提高效率，还可以设置一下 map 的个数。由于我们的每日的业务数据量不是特别大因此也不会出现这个问题。</p><h2 id="分层"><a class="markdownIt-Anchor" href="#分层"></a> 分层</h2><p>用户日志行为数据和业务数据已经都导入到 hdfs 上了。接下来就是进行分层处理，我们参考了美团离线数仓的分层，搭建 ods，dwd，dws，dwt 以及 ads 层。</p><p>我们的 ods 层没有对数据进行处理，起到对源数据进行备份的作用。但是为了节省空间，我们使用了 LZO 进行压缩，并且创建了分区表，防止后续出现全表扫描。</p><p>接下来就是 dwd 层，最重要的一层，我们首先对数据进行了清洗，比如：去重（前期出现的重复数据进行去重），解析（我们需要将日志数据解析成启动日志，页面日志，曝光日志，动作日志，错误日志），对于关键字段为空的数据（该删除的删除该变更的变更），对超期数据进行过滤，对手机号身份证号进行脱敏。前期由于数据的来源的问题，在清洗的时候脏数据的比例还是挺大的，最终经过与前端和 java 进行协调，将脏数据的比例调整到万分之一左右。同时在导入时设置了 LZO 压缩，以及建立分区表，并且设置了进行 parquet 列式存储。接下来就是进行维度建模，维度建模分为 4 个步骤。</p><p>第一步是选择业务，因为我们的业务表没有那么多，因此我们就选择对所有表进行处理分析。 第二部是声明粒度，粒度越小越灵活，因此我们只需要保证不对原始数据进行聚合操作，那就是最小粒度了。       第三步是确定维度，主要考虑怎样去描述一个业务事实，业务行为。最终我们确定了商品，用户，地区，活动，优惠券这几个维度。   第四步是确定事实，主要关注它的一个度量值，因此我们制作了一个总线矩阵，左边把事实表摆上，上面一个一个确定好维度，然后每个事实表都从各个维度看有没有关联，如果有关联，他们的度量值应该是什么？度量值就是像个数、件、数、金额这种可以量化的东西，可以统计。</p><p>这是我们整个建模的一个过程。</p><p>其实我们对部分维度数据进行了降维处理，将商品，spu，品类，一，二，三级分类表降维为商品维度表；将地区，省份表降维为地区维度宽表，将活动详情，活动规则降维成活动维度表。这个是基于星型模型，一张事实表围绕着多张一级维度表。</p><p>接下来就是 dws 层对每日数据进行统计，以及 dwt 层对每周或者每段时间进行统计。</p><p>在 ads 层，我们完成了近 20 个指标左右：</p><p>**设备主题：**活跃设备数（日、周、月）、每日新增设备、留存率、沉默用户数、本周回流用户数、流失用户数、               最近连续三周活跃用户数、最近七天内连续三天活跃用户数</p><p>**会员主题：**会员信息、漏斗分析（转化率）</p><p>**商品主题：**商品个数信息、商品销量排名、商品收藏排名、商品加入购物车排名、商品退款率排名、商品差评率</p><p>**营销主题：**下单数目统计、支付信息统计、品牌复购率</p><p>**地区主题：**地区主题信息</p><p>后面基于临时查询的需求，我们考虑到了即席查询，通过对比，我们发现最终选择了使用 Presto，因为它是基于内存的，因此查询速度快，并且安装起来也没有那么多坑。再往后最终我们将 ads 层数据通过 sqoop 导入到 mysql 中，当时本来想着着自己写一个接口，然后前端开发来做一个可视化的展示，但是由于人力不足，前端那边忙不过来，因此我们就选择了使用开源的可视化工具 superset 进行了可视化处理。</p><p><img src="https://i.loli.net/2021/05/13/1Vo6XEf8nLvgYBx.png#alt=image-20210513140451292" alt="" /></p><p>Kafka 高效读写：</p><p><strong>1）顺序写磁盘</strong></p><p><strong>2）零复制技术</strong></p><p><strong>3）分布式（分区概念并发读写）</strong></p><p><strong>4）Pagecache</strong></p><p>I/O Scheduler 会将连续的小块写组装成大块的物理写从而提高性能</p><p>I/O Scheduler 会尝试将一些写操作重新按顺序排好，从而减少磁盘头的移动时间</p><p>充分利用所有空闲内存（非 JVM 内存），如果使用应用层 Cache（即 JVM 堆内存），会增加 GC 负担</p><p>读操作可直接在 Page Cache 内进行，如果消费和生产速度相当，甚至不需要通过物理磁盘（直接通过 Page Cache）交换数据</p><p>如果进程重启，JVM 内的 Cache 会失效，但 Page Cache 仍然可用</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="posts/0/"/>
      <url>posts/0/</url>
      
        <content type="html"><![CDATA[<p>之后我们搭建了 flink 实时数仓项目，首先我们对业务数据的采集进行了优化，原本我们使用的是 maxwell 采集，但是由于 flink 自带有 flink -cdc 可以很好的对数据进行采集，而且这样也能减少使用第三方组件从而提高采集的性能。但是通过 api 使用 cdc 的方式，会有一些问题，它同步过来的 Json 数据，只有其中一部分是我们需要的数据，其他的是脏数据，CDC 底层使用的是 Debezium。我们可以通过自定义反序列化器，对数据进行解析，只保留我们需要的数据：数据本身，表名，库名，操作类型。之后我们将读取到的业务数据全部写入 Kafka 的一个 topic 中。不过由于我们的 Kafka 默认分区被设置为 5，因此为了确保数据的有序性，在将数据写入 Kafka 的时候，我们可以自定义序列化器，指定库名+表名作为 Kafka 的 key。我们将这作为用户数据的 ODS 层。</p><p>实时数仓中的日志数据采集依然是和离线数仓的一样。不过这一次我们是将第一层 flume 采集到 Kafka 的数据直接作为 ODS 层。</p><p>接下来就是 dwd 层，我们首先需要对日志数据进行处理，标识新老用户，只取一天内同一个用户第一个窗口的第一条数据。然后对数据进行拆分处理，拆分为启动日志，页面日志，曝光日志这三个流。之后对流进行匹配，匹配到就将数据输出到 Kafka 中。</p><p>然后就是对用户数据进行处理，这里的有点复杂，业务数据分为事实表和维度表，由于维度表需要经常进行查询，因此我们选择将维度表数据放入 Hbase 中。这里我们使用动态配置，使用 flink sql cdc 读取 mysql 中的配置，将读取到的数据流封装成广播状态作为配置流，然后读取 Kafka 中的用户数据作为数据流，将数据流和配置流进行 connect 处理。通过将来源表名和操作类型作为主键，然后从匹配广播状态中的配置信息，用来确定该数据流应该流向 Kafka 还是流向 Hbase。不过需要注意，我们在将数据输出到 Kafka 的时候，不需要指定 topic，Kafka 会自动帮我们创建 topic；而将数据输出到 hbase 的话，我们需要在插入数据之前通过读取到的配置文件创建对应的表，然后再进行数据的输出。</p><p>再之后就是 DWM 层，部分需求直接从 DWD 层到 DWS 层中间会有一定的计算量，而且这部分计算的结果很有可能被多个 DWS 层主题复用，所以部分 DWD 层会形成一层 DWM，存入 kafka 中。在这一层，我们主要是对 UV，跳出明细，订单宽表和支付宽表用于提高指标的复用性，减少重复加工。在拉宽表的时候，我们需要对两个事实表数据进行双流 join，我们使用的是 interval join，之后再进行维度的关联。既然要进行维度关联，我们就需要读取 hbase 中的数据，但是频繁的对 hbase 进行连接读取，hbase 会有很大的压力负担，因此我们可以使用 Redis 来作为旁路缓存，首先查询缓存，若无法读取到缓存，就查询从 hbase 中查询数据，以表名+数据 id 作为 key 缓存到 redis 中，并设置 redis 过期时间 24 小时。若查询到数据就直接返回数据。我们还需要考虑数据更新的问题，当我们的业务表中数据发生了变化，redis 是无法感知到的。在对业务数据进行处理，可以识别到数据的操作类型，如果识别到数据为更新操作，那么我们就可以删除掉该数据在 redis 中的缓存。默认情况下，Flink 的 MapFunction 中，单个并行度只能用同步方式去交互；IO 阻塞，等待请求返回，然后继续发送下一个请求。这种交互方式往往在网络等待上就耗费了大量的时间，因此我们可以使用异步 IO 的方式，使得单并行可以连续发送多个请求，不需要阻塞式等待。</p><p>再之后是 DWS 层，我们对数据进行了轻度的聚合，进行 10 秒开窗处理，减少维度数据查询的次数，减少 Hbase 的压力。主要有访客主题，商品主题，地区主题以及热词主题。对 dwd 明细数据、dwm 宽表数据进行合并、统一格式、开窗聚合（考虑到时效性就使用了轻度聚合，设置滚动窗口 10s），最后将数据写入 click house，并且设置选用 ReplecingMergeTree 引擎保证数据的幂等性。由于 click house 中的去重需要一定的时间，因此若在还未去重之前，查询的话就会查询到重复数据，因此我们在编写查询语句的时候就要添加上去重逻辑。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="posts/0/"/>
      <url>posts/0/</url>
      
        <content type="html"><![CDATA[<p>数据来源有两块：前端埋点产生的日志数据</p><p>使用到 Nginx 做到负载均衡</p><p>hadoop 是必问的</p><p>日志服务器 16G （flume 6G）</p><p>实时数仓的时候将日志数据采集到 Kafka 的时候还是使用 flume</p><p>使用 CDC 同步</p><p>cdc 的数据很杂乱，需要将 data 数据提取出来</p><p>自定义反序列化器</p><p><img src="https://i.loli.net/2021/05/09/zn3eyXipScx7GYk.png#alt=image-20210509161726100" alt="" /></p><p>dwd 层将数据写入 Hbase 的时候，如果 mysql 数据发生变化就将 Redis 中的缓存删除</p><p>如果轻度聚合，减少 Hbase 的压力</p><p>ads 层将数据存储到 mysql 或者 hbase 中，项目时省略了</p><p>一两个月将 sparkStreaming 替换为 Flink 数仓</p><p>原本打算使用 1.10 但是存在许多 bug</p><p>使用 CDC 同步 Mysql 的数据，但是有问题？</p><p>它同步过来的 Json 数据，只有其中一部分是我们需要的数据，其他的是脏数据</p><p>CDC 通过 API 的方式，CDC 底层使用的是 Debezium</p><p>我们可以通过自定义反序列化器，对数据进行解析，只保留我们想要的数据：数据本身，表名，库名，操作类型</p><p>interval join 底层是什么？？</p><p>底层是使用 connect 来实现的，每条流都会使用 Map 状态来存储数据，Key 是时间戳，Value 是数据，其中一条流的数据每来一条都会去遍历另一条流的 map 状态，如果匹配上就一起发送到 join 方法，如果超过了它的匹配范围就会被清除掉</p><p>为什么要设计 DWS 层？？？</p><p>为了减轻 Hbase 访问的压力，开窗聚合 10 秒钟</p><p>访客主题，商品主题，地区主题，关键字主题</p><p>clickhouse 去重有延迟，怎么办？</p><p>查询逻辑，加上去重逻辑，重复数据少量（由于之前明细层已经清洗过）</p><p>同步问题？？？</p><p>maxwell 支持断点续传</p><p>maxwell，cannal 往 Kafka 中写入数据时只会写入一个分区</p><p>但是我们现在 ODS 层是使用 Flink CDC 读取数据，并将所有的业务数据表写入一个 topic，如何将同一张表数据写入同一个分区，自定义序列化器通过指定 key 为：库名+表名 ，CDC 时通过自定义反序列化器，对数据进行解析，只保留我们想要的数据：数据本身，表名，库名，操作类型</p><h2 id="设备主题"><a class="markdownIt-Anchor" href="#设备主题"></a> 设备主题</h2><p>活跃设备数（日、周、月）</p><p>每日新增设备</p><p>留存率</p><p>沉默用户数</p><p>本周回流用户数</p><p>流失用户数</p><p>最近连续三周活跃用户数</p><p>最近七天内连续三天活跃用户数</p><h2 id="会员主题"><a class="markdownIt-Anchor" href="#会员主题"></a> 会员主题</h2><p>会员信息</p><p>漏斗分析（转化率）</p><h2 id="商品主题"><a class="markdownIt-Anchor" href="#商品主题"></a> 商品主题</h2><p>商品个数信息</p><p>商品销量排名</p><p>商品收藏排名</p><p>商品加入购物车排名</p><p>商品退款率排名</p><p>商品差评率</p><h2 id="营销主题"><a class="markdownIt-Anchor" href="#营销主题"></a> 营销主题</h2><p>下单数目统计</p><p>支付信息统计</p><p>品牌复购率</p><h2 id="地区主题"><a class="markdownIt-Anchor" href="#地区主题"></a> 地区主题</h2><p>地区主题信息</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="posts/0/"/>
      <url>posts/0/</url>
      
        <content type="html"><![CDATA[<h2 id="一-linux"><a class="markdownIt-Anchor" href="#一-linux"></a> 一、Linux</h2><h3 id="1-常用命令"><a class="markdownIt-Anchor" href="#1-常用命令"></a> 1、常用命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">查看端口： netstat 、 ss</span><br><span class="line">查看进程： ps -ef</span><br><span class="line">查看磁盘空间： df -h</span><br><span class="line">查看负载： top 、 iotop</span><br><span class="line">查看内存： free -h</span><br><span class="line">查找： find</span><br><span class="line">查看磁盘io情况： iostat</span><br><span class="line"></span><br><span class="line">| 管道符</span><br></pre></td></tr></table></figure><h3 id="2-高级-shell-工具"><a class="markdownIt-Anchor" href="#2-高级-shell-工具"></a> 2、高级 shell 工具</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">awk：文本分析、</span><br><span class="line">sed：文本内部做全局替换、</span><br><span class="line">cut：切分、</span><br><span class="line">sort：排序</span><br></pre></td></tr></table></figure><h3 id="3-不知道进程号怎么-kill"><a class="markdownIt-Anchor" href="#3-不知道进程号怎么-kill"></a> 3、不知道进程号，怎么 kill？</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps -ef | grep xxx|grep -v grep | awk <span class="string">&#x27;&#123;print \$2&#125;&#x27;</span> | xargs <span class="built_in">kill</span></span><br></pre></td></tr></table></figure><h3 id="4-写过哪些脚本"><a class="markdownIt-Anchor" href="#4-写过哪些脚本"></a> 4、写过哪些脚本？</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">1）集群启停脚本</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)&#123;</span><br><span class="line">for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">ssh $i &quot;启动命令&quot;（绝对路径）</span><br><span class="line">done</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;stop&quot;)&#123;</span><br><span class="line">for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">ssh $i &quot;停止命令&quot;（绝对路径）</span><br><span class="line">done</span><br><span class="line">&#125;;;</span><br><span class="line">esac</span><br><span class="line">2）数仓分层脚本</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">时间变量 = 今天 - 1</span><br><span class="line">定义库名APP</span><br><span class="line">HIVE路径</span><br><span class="line"></span><br><span class="line">sql=‘先写出一天的，  把时间替换成 变量 ， 表名前面加库名 ， 自定义函数前面也要加库名’</span><br><span class="line"></span><br><span class="line">lzo索引</span><br><span class="line"></span><br><span class="line">执行 hive -e $&#123;sql&#125;</span><br><span class="line"></span><br><span class="line">3）sqoop脚本（业务数据）</span><br><span class="line">sqoop</span><br><span class="line">--import</span><br><span class="line"></span><br><span class="line">--connect 数据库url</span><br><span class="line">--username 用户名</span><br><span class="line">--passwd 密码</span><br><span class="line"></span><br><span class="line">--目标路径</span><br><span class="line">--删除目标路径</span><br><span class="line"></span><br><span class="line">--query “select id，name，age from test where 创建时间=昨天 or 操作时间 = 昨天” and $CONDITIONS   ====》 00点之后才执行的</span><br><span class="line"></span><br><span class="line">--空值处理</span><br></pre></td></tr></table></figure><h3 id="5-单引号-双引号的区别"><a class="markdownIt-Anchor" href="#5-单引号-双引号的区别"></a> 5、单引号、双引号的区别？</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">单引号 =》 取字符串</span><br><span class="line">双引号 =》 取变量值</span><br><span class="line">嵌套 =》 看最外层</span><br><span class="line">‘$do_date’ = $do_date</span><br><span class="line">&quot;$do_date&quot; = 2021-xx-xx</span><br><span class="line">&#x27;&quot;&quot;&#x27;  = &quot;$do_date&quot;</span><br><span class="line">&quot;&#x27;&#x27;&quot;  = &#x27;2021-xx-xx&#x27;</span><br></pre></td></tr></table></figure><h2 id="二-hadoop"><a class="markdownIt-Anchor" href="#二-hadoop"></a> 二、Hadoop</h2><h3 id="1-入门"><a class="markdownIt-Anchor" href="#1-入门"></a> 1、入门</h3><p>​ 1）常用端口号</p><p>​</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> NameNode Http端口Yarn端口历史服务器FS客户端</span><br><span class="line">2.x(2.7.2)    50070   8088198888020</span><br><span class="line">3.x   9870   8088198888020、9820</span><br></pre></td></tr></table></figure><p>​ 2）核心配置文件   ${HADOOP_HOME}/etc/hadoop/</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2.x core-site.xmlhdfs-site.xml mapred-site.xml yarn-site.xmlslaves</span><br><span class="line">3.x core-site.xmlhdfs-site.xml mapred-site.xml yarn-site.xmlworkers</span><br></pre></td></tr></table></figure><h3 id="2-hdfs"><a class="markdownIt-Anchor" href="#2-hdfs"></a> 2、HDFS</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">1） 读写流程、原理：  笔试题， 有没有朋友</span><br><span class="line">2） 副本数 ： 默认3个</span><br><span class="line">3） 块大小：</span><br><span class="line">1.x ： 64m</span><br><span class="line">2.x&#x2F;3.x: 128m</span><br><span class="line">本地： 32m</span><br><span class="line">企业： 128m、256m（一般说128m就可以了）</span><br><span class="line">HIVE： 256m</span><br><span class="line">4） 小文件</span><br><span class="line">危害：</span><br><span class="line">占用NameNode内存（任意大小的小文件都会占用150字节的内存）</span><br><span class="line">影响maptask数量（maptask的大小默认为1个G）</span><br><span class="line">解决：</span><br><span class="line">存储 &#x3D;》 har归档（将小文件打包成一个整体）： 解决历史的小文件</span><br><span class="line">计算 &#x3D;》 切片 &#x3D;》 CombineTextInputFormat ： 把多个小文件当成一个切片</span><br><span class="line">资源 &#x3D;》 JVM重用</span><br><span class="line">5） 纠删码：  CPU换存储空间</span><br><span class="line">原本1个文件300M,那么3个副本就得需要900M。使用纠删码的话就是将一个300m文件切分3个部分每个100M和2个部分的纠删码每个100M加起来只需要500M，但是需要消耗CPU资源。</span><br><span class="line">6） 异构存储（冷热分离）</span><br><span class="line">7） 增加节点、退役节点</span><br><span class="line">增加：安装部署，直接启动Datanode，  如果开启了白名单，要把新节点加入白名单</span><br><span class="line">退役：加入黑名单，   退役中会自动复制数据到其他节点， 退役完成即可</span><br><span class="line">8） Namenode 并发（Namenode线程池的线程数）</span><br><span class="line">经验公式：dfs.namenode.handler.count&#x3D;20×ln(Cluster Size)</span><br><span class="line">9） Namenode 内存</span><br><span class="line">2.x   2000m</span><br><span class="line">3.x   动态分配</span><br></pre></td></tr></table></figure><p>存储类型和存储策略</p><p><img src="https://i.loli.net/2021/05/06/PYSJ1byrHz5FCtB.png#alt=image-20210506184429397" alt="" /></p><h3 id="3-mr"><a class="markdownIt-Anchor" href="#3-mr"></a> 3、MR</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Shuffle及其优化</span><br><span class="line">map方法之后，   reduce方法之前</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/05/06/ZC2q3fsIHpj6kSm.png#alt=image-20210506185522179" alt="" /></p><p><img src="https://i.loli.net/2021/05/06/9mhrl3K4qXaDAHo.png#alt=image-20210506185603301" alt="" /></p><h3 id="4-yarn"><a class="markdownIt-Anchor" href="#4-yarn"></a> 4、YARN</h3><p>​ 1）工作机制</p><p><img src="https://i.loli.net/2021/05/06/8Vi7CfujsHYp9eQ.png#alt=1" alt="" /></p><p><img src="https://i.loli.net/2021/05/06/T8hDzy357ZKjQdR.png#alt=image-20210506193139651" alt="" /></p><p>2）调度器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">apache，默认 容量；   CDH &#x3D;》 公平</span><br><span class="line">FIFO：单队列，先进先出，先提交的先执行</span><br><span class="line">容量：多队列，每个队列是FIFO，队列之间可以借用资源（弹性队列）</span><br><span class="line">默认只有一个default队列，自己配置多队列 &#x3D;》 hadoop_home&#x2F;etc&#x2F;hadoop&#x2F;CapacityScheduler.xml</span><br><span class="line">公平：多队列，每个任务公平享有资源（缺额、权重），并发最高</span><br><span class="line"></span><br><span class="line">企业怎么选？</span><br><span class="line">一般企业选容量，    有钱的大企业选公平</span><br><span class="line"></span><br><span class="line">为什么要多队列？</span><br><span class="line">避免某个菜鸟，递归死循环，把资源干挂 &#x3D;-&#x3D;》 解耦</span><br><span class="line"></span><br><span class="line">按照什么原则划分？</span><br><span class="line">按框架分： hive、spark、flink</span><br><span class="line">按业务分： 登陆、注册、下单、支付、物流</span><br><span class="line">双11：</span><br><span class="line">登陆</span><br><span class="line">注册</span><br><span class="line">下单</span><br><span class="line">物流</span><br></pre></td></tr></table></figure><h2 id="三-flume"><a class="markdownIt-Anchor" href="#三-flume"></a> 三、Flume</h2><h3 id="1-组成-原理"><a class="markdownIt-Anchor" href="#1-组成-原理"></a> 1、组成、原理</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">1）taildirsource：</span><br><span class="line">好处：支持断点续传、多目录</span><br><span class="line">什么时候产生？  apache &#x3D;》 1.7</span><br><span class="line">断点续传原理：保存了offset，  如果挂了，可能产生少量重复</span><br><span class="line">重复怎么办？</span><br><span class="line">处理：自定义，加事务，   没必要</span><br><span class="line">不处理：数据采集重点考虑传输效率，重复的数据，下游再进行处理 &#x3D;》 hive（groupby，开窗取第一条）、spark、flink</span><br><span class="line">支不支持递归子文件夹？</span><br><span class="line">不支持 ，怎么办？ &#x3D;》 非要办，自定义</span><br><span class="line">不办 &#x3D;》 已经规范了</span><br><span class="line">2）channel：</span><br><span class="line">memory channel： 基于内存， 效率高，可靠性低， 100个Event</span><br><span class="line">file channel：  基于磁盘， 效率低，可靠性高， 100万个Event</span><br><span class="line">kafka channel：  基于kafka的磁盘，效率高，可靠性高</span><br><span class="line">效率为什么高？   省去了 sink，   kafka channel &gt; memory channel + kafka sink</span><br><span class="line">哪个版本产生？ apache 1.6</span><br><span class="line">为什么没火？ 有bug，会携带一个 header信息 , parseAsFlumeEvent(boolean)不起作用</span><br><span class="line">&quot;asdfasdf&quot;:&#123;aaa:&quot;b&quot;&#125;</span><br><span class="line">哪个版本解决？</span><br><span class="line">1.7解决</span><br><span class="line">怎么选？</span><br><span class="line">如果上游或下游是kafka，优先kafka channel</span><br><span class="line">如果不是kafka， 一般日志数据， memory channel，主要考虑效率，丢的话，最多丢100个Event</span><br><span class="line">金融公司、跟钱相关， file channel，主要考虑可靠性</span><br><span class="line"></span><br><span class="line">3）hdfs sink：</span><br><span class="line">小文件：</span><br><span class="line">滚动时间：1-2小时（一般设置为1个小时）</span><br><span class="line">滚动大小：128M</span><br><span class="line">Event数量：0，不开启，每个Event大小不一</span><br><span class="line">4）事务</span><br><span class="line">source -》 channel    put事务</span><br><span class="line">channel &#x3D;》 sink      take事务</span><br></pre></td></tr></table></figure><h3 id="2-三个器"><a class="markdownIt-Anchor" href="#2-三个器"></a> 2、三个器</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">1）拦截器</span><br><span class="line">ETL拦截器：过滤json格式不完整的数据，轻度的过滤</span><br><span class="line">不要行不行？ &#x3D;》 可以</span><br><span class="line">为什么还要做呢？ &#x3D;》  1.只是轻度的校验过滤    2.防止这种脏数据从源头一直往下游传递</span><br><span class="line"></span><br><span class="line">时间拦截器：  提取数据里的时间，避免  零点漂移的 问题</span><br><span class="line">所以第二层flume不使用 kafka channel， 因为要使用拦截器</span><br><span class="line"></span><br><span class="line">步骤：</span><br><span class="line">实现Interceptor接口，重写4个方法：</span><br><span class="line">初始化</span><br><span class="line">单Event处理</span><br><span class="line">多Event处理</span><br><span class="line">关闭</span><br><span class="line">定义一个静态内部类 Builder</span><br><span class="line"></span><br><span class="line">打包 &#x3D;》 上传到 flume的lib目录下 &#x3D;》 配置文件进行关联，  全类名+$Builder</span><br><span class="line">2）Channel选择器</span><br><span class="line">replicating（默认）： 把Event发往所有的Channel 。 项目里使用的，日志数据采集到kafka只有一个Topic</span><br><span class="line">multiplexing： 选择性的发往指定的Channel，</span><br><span class="line"></span><br><span class="line">3）监控器</span><br><span class="line">ganglia   监控事务的情况</span><br><span class="line">尝试提交的次数    成功提交的次数</span><br><span class="line">如果差值较大，说明有问题</span><br></pre></td></tr></table></figure><h3 id="3-优化"><a class="markdownIt-Anchor" href="#3-优化"></a> 3、优化</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1）调整内存： 默认 2000m   flume-env.sh &#x3D;&#x3D;&#x3D;&gt; 调整为 6G</span><br><span class="line">2）channel &#x3D;》 多目录（多磁盘）</span><br><span class="line">3）hdfs sink 小文件：</span><br><span class="line">滚动时间：1-2小时</span><br><span class="line">滚动大小：128M</span><br><span class="line">Event数量：0，不开启，每个Event大小不一</span><br></pre></td></tr></table></figure><h3 id="4-挂了"><a class="markdownIt-Anchor" href="#4-挂了"></a> 4、挂了</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1）没事，不会丢数据，taildirsouce 可能产生少量重复，可以接受</span><br><span class="line">2）如果是memory channel，最多丢 100个Event，</span><br><span class="line">   如果是 file channel，持久化到磁盘，不丢</span><br><span class="line">3）原始日志数据设置30天保存</span><br></pre></td></tr></table></figure><h2 id="五-kafka23-件事"><a class="markdownIt-Anchor" href="#五-kafka23-件事"></a> 五、Kafka（23 件事）</h2><p>（Pulsar 新出来的消息中间件）</p><h3 id="1-基本信息"><a class="markdownIt-Anchor" href="#1-基本信息"></a> 1、基本信息</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">1）基本架构：Broker，Producer，Consumer</span><br><span class="line">2）集群规模：  经验公式   2*（生产者峰值速率 * 副本数&#x2F;100） + 1 &#x3D; 台数</span><br><span class="line">3）压测：自带的脚本    生产者峰值速率 、  消费者峰值速率</span><br><span class="line">4）几个分区：3-10个都行</span><br><span class="line">预期的吞吐率 T &#x3D;  100M&#x2F;s</span><br><span class="line">生产者单分区峰值速率 Tp、  消费者单分区峰值速率Tc  &#x3D;》 创建一个单分区的Topic，进行压测</span><br><span class="line">估算的分区数 &#x3D; T &#x2F; min(Tp,Tc)</span><br><span class="line">5）保存时间：默认7天 &#x3D;》 3天</span><br><span class="line">6）副本数：默认1个 &#x3D;》 2个， 兼顾 可靠性和性能</span><br><span class="line">7）ISR</span><br><span class="line">解决leader挂了，哪个follower当leader的问题</span><br><span class="line">条件： 老版本 延迟时间、条数</span><br><span class="line">   新版本  延迟时间</span><br><span class="line">8）消费分区分配策略</span><br><span class="line">Range（默认）： 可能产生数据倾斜 （对于一个topic存在1条，那么100个topic，那么就会有100条数据都倾斜到消费者c0了）</span><br><span class="line">某Topic 10个分区， 10&#x2F;3 &#x3D; 3 余 1</span><br><span class="line">C001 2 3</span><br><span class="line">C1  4   56</span><br><span class="line">C2 7 8 9</span><br><span class="line"></span><br><span class="line">RoundRobin：取hash，轮询    前提：每个消费者订阅的Topic一样</span><br><span class="line"></span><br><span class="line">sticky</span><br><span class="line">9）几个topic</span><br><span class="line">看下游的需要：</span><br><span class="line">日志1个Topic，  业务数据，1张表1个Topic</span><br><span class="line">10）数据量计算</span><br><span class="line">100万日活， 每人每天产生100条 &#x3D;》 100万 * 100条 &#x3D;  1亿条</span><br><span class="line">数据每条0.5k~2k，取平均值1k</span><br><span class="line"></span><br><span class="line">平均速率： 1亿条&#x2F;（24小时 * 60分钟 * 60秒） &#x3D; 约 1150条&#x2F;s （不要说这么具体）</span><br><span class="line">       * 1k &#x3D;  约 1M&#x2F;s</span><br><span class="line">峰值速率： 平均的2-20倍，     2300条&#x2F;s ~ 23000条&#x2F;s  ,  出现在 晚上下班后， 7~12点</span><br><span class="line">    2M&#x2F;s ~ 20M&#x2F;s</span><br><span class="line">低谷速率： 除以平均的2-20倍，     50条&#x2F;s ~ 500条&#x2F;s  ,  凌晨、半夜， 3~5点</span><br><span class="line">50K&#x2F;s ~ 500K&#x2F;s</span><br><span class="line"></span><br><span class="line">峰值同时几个人在线： 1万~2万人</span><br><span class="line"></span><br><span class="line">11）磁盘大小</span><br><span class="line">100万日活 * 每人每天100条 * 每条平均1k &#x3D; 约 100G</span><br><span class="line"></span><br><span class="line">100G * 2副本 * 保存3天 &#x2F; 0.7 &#x3D; 约860G &#x3D;》 给1个T</span><br><span class="line"></span><br><span class="line">12）监控器</span><br><span class="line">kafka eagle 、 kafka manager</span><br><span class="line">我们是自己开发的？好厉害，好想加入你们，向你们学习</span><br></pre></td></tr></table></figure><h3 id="2-积压了"><a class="markdownIt-Anchor" href="#2-积压了"></a> 2、积压了</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1）增加分区数，增加消费线程 &#x3D;》 达到 1：1</span><br><span class="line">2）flume拉取Kafka时，调整每批次的大小  batchsize</span><br><span class="line">3）优化下游处理能力（spark、flink）</span><br></pre></td></tr></table></figure><h3 id="3-丢数"><a class="markdownIt-Anchor" href="#3-丢数"></a> 3、丢数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">生产者：ack</span><br><span class="line">0 发送完，不需要应答效率高，可靠性低</span><br><span class="line">1 发送完，leader应答效率中，可靠性中</span><br><span class="line">-1  发送完，leader和所有ISR里的follower应答效率低，可靠性高（极端情况：ISR里的follower个数为0，这个时候  leader挂了，还是会丢数）---&gt;解决办法：参数设置ISR里follower的最小个数</span><br><span class="line"></span><br><span class="line">企业里面怎么选？</span><br><span class="line">一般日志，1就行，兼顾 效率和可靠性</span><br><span class="line">如果金融、跟钱相关， 主要考虑可靠性，选 -1</span><br><span class="line"></span><br><span class="line">消费者：</span><br><span class="line">Spark： 手动维护offset</span><br><span class="line">Flink： 状态、checkpoint</span><br><span class="line">flume： put事务</span><br></pre></td></tr></table></figure><h3 id="4-重复"><a class="markdownIt-Anchor" href="#4-重复"></a> 4、重复</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">幂等 + 事务 +  ack（-1）</span><br><span class="line"></span><br><span class="line">企业怎么做？</span><br><span class="line">一般日志，重复一些无所谓， 下游要去重  &#x3D;》  hive 、spark 、flink</span><br><span class="line">如果金融、跟钱相关，就三者都用</span><br></pre></td></tr></table></figure><h3 id="5-优化"><a class="markdownIt-Anchor" href="#5-优化"></a> 5、优化</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1） 调内存 &#x3D;》 默认1G ，调整6G（若太大，会使用到一个PageCache页缓存，站到28~32G左右内存）</span><br><span class="line">2） 保存时间 7 &#x3D;》 3天</span><br><span class="line">3） 副本数 1 &#x3D;》 2</span><br><span class="line">4） 压缩（Kafka自身会进行压缩）</span><br><span class="line">5） 单条消息大小： 默认最大1M</span><br><span class="line">6） 通信时间：</span><br></pre></td></tr></table></figure><h3 id="6-挂了"><a class="markdownIt-Anchor" href="#6-挂了"></a> 6、挂了</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1）快速重启&#x3D;》 观察 &#x3D;》 排查之前的错误日志 &#x3D;》 定位问题 &#x3D;》 调整</span><br><span class="line">2）会不会丢数？ 不会， 数据持久化到磁盘，另外原始数据有备份 ；</span><br></pre></td></tr></table></figure><h3 id="7-kafka-消费有序性"><a class="markdownIt-Anchor" href="#7-kafka-消费有序性"></a> 7、kafka 消费有序性：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">单分区内有序，多分区间无序</span><br><span class="line">为什么讲究有序： 业务数据的操作</span><br><span class="line">同一张表的操作，需要有序，否则出问题</span><br><span class="line"></span><br><span class="line">canal,maxwell默认只往topic的一个分区发送数据</span><br><span class="line"></span><br><span class="line">解决方案：</span><br><span class="line">1） 一张表一个Topic： 使用单分区</span><br><span class="line">2） 所有表的数据在同一个Topic &#x3D;&#x3D;&#x3D;&#x3D;》 指定key： 库名+表名， 同一张表的数据，进入到同一个分区</span><br><span class="line">3） 某些场景，flink的watermark可以搞定（可以用于读取多分区的topic）</span><br></pre></td></tr></table></figure><h3 id="8-高效读写"><a class="markdownIt-Anchor" href="#8-高效读写"></a> 8、高效读写</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1）集群，分区</span><br><span class="line">2）顺序写</span><br><span class="line">3）零拷贝</span><br></pre></td></tr></table></figure><h2 id="六-zookeeper"><a class="markdownIt-Anchor" href="#六-zookeeper"></a> 六、Zookeeper</h2><h3 id="1-选举机制"><a class="markdownIt-Anchor" href="#1-选举机制"></a> 1、选举机制</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">半数机制： 2n+1 台</span><br><span class="line">某个节点挂了，会不会出现脑裂？ &#x3D;》 我们是奇数台，避免脑裂问题</span><br><span class="line">Paxos算法</span><br></pre></td></tr></table></figure><h3 id="2-集群规模"><a class="markdownIt-Anchor" href="#2-集群规模"></a> 2、集群规模</h3><p>​ 10 台服务器 3 台 zk</p><p>​ 20 台服务器 5 台 zk</p><p>​ 50 台服务器 7 台 zk</p><p>​ 100 台服务器 11 台 zk</p><p>​ 200 台服务器 11 台 zk</p><p>​</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">节点不是越多越好，越多&#x3D;》可靠性更高 &#x3D;》 性能差，同步副本需要通信</span><br></pre></td></tr></table></figure><h3 id="3-常用命令"><a class="markdownIt-Anchor" href="#3-常用命令"></a> 3、常用命令</h3><p>​ ls 、 delete （all） 、 get 、 create</p><h2 id="七-hive"><a class="markdownIt-Anchor" href="#七-hive"></a> 七、Hive</h2><h3 id="1-基本架构"><a class="markdownIt-Anchor" href="#1-基本架构"></a> 1、基本架构</h3><p><img src="https://i.loli.net/2021/05/06/QX54u6K29fHiEcP.png#alt=2" alt="" /></p><h3 id="2-与数据库的区别"><a class="markdownIt-Anchor" href="#2-与数据库的区别"></a> 2、与数据库的区别</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">除了sql有点类似，其他都不一样</span><br><span class="line">hivemysql</span><br><span class="line">速度数据量大，查询快数据量小，快</span><br><span class="line">适用场景大数据量的查询分析小数据量的增删改查</span><br></pre></td></tr></table></figure><h3 id="3-内部表-外部表"><a class="markdownIt-Anchor" href="#3-内部表-外部表"></a> 3、内部表、外部表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">元数据、原始数据</span><br><span class="line"></span><br><span class="line">删内部表：元数据、原始数据 都删除</span><br><span class="line">删外部表：只删 元数据</span><br><span class="line"></span><br><span class="line">企业里面怎么选？</span><br><span class="line">主要都是外部表</span><br><span class="line">自己建的临时表、测试表用内部表</span><br></pre></td></tr></table></figure><h3 id="4-4-个-by"><a class="markdownIt-Anchor" href="#4-4-个-by"></a> 4、4 个 by</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">order by： 全局排序， 所有数据进入一个reduce（生产中避免使用）</span><br><span class="line">sort by：  分区内排序</span><br><span class="line">distribute by ： 相当于分区，指定如何划分reduce</span><br><span class="line">cluster by： sort by 与 distribute by字段相同时，可以替换， 只能用于 升序</span><br></pre></td></tr></table></figure><h3 id="5-系统函数"><a class="markdownIt-Anchor" href="#5-系统函数"></a> 5、系统函数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1）时间： date_add\date_sub\date_diff  \ next_day\ last_day \ date_format \ from_unixtimep \ unix_timstamp</span><br><span class="line">2）字符串:  substring\ split \  concat \ concat_ws \ get_json_object()</span><br><span class="line">3）行列转换:</span><br><span class="line">一行变多行： explode split   laterval view</span><br><span class="line">多行变一行： collect_list    &#x2F; collect_set</span><br><span class="line">一列转多列： case when</span><br></pre></td></tr></table></figure><h3 id="6-开窗函数"><a class="markdownIt-Anchor" href="#6-开窗函数"></a> 6、开窗函数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">聚合函数、排名函数、范围函数 over(partition by    order by   desc )</span><br></pre></td></tr></table></figure><h3 id="7-自定义函数"><a class="markdownIt-Anchor" href="#7-自定义函数"></a> 7、自定义函数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">UDF： 一进一出</span><br><span class="line">UDTF：一进多出</span><br><span class="line">UDAF：多进一出</span><br><span class="line"></span><br><span class="line">为什么用UDF、UDTF？</span><br><span class="line">方便调试查看， 系统函数满足不了， 需要调用第三方jar包</span><br><span class="line"></span><br><span class="line">步骤：</span><br><span class="line">UDF：继承UDF， 重写一个方法： evaluate（）</span><br><span class="line">UDTF： 继承GenericUDTF，重写三个方法： 初始化（返回值的校验）、process、关闭</span><br><span class="line"></span><br><span class="line">打包 &#x3D;》 上传到HDFS &#x3D;》 hive客户端进行注册</span><br></pre></td></tr></table></figure><h3 id="8-数据倾斜"><a class="markdownIt-Anchor" href="#8-数据倾斜"></a> 8、数据倾斜</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">1）现象： ui页面， 某些reduce卡在99%，并且执行时间明显特别长</span><br><span class="line">2）原因：</span><br><span class="line">语法的角度： join 、 group by</span><br><span class="line">数据的角度： 数据本身不均匀、 join时类型不一致（注意：hive中会自动进行类型转换，可能出现string类型转化之后的BigInt的值是一样的。注意：超过范围的数值在转换为bigint时，都会变成相同的结果。如果对这些数据求hash值，得出的结果也都是一样的。）、 null值</span><br><span class="line">3）解决：</span><br><span class="line">本身不均匀： 二次聚合 设置参数skewindata &#x3D; true</span><br><span class="line">count</span><br><span class="line">(</span><br><span class="line">count()</span><br><span class="line">group by concat_ws(key + rand())</span><br><span class="line">)</span><br><span class="line">group by substring\split</span><br><span class="line">类型不一致： 用cast先转换类型</span><br><span class="line">A join B on A.id &#x3D; cast(B.id as xxxxx)</span><br><span class="line">null值：</span><br><span class="line">不算脏数据，允许有null值 &#x3D;》 类似数据不均匀，   加随机值打散，二次聚合</span><br><span class="line">不需要null值 &#x3D;&gt;  where 过滤掉 、 is not null</span><br></pre></td></tr></table></figure><h3 id="8-优化"><a class="markdownIt-Anchor" href="#8-优化"></a> 8、优化</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">1）mapjoin ： 默认打开， 大小表join，小表加载到内存，在map阶段就完成关联</span><br><span class="line">老版本必须在sql中体现： select xxx ,xxx &#x2F;* mapjoin *&#x2F; from xxx</span><br><span class="line">2）行列过滤： 避免 select * ；    先 join 再 where  &#x3D;&#x3D;&#x3D;&#x3D;》 先 where 再 join</span><br><span class="line">3）分区、分桶（抽样、不清楚key、、、、SMB join--&gt;大表与大表join的时候）</span><br><span class="line">4）压缩 snappy 、lzo</span><br><span class="line">5）列式存储 （OLAP）</span><br><span class="line">idnameage</span><br><span class="line">1 zhangsan18</span><br><span class="line">2 lisi19</span><br><span class="line"></span><br><span class="line">列式： 12 zhangsan lisi 18 19</span><br><span class="line">sql:select 字段名 ，查询效率高</span><br><span class="line">缺点：不适合 更新、插入、删除操作</span><br><span class="line"></span><br><span class="line">行式：1 zhangsan182 lisi19</span><br><span class="line">缺点：不适合 查询</span><br><span class="line">6）小文件</span><br><span class="line">原因：动态分区、reduce个数太多、数据本身</span><br><span class="line">解决：</span><br><span class="line">JVM重用： 10-20</span><br><span class="line">切片的角度： CombineHiveInputFormat</span><br><span class="line">merge： 在启动一个任务，自动将小于16m的小文件，合并成一个256m的大文件</span><br><span class="line">map-only，默认打开的</span><br><span class="line">map-reduce，需要参数设置打开</span><br><span class="line"></span><br><span class="line">7）提前Combiner：前提不影响最终业务逻辑</span><br><span class="line">8）合理设置map、reduce数量</span><br><span class="line">map数量： splitSize&#x3D; max（minSize，min（maxSize，blockSize））</span><br><span class="line">reduce数量： hive默认开启的估算机制 &#x3D;&#x3D;&#x3D;&#x3D;》 reduce个数 &#x3D;  数据量 &#x2F; 每个reducer大小（默认1G）</span><br><span class="line">  参数指定： mapred.reduce.tasks ，默认-1，不开启。如果指定了该参数，那么就不会再走估算机制</span><br><span class="line"></span><br><span class="line">9）替换引擎</span><br><span class="line">MR：默认的引擎。 数据量大的任务，比如：年、月、周</span><br><span class="line">Tez：使用上可能会有某些问题， 基于内存，  用来 测试、临时指标查询（减少reduce的落盘）--&gt;不支持子文件夹递归</span><br><span class="line">Spark： 基于内存，快， 用来 日常指标，天</span><br><span class="line"></span><br><span class="line">set xxxx &#x3D; tez;</span><br></pre></td></tr></table></figure><h3 id="9-分隔符"><a class="markdownIt-Anchor" href="#9-分隔符"></a> 9、分隔符</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive默认列分隔符： \001   \u0001   表现：看起来乱码，显示 ^A</span><br></pre></td></tr></table></figure><h3 id="10-union-union-all"><a class="markdownIt-Anchor" href="#10-union-union-all"></a> 10、union  union all</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">union 去重</span><br><span class="line">union all 不去重</span><br></pre></td></tr></table></figure><h3 id="11-注释中文乱码"><a class="markdownIt-Anchor" href="#11-注释中文乱码"></a> 11、注释中文乱码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">元数据在mysql的库，默认是 latin1， 如果表的注释是中文，查询会显示 ？？？？？</span><br><span class="line">对应的库表编码改成 utf8   连接的时候  jdbc:mysqlxxxxxxxxxxxxxx&#x2F;库名&amp;charactorEncode&#x3D;UTF8</span><br></pre></td></tr></table></figure><h2 id="八-sqoop"><a class="markdownIt-Anchor" href="#八-sqoop"></a> 八、Sqoop</h2><h3 id="1-遇到什么问题"><a class="markdownIt-Anchor" href="#1-遇到什么问题"></a> 1、遇到什么问题？</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">1）空值问题</span><br><span class="line">mysql Hive</span><br><span class="line">null \N</span><br><span class="line"></span><br><span class="line">导入2个： --null-string ‘\\N’ --null-non-string</span><br><span class="line">导出2个： --input-null-string  --input-null-non-string</span><br><span class="line">2）一致性问题</span><br><span class="line">导出的时候</span><br><span class="line">底层是4个map，</span><br><span class="line">1000万 &#x3D;》 1亿 ， 跑路</span><br><span class="line">1亿 &#x3D;》 10万，跳楼</span><br><span class="line"></span><br><span class="line">讲究结果的准确性，可以慢，不能错</span><br><span class="line"></span><br><span class="line">解决： 临时表的表示</span><br><span class="line">--staging-table   --clear-staging-table</span><br></pre></td></tr></table></figure><h3 id="2-数据倾斜"><a class="markdownIt-Anchor" href="#2-数据倾斜"></a> 2、数据倾斜</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">100万日活， （综合电商）每天10万订单， 每个订单对应10条数据，</span><br><span class="line">10万* 10 &#x3D; 100万条，平均每条1k</span><br><span class="line">100万 * 1k &#x3D; 1G</span><br><span class="line"></span><br><span class="line">我们的数据量，没什么问题。</span><br><span class="line"></span><br><span class="line">我的思路：  split-by  主键（这个字段最好不要使字符串类型），如果还慢 &#x3D;》 增加map个数 --mappers 100</span><br></pre></td></tr></table></figure><h3 id="3-怎么执行每天什么时候执行执行多久"><a class="markdownIt-Anchor" href="#3-怎么执行每天什么时候执行执行多久"></a> 3、怎么执行？每天什么时候执行？执行多久？</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">写在脚本里，每天azkaban定时调度。</span><br><span class="line">00：10-00：30</span><br><span class="line">多久： 10-20分钟</span><br></pre></td></tr></table></figure><h3 id="4-参数"><a class="markdownIt-Anchor" href="#4-参数"></a> 4、参数</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">sqoop</span><br><span class="line">--import</span><br><span class="line">--connect 数据库url</span><br><span class="line">--username</span><br><span class="line">--passwd</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">目标路径及删除</span></span><br><span class="line">--target-dir</span><br><span class="line">--delete-target-dir</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">空值处理</span></span><br><span class="line">--null-string &#x27;\\N&#x27;</span><br><span class="line">--null-non-string &#x27;\\N&#x27;</span><br><span class="line"></span><br><span class="line">--query &quot;select id,name,age from user xxxxxx and $CONDITIONS&quot;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">可选（切分字段、map数）</span></span><br><span class="line">--split-by id</span><br><span class="line">--mappers 100</span><br></pre></td></tr></table></figure><h3 id="5-同步策略"><a class="markdownIt-Anchor" href="#5-同步策略"></a> 5、同步策略</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">全量:  sql后面加一个 where 1&#x3D;1</span><br><span class="line">新增： sql后面加   where 创建时间&#x3D;昨天 or 操作时间&#x3D;昨天</span><br><span class="line">--increment append（用在直接导入hive表的时候）</span><br><span class="line">特殊： 只导一次</span><br><span class="line">新增及变化： 拉链</span><br></pre></td></tr></table></figure><h3 id="6-到处是-parquet-问题"><a class="markdownIt-Anchor" href="#6-到处是-parquet-问题"></a> 6、到处是 Parquet 问题</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1）先导出到一张临时的hive表，使用text格式</span><br><span class="line">2）ads层一开始就不要创建parquet格式的表</span><br></pre></td></tr></table></figure><h3 id="7-分隔符的问题"><a class="markdownIt-Anchor" href="#7-分隔符的问题"></a> 7、分隔符的问题</h3><h2 id="九-azkaban"><a class="markdownIt-Anchor" href="#九-azkaban"></a> 九、Azkaban</h2><h3 id="1-每天跑多少指标"><a class="markdownIt-Anchor" href="#1-每天跑多少指标"></a> 1、每天跑多少指标？</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">100来个， 活动时 200来个</span><br></pre></td></tr></table></figure><h3 id="2-每天调度多久什么时候开始什么时候跑完"><a class="markdownIt-Anchor" href="#2-每天调度多久什么时候开始什么时候跑完"></a> 2、每天调度多久？什么时候开始？什么时候跑完？</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">第一个脚本：sqoop同步脚本 &#x3D;&#x3D;&#x3D;》  00：10-00：30</span><br><span class="line">执行多久： 正常3-4小时跑完，</span><br><span class="line">几点跑完： 5点钟左右</span><br><span class="line">保证8点之前结果都出来 &#x3D;》 老板要看</span><br></pre></td></tr></table></figure><h3 id="3-异常怎么办怎么告警"><a class="markdownIt-Anchor" href="#3-异常怎么办怎么告警"></a> 3、异常怎么办？怎么告警？</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">告警</span><br><span class="line">azakaban &#x3D;》 发邮件 、微信、钉钉、发短信</span><br><span class="line">打电话 &#x3D;》 调用电信运营商的接口 &#x3D;&#x3D;&#x3D;&#x3D;》 借用第三方运维平台 onealter</span><br><span class="line">处理异常： 优先重跑： 1、自动重跑  2、手动</span><br><span class="line">   重跑还失败：看日志   yarn logs -applicationId appId | less &#x3D;》 shift+g &#x3D;》 上翻键 pgUp</span><br><span class="line"></span><br><span class="line">Exception： xxxxxx</span><br><span class="line">at</span><br><span class="line">at</span><br><span class="line">at</span><br><span class="line">Causeby： xxxxx</span><br><span class="line">at</span><br><span class="line">at</span><br><span class="line">at</span><br><span class="line">Cause by： xxxx</span><br><span class="line">at</span><br><span class="line">at</span><br><span class="line">at</span><br><span class="line"></span><br><span class="line">看最后一个cause by</span><br></pre></td></tr></table></figure><h2 id="十-hbase"><a class="markdownIt-Anchor" href="#十-hbase"></a> 十、HBase</h2><h3 id="1-架构"><a class="markdownIt-Anchor" href="#1-架构"></a> 1、架构</h3><p><img src="https://i.loli.net/2021/05/07/OM37ujZYNgQdxtD.png#alt=image-20210507224558236" alt="" /></p><h3 id="2-写流程"><a class="markdownIt-Anchor" href="#2-写流程"></a> 2、写流程</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">先写 hlog，再写memstore</span><br><span class="line">&#x3D;》扩展： 写Hlog有一个事务，如果memstore写成功了，之后提交HLog事务</span><br><span class="line">如果memstore写失败了，之后回滚事务</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/05/07/QwCSuRHOnoLcB3D.png#alt=image-20210507224725369" alt="" /></p><h3 id="3-读流程"><a class="markdownIt-Anchor" href="#3-读流程"></a> 3、读流程</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">依次读 memstore（写内存）、blockcache（读缓存）、storefile</span><br><span class="line">blockcache淘汰：基于LRU算法（最近最少使用）【BlockFile存储的是HFile中的Block信息】</span><br><span class="line">写比读快：自己跟自己对比</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/05/07/TgqucBQD9vWeRPF.png#alt=image-20210507225317137" alt="" /></p><h3 id="4-刷写条件"><a class="markdownIt-Anchor" href="#4-刷写条件"></a> 4、刷写条件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">memStore级别： 某个memStore 达到 xxxx.flush.size &#x3D; 128M</span><br><span class="line">Region级别：   region内部的所有memStore总和达到了 128M * 4 ， 阻塞写</span><br><span class="line">RegionServer级别： 公式 &#x3D;》   堆内存 * 0.4 * 0.95  ，超过这个阈值，开始刷写， 由大到小依次刷写</span><br><span class="line">刷写到什么时候： 小于 堆内存 * 0.4 * 0.95</span><br><span class="line">HLog数量： 现在不用手动配置，上限 32个</span><br><span class="line">定期刷写： 默认1小时，最后一次编辑时间</span><br><span class="line">手动刷写： 执行命令</span><br><span class="line"></span><br><span class="line">注意：刷写的时候如果数据存在多个版本，那么就只会刷写最新的版本，老版本会直接被丢弃掉</span><br></pre></td></tr></table></figure><h3 id="5-合并"><a class="markdownIt-Anchor" href="#5-合并"></a> 5、合并</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">为什么要合并： 每次刷写都生成一个新的 HFile ，时间久了很多小文件</span><br><span class="line">小合并：相邻的几个HFile，合并成一个新的大的HFile，原先的小HFile删除掉</span><br><span class="line">不会删除 被标记为删除、过期的数据</span><br><span class="line">大合并：所有的HFile，合并成一个大的HFile</span><br><span class="line">删除 被标记为删除、过期的数据</span><br><span class="line">注意：大合并会影响正常使用</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/05/07/KUHYx6bN8Wy3RGD.png#alt=image-20210507230639275" alt="" /></p><h3 id="6-自动切分"><a class="markdownIt-Anchor" href="#6-自动切分"></a> 6、自动切分</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">0.94之前： 固定达到10G就切成两个5G</span><br><span class="line">0.94-2.0： Min(2*R^3 * 128M,10G&quot;)</span><br><span class="line">R : 当前Region Server中属于该Table的Region个数</span><br><span class="line">最开始第一个，达到256M切分成两个 128M的 Region</span><br><span class="line">后面， 变成 达到10G，切分成两个 5G的Region</span><br><span class="line">存在缺陷： 数据倾斜（不是热点问题）</span><br><span class="line"></span><br><span class="line">2.0之后：除了第一次用 256M 切分，后续直接就是按照10G切</span><br></pre></td></tr></table></figure><p>​ <img src="https://i.loli.net/2021/05/07/DzVbasHjBtAdS8X.png#alt=image-20210507230857911" alt="" /></p><h3 id="7-rowkey-设计"><a class="markdownIt-Anchor" href="#7-rowkey-设计"></a> 7、rowkey 设计</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">长度： 一般16字节以下</span><br><span class="line">唯一： rowkey不能重复</span><br><span class="line">散列： 尽量分布均匀，散开   &#x3D;》 hash、MD5、 反转</span><br></pre></td></tr></table></figure><h3 id="8-预分区"><a class="markdownIt-Anchor" href="#8-预分区"></a> 8、预分区</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">指定分区键：</span><br><span class="line">建表语句，指定 splits &#x3D;&gt; &#123; 001|,002|,003|&#125;</span><br><span class="line">为什么给 | ，因为 rowkey是 按位比较，按照 ASC码，   _比较小的值， |比较大的值</span><br><span class="line"></span><br><span class="line">注意：即使指定了分区键，后续还是会进行自动切分 直接按照10G切</span><br><span class="line">-∞ ~ 001  &#x3D;&#x3D;&#x3D;&#x3D;》 -∞ ~ 中间rowkey  ，  中间rowkey ~ 001</span><br><span class="line">001 ~ 002</span><br><span class="line">002 ~ +∞</span><br></pre></td></tr></table></figure><h3 id="9-数据倾斜与数据热点"><a class="markdownIt-Anchor" href="#9-数据倾斜与数据热点"></a> 9、数据倾斜与数据热点</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">数据倾斜：存储位置、大小</span><br><span class="line">热点：数据本身分布不均</span><br></pre></td></tr></table></figure><h3 id="10-rowkey-设计与分区键的使用"><a class="markdownIt-Anchor" href="#10-rowkey-设计与分区键的使用"></a> 10、rowkey 设计与分区键的使用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">手机号：12312312312，对于不同的手机号码使用的频率可能不同，因此就有可能出现热点问题</span><br><span class="line"></span><br><span class="line">分区键： 001|   002|  003|</span><br><span class="line"></span><br><span class="line">第一步：rowkey？？？？？？？？？？&#x3D;&#x3D;&#x3D;》  (分区号+手机号)作为rowkey &#x3D;&#x3D;》 分区号怎么获取？ &#x3D;&#x3D;&#x3D;》 由手机号经过计算获取</span><br><span class="line">12312312312进行hash，得到一个hash值，  然后对 分区数进行取模</span><br><span class="line">a%4 &#x3D; b ， 0《&#x3D;b《&#x3D;3</span><br><span class="line"></span><br><span class="line">rowkey &#x3D;   00b_12312312312</span><br><span class="line"></span><br><span class="line">第二步：考虑散列原则 &#x3D;》 每个人产生的数据量可能差异巨大， 各分区数据量差异巨大， 违背散列</span><br><span class="line">在rowkey再拼接上一个 salt（随机数），一般放在rowkey最前面 这样设计rowkey就可以使得同一个手机号码处于可以处于不同的分区（我们可以对随机数设置一个恰好的范围）</span><br></pre></td></tr></table></figure><h3 id="10-优化"><a class="markdownIt-Anchor" href="#10-优化"></a> 10、优化</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1）调内存 &#x3D;》 16-48G</span><br><span class="line">2）rowkey、预分区</span><br><span class="line">3）文件追加</span><br></pre></td></tr></table></figure><h3 id="11-二级索引"><a class="markdownIt-Anchor" href="#11-二级索引"></a> 11、二级索引</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">什么是二级索引？</span><br><span class="line">Phoenix （或ES）：基于协处理器（写入数据之后）</span><br><span class="line"></span><br><span class="line">全局索引： 索引表和 数据表 不在一个地方</span><br><span class="line">create index 索引表名  ON 数据表名 (索引列) include（其他列）</span><br><span class="line">索引表的id &#x3D;  索引列_原始rowkey ，如果有include的列，也会在里面</span><br><span class="line">本地索引： 索引数据 和 数据表 在同一个region</span><br><span class="line">CREATE LOCAL INDEX 索引表名 ON 数据表名 (索引列) include（其他列）;</span><br><span class="line">在原表中插入索引数据：</span><br><span class="line">分区键_索引列_原始rowkey，多加一列IDX ： 1表示对应数据在， 0表示对应数据过期了</span><br><span class="line"></span><br><span class="line">如果直接往HBase写数据，Phoenix获取不到</span><br><span class="line">补救：往Phoenix再写一份</span><br><span class="line"></span><br><span class="line">建议：写的时候通过Phoenix写（由于存在主键因此只会有一条数据）</span><br></pre></td></tr></table></figure><h2 id="十一-spark"><a class="markdownIt-Anchor" href="#十一-spark"></a> 十一、Spark</h2><h3 id="1-spark-core"><a class="markdownIt-Anchor" href="#1-spark-core"></a> 1、Spark Core</h3><h4 id="1用来计算"><a class="markdownIt-Anchor" href="#1用来计算"></a> 1）用来计算：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">企业里面怎么用？   离线数仓，Hive引擎换成Spark； 直接用SparkSQL操作Hive表； SparkStreaming 用来做实时分析</span><br><span class="line">特点：快 &#x3D;》 基于内存</span><br></pre></td></tr></table></figure><h4 id="2一开始自己做资源管理"><a class="markdownIt-Anchor" href="#2一开始自己做资源管理"></a> 2）一开始自己做资源管理？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">因为它比yarn早</span><br></pre></td></tr></table></figure><h4 id="3部署模式"><a class="markdownIt-Anchor" href="#3部署模式"></a> 3）部署模式？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Local本地测试</span><br><span class="line">Standalone自己管资源</span><br><span class="line">Yarn国内主要的模式</span><br><span class="line">Client Driver跟submit在同一个节点</span><br><span class="line">ClusterDriver在哪启，不一定，由Yarn来决定</span><br><span class="line">Mesos  国外用的</span><br><span class="line">K8S国内趋势</span><br></pre></td></tr></table></figure><h4 id="4几大划分"><a class="markdownIt-Anchor" href="#4几大划分"></a> 4）几大划分？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Job： 一个行动算子一个Job</span><br><span class="line">Stage：遇到宽依赖（Shuffle），就划分stage， 数量&#x3D; 宽依赖数量 + 1</span><br><span class="line">TaskSet：一个Stage里最后一个RDD的分区数，Task的集合</span><br><span class="line">Task：运行在Executor 上的任务</span><br></pre></td></tr></table></figure><h4 id="5rdd-可不可变"><a class="markdownIt-Anchor" href="#5rdd-可不可变"></a> 5）RDD 可不可变？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">不可变</span><br></pre></td></tr></table></figure><h4 id="6rdd-的五大属性"><a class="markdownIt-Anchor" href="#6rdd-的五大属性"></a> 6）RDD 的五大属性？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">计算分区</span><br><span class="line">计算逻辑</span><br><span class="line">依赖</span><br><span class="line">分区器</span><br><span class="line">移动数据不如移动计算</span><br></pre></td></tr></table></figure><h4 id="7rdd-存储数据吗"><a class="markdownIt-Anchor" href="#7rdd-存储数据吗"></a> 7）RDD 存储数据吗？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RDD不存储真正的数据</span><br></pre></td></tr></table></figure><h4 id="8缓存和-checkpoint"><a class="markdownIt-Anchor" href="#8缓存和-checkpoint"></a> 8）缓存和 checkpoint</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">cache ： 默认存储级别，MEMORY_ONLY， 它不会切断血缘关系</span><br><span class="line">checkpoint：默认存储级别，存HDFS， 切断前面的血缘关系</span><br><span class="line">存什么数据： Driver的元数据 、</span><br><span class="line">Streaming里的 updateStateByKey算子，需要保存历史计算结果，也就是数据</span><br><span class="line">如果是SparkStreaming，可能产生小文件问题（由于是按批次来处理的，每3秒一个批次就会产生一个文件）</span><br><span class="line">怎么解决： 合并、使用追加的方式（统一追加到一个文件，或者单独起一个任务定期合并）</span><br><span class="line"></span><br><span class="line">跟Flink有什么区别：</span><br><span class="line">1. spark存储的是 driver元数据，用于故障恢复，只有带状态的算子（如：updateStateByKey）才会存储数据数据：</span><br><span class="line">2. 实现算法不一样（Flink使用Chandy-Lamport算法）</span><br><span class="line">3. flink轻量级的（处理数据和执行checkpoint是异步的），不用 STOP THE WORLD（暂停，做检查点，然后恢复应用执行的行为）</span><br><span class="line">4. 存储位置： 相同点都在 HDFS</span><br><span class="line">5. Flink没有小文件问题：  默认只保留最近一次完整的Checkpoint</span><br><span class="line">6. FLink支持 手动触发的 保存点</span><br><span class="line"></span><br><span class="line">cache + checkpoint 结合使用</span><br><span class="line"></span><br><span class="line">cache什么时候执行？ 行动算子触发之后才会执行</span><br></pre></td></tr></table></figure><h4 id="8重点-算子"><a class="markdownIt-Anchor" href="#8重点-算子"></a> 8）重点： 算子</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">转换算子</span><br><span class="line">单value</span><br><span class="line">map</span><br><span class="line">flatmap</span><br><span class="line">filter</span><br><span class="line">groupby</span><br><span class="line"></span><br><span class="line">coalesce：合并 &#x3D;》 减少分区 ， 不一定会Shuffle</span><br><span class="line">repartition： 扩大分区， 底层调用的 coalesce， 一定会产生Shuffle</span><br><span class="line"></span><br><span class="line">双value（双RDD）</span><br><span class="line">rdd1.算子（rdd2）</span><br><span class="line">union</span><br><span class="line">intesection：交集</span><br><span class="line">subtract：差集</span><br><span class="line">zip</span><br><span class="line"></span><br><span class="line">k-v：</span><br><span class="line">groupbykey ： 根据key分组</span><br><span class="line">reducebykey： 有预聚合.    无初始值，分区内 和 分区间 计算逻辑 相同</span><br><span class="line">foldbykey:      有初始值，分区内 和 分区间 计算逻辑 相同</span><br><span class="line">aggregatebykey：   有初始值，分区内 和 分区间 计算逻辑 可以不相同</span><br><span class="line">combinebykey有初始值（函数，变换结构），分区内 和 分区间 计算逻辑 可以不相同</span><br><span class="line">sortbykey对key进行排序</span><br><span class="line">....</span><br><span class="line"></span><br><span class="line">行动算子</span><br><span class="line">collect</span><br><span class="line">count</span><br><span class="line">take</span><br><span class="line">first</span><br><span class="line">foreach</span><br><span class="line">countbykey</span><br></pre></td></tr></table></figure><h4 id="9宽依赖-窄依赖"><a class="markdownIt-Anchor" href="#9宽依赖-窄依赖"></a> 9）宽依赖、窄依赖</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">”独生子“ &#x3D;》 窄依赖</span><br><span class="line">”非腹生子“ &#x3D;》 宽依赖</span><br></pre></td></tr></table></figure><h4 id="10共享变量累加器-和-广播变量"><a class="markdownIt-Anchor" href="#10共享变量累加器-和-广播变量"></a> 10）共享变量：累加器 和 广播变量</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">存在Executor的哪里？</span><br><span class="line">BlockManager（Driver端是存储在BlockManagerMaster,Executor端是存储在BlockManagerWorker）</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/05/07/5uNgYK2RF8iInjp.png#alt=image-20210507195406900" alt="" /></p><p><img src="https://i.loli.net/2021/05/07/X5sn7wRofCqUlZO.png#alt=image-20210507195257852" alt="" /></p><h3 id="2-spark-sql"><a class="markdownIt-Anchor" href="#2-spark-sql"></a> 2、Spark SQL</h3><h4 id="1怎么支持-hive-表操作"><a class="markdownIt-Anchor" href="#1怎么支持-hive-表操作"></a> 1）怎么支持 Hive 表操作？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">session初始化时，调用surportHive</span><br></pre></td></tr></table></figure><h4 id="2rdd-df-ds"><a class="markdownIt-Anchor" href="#2rdd-df-ds"></a> 2）RDD、DF、DS</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">三者转换图</span><br><span class="line">Spark默认支持Java序列化， DF、DS支持Kryo序列化</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/05/07/h9e6nqLsGlxjwYt.png#alt=image-20210507195521448" alt="" /></p><h3 id="3-spark-streaming"><a class="markdownIt-Anchor" href="#3-spark-streaming"></a> 3、Spark Streaming</h3><h4 id="1批次间隔-秒级"><a class="markdownIt-Anchor" href="#1批次间隔-秒级"></a> 1）批次间隔   秒级</h4><h4 id="2代码执行位置"><a class="markdownIt-Anchor" href="#2代码执行位置"></a> 2）代码执行位置：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Driver执行： main（）里的代码，算子外面；  foreachRDD（）</span><br><span class="line">Executor执行： foreachPartition（）</span><br><span class="line">分区内执行： foreach：</span><br><span class="line">如果涉及到外部系统链接，写在哪里？ foreachPartition</span><br></pre></td></tr></table></figure><h4 id="3设置参数-背压-速率控制-优雅关闭"><a class="markdownIt-Anchor" href="#3设置参数-背压-速率控制-优雅关闭"></a> 3）设置参数： 背压、速率控制、优雅关闭</h4><p>spark.streaming.backpressure.enabled，背压机制，默认为 false</p><p>spark.streaming.kafka.maxRatePerPartition，速率控制（每个分区）</p><p>spark.streaming.stopGracefullyOnShutdown，优雅关闭，默认为 false</p><p><a href="http://spark.apache.org/docs/3.0.0/configuration.html#spark-streaming">http://spark.apache.org/docs/3.0.0/configuration.html#spark-streaming</a></p><h4 id="4窗口"><a class="markdownIt-Anchor" href="#4窗口"></a> 4）窗口：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">两个参数：窗口大小，滑动步长</span><br><span class="line">窗口大小必须 是 批次间隔的 整数倍（一个窗口有多个RDD，也就是为什么通过foreachRDD可以将流转换为RDD）</span><br></pre></td></tr></table></figure><h3 id="4-提交参数"><a class="markdownIt-Anchor" href="#4-提交参数"></a> 4、提交参数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">spark-submit</span><br><span class="line">#部署模式</span><br><span class="line">--master yarn</span><br><span class="line">--deploy-mode cluter</span><br><span class="line">#Driver资源：内存、核数</span><br><span class="line">--driver-memory 8G</span><br><span class="line">--driver-cores xx</span><br><span class="line">#Executor资源： 内存、核数、数量</span><br><span class="line">--executor-memory 8G</span><br><span class="line">--executor-cores xx    cpu核数大概为task个数的 1&#x2F;3~1&#x2F;2</span><br><span class="line">--num-executors xx</span><br><span class="line">#类入口、jar包</span><br><span class="line">--class com.atuigu.test.wordcount</span><br><span class="line">jar包全路径</span><br><span class="line">main方法传参</span><br></pre></td></tr></table></figure><h3 id="5-高阶内核-优化"><a class="markdownIt-Anchor" href="#5-高阶内核-优化"></a> 5、高阶：内核、优化</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1）提交流程</span><br><span class="line">2）通讯架构</span><br><span class="line">3）任务切分和调度</span><br><span class="line">4）Shuffle</span><br></pre></td></tr></table></figure><h4 id="1yarncluster-模式提交流程"><a class="markdownIt-Anchor" href="#1yarncluster-模式提交流程"></a> 1）YarnCluster 模式提交流程</h4><p><img src="https://i.loli.net/2021/05/07/dZFwEbRTuDNamcV.png#alt=image-20210507201306760" alt="" /></p><h4 id="2spark-通讯框架"><a class="markdownIt-Anchor" href="#2spark-通讯框架"></a> 2）Spark 通讯框架</h4><p><img src="https://i.loli.net/2021/05/07/wRibrJ5g2oeSfIT.png#alt=image-20210507201433497" alt="" /></p><h4 id="3stage-任务划分"><a class="markdownIt-Anchor" href="#3stage-任务划分"></a> 3）Stage 任务划分</h4><p><img src="https://i.loli.net/2021/05/07/hS6ne74ORY3TXzj.png#alt=image-20210507201521800" alt="" /></p><h4 id="4task-任务调度执行"><a class="markdownIt-Anchor" href="#4task-任务调度执行"></a> 4）Task 任务调度执行</h4><p><img src="https://i.loli.net/2021/05/07/aPJr21XxELAulWh.png#alt=image-20210507201558960" alt="" /></p><h4 id="5shuffle-流程"><a class="markdownIt-Anchor" href="#5shuffle-流程"></a> 5）Shuffle 流程</h4><p><img src="https://i.loli.net/2021/05/07/qEmo9Y1sClHzth3.png#alt=image-20210507201901394" alt="" /></p><p><img src="https://i.loli.net/2021/05/07/lnm8J6MHoTdBicR.png#alt=image-20210507201943114" alt="" /></p><p><img src="https://i.loli.net/2021/05/07/59ZBXgTfjrpu4l1.png#alt=image-20210507202031078" alt="" /></p><p><img src="https://i.loli.net/2021/05/07/f2BOGTpcM6i537Z.png#alt=image-20210507202121645" alt="" /></p><h3 id="6-手写-wordcount"><a class="markdownIt-Anchor" href="#6-手写-wordcount"></a> 6、手写 wordcount</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc.text . map .flatmap .reducebykey .collect .</span><br></pre></td></tr></table></figure><h2 id="十二-flink"><a class="markdownIt-Anchor" href="#十二-flink"></a> 十二、Flink</h2><h3 id="1-flink-计算引擎"><a class="markdownIt-Anchor" href="#1-flink-计算引擎"></a> 1、Flink 计算引擎</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">流的世界观：有界流、无界流</span><br><span class="line">事件驱动：被动；带状态，不需要依赖数据库</span><br><span class="line">分层API： Process、apply  &#x3D;》 DataStream API,DataSetAPI &#x3D;&gt; Flink Table &#x3D;&gt; Flink SQL</span><br></pre></td></tr></table></figure><h3 id="2-部署模式"><a class="markdownIt-Anchor" href="#2-部署模式"></a> 2、部署模式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Local 单机版，自己管资源</span><br><span class="line">Standalone 集群版，自己管资源</span><br><span class="line">Yarn 国内主要的模式</span><br><span class="line">session 先启一个集群，后续往这提交，共享集群 &#x3D;》 互相之间相互影响</span><br><span class="line">per-job 独立集群模式，每提交一次，yarn启动一个集群，job间隔离</span><br><span class="line">application     基本同per-job，区别： 用户代码是在 JobManager里解析</span><br><span class="line"></span><br><span class="line">企业怎么选？   application 或 per-job</span><br><span class="line">可以同时存在吗？ 可以 ，他们本质都是 yarn的一个 application</span><br><span class="line">提交怎么指定 ，  -Dxxxxx&#x3D;appId</span><br><span class="line"></span><br><span class="line">Mesos</span><br><span class="line">K8S</span><br></pre></td></tr></table></figure><h3 id="3-手写-wordcount"><a class="markdownIt-Anchor" href="#3-手写-wordcount"></a> 3、手写 wordcount</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.readtextfile .flatmap .keyby .sum .print</span><br></pre></td></tr></table></figure><h3 id="4-编程模型-env-source-transform-sink-分层-api"><a class="markdownIt-Anchor" href="#4-编程模型-env-source-transform-sink-分层-api"></a> 4、编程模型： env、source 、transform、sink  =》 分层 API</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">map、flatmap、filter</span><br><span class="line">keyby： 两次hash，第一次hashcode，第二次murmurhash ； 跟 最大并行度 计算 &#x3D;&#x3D;》 keygroupId</span><br><span class="line">公式： id * 下游并行度&#x2F; 最大并行度 &#x3D;</span><br><span class="line">最大并行度，默认 128 ， 可以改 &#x3D;&#x3D;&#x3D;&gt; env.setMaxParallelism(256)</span><br><span class="line">connect：</span><br><span class="line">union</span><br><span class="line"></span><br><span class="line">intervaljoin：底层是 connect + keyby</span><br><span class="line">两条流都存了一个 Map类型的状态，数据来先存里面， key是 ts，value是数据的集合</span><br><span class="line">每条流的数据来，都会遍历对方的 Map状态</span><br><span class="line">到达一定条件，会清理状态</span><br></pre></td></tr></table></figure><h3 id="5-时间语义"><a class="markdownIt-Anchor" href="#5-时间语义"></a> 5、时间语义</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">事件时间、处理时间、注入时间</span><br></pre></td></tr></table></figure><h3 id="6-watermark"><a class="markdownIt-Anchor" href="#6-watermark"></a> 6、watermark</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">1）概念、原理、理解</span><br><span class="line">表示 事件时间的 进展</span><br><span class="line">处理乱序问题</span><br><span class="line">单调不减的（不会倒退）</span><br><span class="line">触发 窗口、定时器 等</span><br><span class="line">认为，在它的时间之前的数据都已经到齐了。 如果后面来一个比它小的，称为迟到数据</span><br><span class="line">是一个特殊的时间戳，特殊的数据，插入到流里面 传递</span><br><span class="line">2）分类</span><br><span class="line">周期性：默认的，官方提供的， 200ms</span><br><span class="line">间歇性：来一条更新一次</span><br><span class="line">3）多并行度下的传递</span><br><span class="line">一对多： 广播</span><br><span class="line">多对一： 取最小</span><br><span class="line">多对多： 两者结合</span><br><span class="line">4）watermark生成公式</span><br><span class="line">有序 watermark &#x3D; 当前最大的事件时间 - 1ms</span><br><span class="line">乱序 watermark &#x3D; 当前最大的事件时间 - 乱序程度 - 1ms</span><br><span class="line">5）读文件问题</span><br><span class="line">在最终结束的时候，一下子更新为Long的最大值</span><br></pre></td></tr></table></figure><h3 id="7-窗口处理时间-事件时间"><a class="markdownIt-Anchor" href="#7-窗口处理时间-事件时间"></a> 7、窗口（处理时间、事件时间）</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">时间：</span><br><span class="line">滚动 ：窗口大小</span><br><span class="line">滑动 ：窗口大小、滑动步长</span><br><span class="line">会话 ：静态 Gap 、动态 Gap</span><br><span class="line">计数：</span><br><span class="line">滚动</span><br><span class="line">滑动</span><br><span class="line"></span><br><span class="line">窗口怎么划分的？为什么左闭右开？</span><br><span class="line">start &#x3D; 事件时间 - （事件时间 - offset + windowSize） % windowSize</span><br><span class="line">以1970年1月1日0点0分0秒0毫秒为基准（伦敦时间，时间戳的0ms）， 对窗口长度取整数倍，就是窗口开始时间</span><br><span class="line">end &#x3D; start + windowSize</span><br><span class="line"></span><br><span class="line">窗口的 最大时间戳 &#x3D; end - 1ms ，所以左闭右开</span><br><span class="line"></span><br><span class="line">窗口的生命周期？ 什么时候创建，什么时候销毁？</span><br><span class="line">属于本窗口的第一条数据来的，直接 new，放到一个 SingletonList（会执行去重处理，后面该窗口的数据再来即使new了，也是无法放进去的）</span><br><span class="line">销毁时间 &#x3D; （窗口结束时间 - 1ms） + 允许迟到时间</span><br><span class="line"></span><br><span class="line">窗口什么时候触发输出</span><br><span class="line">窗口结束时间 - 1ms</span><br></pre></td></tr></table></figure><h3 id="8-slot-与并行度关系"><a class="markdownIt-Anchor" href="#8-slot-与并行度关系"></a> 8、slot 与并行度关系</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">env.readtextfile.flatmap.keyby.sum.print</span><br><span class="line">同一个算子的子任务，不能存在于同一个slot中</span><br><span class="line">不同算子的子任务，可以同时存在与同一个slot中 &#x3D;》 slot可以共享，slotSharingGroup--槽共享组</span><br></pre></td></tr></table></figure><h3 id="9-状态"><a class="markdownIt-Anchor" href="#9-状态"></a> 9、状态</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">算子状态：没有keyby的算子 &#x3D;》 一个并行实例，一个状态 &#x3D;》 一般是 source的时候用 &#x3D;》</span><br><span class="line">list：（实习一个接口CheckpointedFunction）</span><br><span class="line">普通list</span><br><span class="line">联合list</span><br><span class="line">广播状态：</span><br><span class="line">键控状态：keyby后的算子 &#x3D;》 一个key 一个状态</span><br><span class="line">value</span><br><span class="line">list</span><br><span class="line">map</span><br><span class="line">reducing</span><br><span class="line">aggregating</span><br></pre></td></tr></table></figure><h3 id="10-checkpoint"><a class="markdownIt-Anchor" href="#10-checkpoint"></a> 10、checkpoint</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Chandy-Lamport ： 异步分界线快照 &#x3D;》 轻量级的</span><br><span class="line">异步： 处理数据 和 checkpoint 异步的</span><br><span class="line"></span><br><span class="line">barrier 对齐 （精准一次）、不对齐（至少一次）</span><br></pre></td></tr></table></figure><p>​ <img src="https://i.loli.net/2021/05/07/DBm5q7bTyVjhNrU.png#alt=image-20210507223607244" alt="" /></p><h3 id="11-savepoint"><a class="markdownIt-Anchor" href="#11-savepoint"></a> 11、savepoint</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">算法与 checkpoint一致， 手动触发 ： flink savepoint xxxxx</span><br><span class="line">从保存点获取checkpoint： -s 指定路径</span><br></pre></td></tr></table></figure><h3 id="12-cep"><a class="markdownIt-Anchor" href="#12-cep"></a> 12、CEP</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">复杂事件处理：</span><br><span class="line">模式组（模式序列）： next、followedby 、followedbyany</span><br><span class="line">量词： times   、 oneormore  、 until</span><br><span class="line">超时：within</span><br><span class="line">乱序怎么办？ 只要是事件时间语义，自己会处理</span><br><span class="line"></span><br><span class="line">超时数据： select（侧输出流标签，超时数据处理函数，匹配上的数据处理函数）</span><br><span class="line">数据存在哪里？ Map状态</span><br><span class="line"></span><br><span class="line">option 可选的， greedy 贪婪的</span><br></pre></td></tr></table></figure><h3 id="13-table-api-sql"><a class="markdownIt-Anchor" href="#13-table-api-sql"></a> 13、Table API &amp; SQL</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">动态表：</span><br><span class="line">连续查询：</span><br></pre></td></tr></table></figure><p>参考 FLINK 版实时项目_优化   的 FlinkSQL 调优</p><p><img src="https://i.loli.net/2021/05/07/qntDuw8blPeUWOS.png#alt=image-20210507223250424" alt="" /></p><h3 id="14-提交脚本"><a class="markdownIt-Anchor" href="#14-提交脚本"></a> 14、提交脚本</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;flink run \</span><br><span class="line">-t yarn-per-job \</span><br><span class="line">-d \</span><br><span class="line">-p 5 \ 指定并行度</span><br><span class="line">-Dyarn.application.queue&#x3D;test \ 指定yarn队列</span><br><span class="line">-Djobmanager.memory.process.size&#x3D;2048mb \ JM2~4G足够</span><br><span class="line">-Dtaskmanager.memory.process.size&#x3D;6144mb \ 单个TM2~8G足够</span><br><span class="line">-Dtaskmanager.numberOfTaskSlots&#x3D;2 \ 与容器核数1core：1slot或1core：2slot （对于cpu核数是yarn启动时，在yarn的配置文件中配置的）</span><br><span class="line">-Denv.java.opts&#x3D;&quot;-XX:+PrintGCDetails -XX:+PrintGCDateStamps&quot; \GMV参数</span><br><span class="line">-c com.atguigu.app.dwd.LogBaseApp \</span><br><span class="line">&#x2F;opt&#x2F;module&#x2F;gmall-flink&#x2F;gmall-realtime-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure><p>-t 指定模式时，参数的指定方式是-D 参数名=参数值（参考 FLINK 版实时项目_优化）</p><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/config.html">https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/config.html</a></p><h3 id="15-监控"><a class="markdownIt-Anchor" href="#15-监控"></a> 15、监控</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Prometheus + Grafana</span><br></pre></td></tr></table></figure><h3 id="资料"><a class="markdownIt-Anchor" href="#资料"></a> 资料：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Hbase ： HBase技术社区</span><br><span class="line">ES ：   铭毅天下           死磕Elastic Search</span><br><span class="line">Flink： zhisheng</span><br><span class="line">Spark： 浪尖聊大数据、大数据数仓与架构</span><br><span class="line">Hadoop：谁都行</span><br></pre></td></tr></table></figure><h2 id="十三-数仓架构"><a class="markdownIt-Anchor" href="#十三-数仓架构"></a> 十三、数仓架构</h2><h3 id="1-来源-产出"><a class="markdownIt-Anchor" href="#1-来源-产出"></a> 1、来源、产出</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">来源：前端埋点产生的用户行为日志数据</span><br><span class="line"> javaee后台的业务数据</span><br><span class="line"> 爬虫（面向监狱）</span><br><span class="line"></span><br><span class="line">产出：出报表、 用户画像、 推荐系统、算法预测</span><br></pre></td></tr></table></figure><h3 id="2-版本选型"><a class="markdownIt-Anchor" href="#2-版本选型"></a> 2、版本选型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">apache开源版：</span><br><span class="line">hadoop发行版：</span><br><span class="line">CDH ： Cloudera ，  6.3.2</span><br><span class="line">HDP ： Hortonworks3.1.4 + Ambari 2.7.4</span><br><span class="line">合并后的CDP： Cloudera ，一个节点一万美金   byd比亚迪</span><br><span class="line"></span><br><span class="line">选型： 从长远考虑， apache更好。  兼容性问题、麻烦 &#x3D;&gt; 我会</span><br></pre></td></tr></table></figure><h3 id="3-服务器"><a class="markdownIt-Anchor" href="#3-服务器"></a> 3、服务器</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">单台 128G内存，20核40线程， 8T磁盘 2T固态</span><br></pre></td></tr></table></figure><h3 id="4-集群规模-重"><a class="markdownIt-Anchor" href="#4-集群规模-重"></a> 4、集群规模 （重）</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">磁盘空间</span><br><span class="line">100万日活，1天100G日志数据</span><br><span class="line"></span><br><span class="line">日志数据数仓分层：</span><br><span class="line">ods：10G（压缩后）</span><br><span class="line">dwd：10G（压缩后）</span><br><span class="line">dws + dwt ： 50G</span><br><span class="line">ads：忽略</span><br><span class="line"></span><br><span class="line">10+10+50 &#x3D; 70G</span><br><span class="line">70G * 3副本 * 180天 &#x2F; 0.7 &#x3D; 约 53T</span><br><span class="line"></span><br><span class="line">业务数据数仓分层：</span><br><span class="line">10万订单、每个订单10条，每条1k &#x3D; 1G</span><br><span class="line">往大了估算 3G</span><br><span class="line">3G * 3副本 * 180天 &#x2F;0.7 &#x3D; 约2T</span><br><span class="line"></span><br><span class="line">Kafka：</span><br><span class="line">100G * 2副本 * 3天 &#x2F;0.7 &#x3D; 约1T</span><br><span class="line"></span><br><span class="line">综上 ： 53 + 2 + 1 &#x3D; 56T</span><br><span class="line"></span><br><span class="line">半年： 56&#x2F;8 &#x3D; 7台</span><br><span class="line">一年： 14台， 磁盘可以单独扩容， 10-14台都可以</span><br><span class="line"></span><br><span class="line">内存</span><br><span class="line">128G * 7台 &#x3D; 896G</span><br><span class="line">多少用于计算 （单台128G，100G用于Yarn）： 100*7&#x3D;700G，其他的给其他的框架、系统使用</span><br><span class="line">离线 ：   128M数据： 1G内存  &#x3D;》 同时最多一起处理 87G左右的数据</span><br><span class="line">实时 ： Job，老大2G、小弟4G</span><br><span class="line"></span><br><span class="line">离线是夜里才占用资源， 实时是24小时都在执行</span><br><span class="line"></span><br><span class="line">CPU</span><br><span class="line">40线程 * 7台 &#x3D; 280线程 （200线程用来计算+ 80线程给其他）</span><br></pre></td></tr></table></figure><h3 id="5-从-0-1问你怎么办需要什么"><a class="markdownIt-Anchor" href="#5-从-0-1问你怎么办需要什么"></a> 5、从 0-1，问你怎么办？需要什么？</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1）数据量多大？</span><br><span class="line">2）预算</span><br><span class="line">3）周期</span><br><span class="line">4）人员</span><br><span class="line">5）多少服务器 0</span><br><span class="line">6）需求指标，前期20个</span><br><span class="line">7）数据分布情况mysql、oracle、db2、sqlserver。。。 &#x3D;&#x3D;&#x3D;》 ETL方案</span><br><span class="line">8）出一个方案： 服务器、怎么部署</span><br></pre></td></tr></table></figure><h2 id="十四-数仓分层重点"><a class="markdownIt-Anchor" href="#十四-数仓分层重点"></a> 十四、数仓分层（重点）</h2><h3 id="1-怎么建模什么工具"><a class="markdownIt-Anchor" href="#1-怎么建模什么工具"></a> 1、怎么建模？什么工具？</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">了解熟悉业务 &#x3D;》 表与表之间的关系  EZDML</span><br></pre></td></tr></table></figure><h3 id="2-每层干了什么事"><a class="markdownIt-Anchor" href="#2-每层干了什么事"></a> 2、每层干了什么事</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">ods层</span><br><span class="line">1)保持数据原貌备份</span><br><span class="line">2)压缩 减少磁盘空间</span><br><span class="line">3)创建分区表避免后续全表扫描</span><br><span class="line">dwd层</span><br><span class="line">1）清洗规则</span><br><span class="line">去重</span><br><span class="line">解析</span><br><span class="line">核心字段不为空：要么过滤，要么替换（nvl、case when 、if）</span><br><span class="line">脱敏：个人的手机号、身份证号（个人隐私）等</span><br><span class="line">超期数据（爬到的数据）</span><br><span class="line">2）清洗的手段</span><br><span class="line">mr、hql、spark、flink、 kettle（工具）、python</span><br><span class="line">我们项目用的：hql</span><br><span class="line">3）清洗比例</span><br><span class="line">万分之一</span><br><span class="line">4）分区表</span><br><span class="line">5）压缩</span><br><span class="line">6）列式存储 parquet</span><br><span class="line">7）维度退化（降维）</span><br><span class="line">基于维度建模的星型模型理论，</span><br><span class="line">商品表、品类表、SPU表、三、二、一分类 &#x3D;》 商品维度表</span><br><span class="line">省份表、地区表 &#x3D;》 地区维度表</span><br><span class="line">8）维度建模</span><br><span class="line">选择业务过程</span><br><span class="line">小公司 &#x3D;》 全都要 （20-30）</span><br><span class="line">大公司，上千张表 &#x3D;》 选择需要的业务模块</span><br><span class="line"></span><br><span class="line">声明粒度</span><br><span class="line">什么是粒度； 一行数据代表什么样的行为：   一次、一天、一周</span><br><span class="line">最小粒度：不要进行聚合汇总操作，就是当前能达到的最小粒度</span><br><span class="line"></span><br><span class="line">确定维度</span><br><span class="line">什么时间、什么地点、什么人、对什么东西</span><br><span class="line"></span><br><span class="line">时间、地区、用户、商品、优惠券、活动</span><br><span class="line"></span><br><span class="line">确定事实</span><br><span class="line">关注事实的度量值</span><br><span class="line">个数、件数、金额（可以统计的）</span><br><span class="line"></span><br><span class="line">dws层： 按天汇总（宽表）</span><br><span class="line">有哪些宽表：有多少维度就有多少宽表 &#x3D;》 地区、会员、设备、商品、活动</span><br><span class="line">宽表有哪些字段： 站在维度的角度，关注事实的度量值</span><br><span class="line"></span><br><span class="line">dwt层： 累积汇总</span><br><span class="line">有哪些宽表：有多少维度就有多少宽表 &#x3D;》 地区、会员、设备、商品、活动</span><br><span class="line">宽表有哪些字段： 站在维度的角度，关注事实的 开始时间&#x2F;结束时间&#x2F;从开始到结束的度量值的累积值&#x2F;最近一段时间内度量值的累积值</span><br><span class="line"></span><br><span class="line">ads层：一口气说出30个指标：日活、月活、周活、留存、留存率、新增（日、周、年）、转化率、流失、回流、七天内连续3天登录（点赞、收藏、评价、购买、加购、下单、活动）、连续3周（月）登录、GMV、复购率、复购率排行、点赞、评论、收藏、领优惠价人数、使用优惠价、沉默、值不值得买、退款人数、退款率  topn  热门商品</span><br><span class="line">产品经理最关心的：留转G复活</span><br></pre></td></tr></table></figure><h2 id="十五-数仓业务"><a class="markdownIt-Anchor" href="#十五-数仓业务"></a> 十五、数仓业务</h2><h3 id="1-拉链表"><a class="markdownIt-Anchor" href="#1-拉链表"></a> 1、拉链表：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">初始拉链表</span><br><span class="line">变化表</span><br><span class="line"></span><br><span class="line">第一步： 将变化表 加两个字段， 开始时间（dt -1 ）、结束时间（9999） &#x3D;&#x3D;&#x3D;》 表A</span><br><span class="line">第二步： 将 初始拉链表 left join 变化表 on 用户id， 如果关联上，</span><br><span class="line">将初始拉链表对应数据的 结束时间改成 dt - 1 ，否则保持</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;》 表 B</span><br><span class="line"></span><br><span class="line">第三步： A union all B</span><br><span class="line">第四步： 将 第三步得到的临时表， insert overwrite 到 初始拉链表</span><br><span class="line"></span><br><span class="line">如果前天的拉链表出现问题， 今天拉链表做完了，怎么办？</span><br><span class="line">今天 5.8 ,  5.6有问题，怎么办？</span><br><span class="line"></span><br><span class="line">解决： 1.退链（sql实现）  ，退到5.6之前</span><br><span class="line">   2.从 5.6开始，重新跑。   5.6 、 5.7 、5.8 跑一遍</span><br></pre></td></tr></table></figure><h3 id="2-几张表"><a class="markdownIt-Anchor" href="#2-几张表"></a> 2、几张表？</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ods ： 1张用户行为日志表  +  23张表业务表 &#x3D;》 24张</span><br><span class="line">dwd ： 5张用户行为表 + 23张业务表 - （商品 6 - 1） - （地区 2 - 1） — （活动 2-1） + 1时间  &#x3D;&#x3D;》 22张</span><br><span class="line">dws ： 5张宽表</span><br><span class="line">dwt ： 5张宽表</span><br><span class="line">ads ： 小几十张， 30张</span><br><span class="line"></span><br><span class="line">24 + 22 + 10 + 30 &#x3D; 86张 （不要说 准确数字）</span><br></pre></td></tr></table></figure><h2 id="十六-测试"><a class="markdownIt-Anchor" href="#十六-测试"></a> 十六、测试</h2><h3 id="1-上线"><a class="markdownIt-Anchor" href="#1-上线"></a> 1、上线</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">离线 ： 脚本提交到azkaban调度</span><br><span class="line">实时 ： 打包，执行提交命令</span><br><span class="line"></span><br><span class="line">先下，再上</span><br></pre></td></tr></table></figure><h3 id="2-测试手段"><a class="markdownIt-Anchor" href="#2-测试手段"></a> 2、测试手段</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">边界值： sum（a，b）  &#x3D;&#x3D;&#x3D;》   a 1-100  b 100-1000</span><br><span class="line">临界条件： a 1  100 0 -1 101</span><br><span class="line">   b 100 1000  1   10000</span><br><span class="line"></span><br><span class="line">等价类： a 1 3 5 10 20 23   b   100 200 210 250 300 800</span><br><span class="line"></span><br><span class="line">测试用例：   功能 sum（a，b）  输入 1， 1   预期结果  2  ，实际结果 3  测试结论： 计算不准确</span><br><span class="line"></span><br><span class="line">测试没过 &#x3D;》 1 这不是一个bug  &#x3D;</span><br><span class="line"> &#x3D;》 是一个bug &#x3D;》 禅道、bugzila</span><br><span class="line"></span><br><span class="line">黑盒测试  、 白盒测试</span><br></pre></td></tr></table></figure><h3 id="3-敏捷开发小步快跑"><a class="markdownIt-Anchor" href="#3-敏捷开发小步快跑"></a> 3、敏捷开发（小步快跑）</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">需求10个</span><br><span class="line">开发</span><br><span class="line">测试</span><br><span class="line">上线</span><br><span class="line"></span><br><span class="line">需求10个</span><br><span class="line">开发</span><br><span class="line">测试</span><br><span class="line">上线</span><br></pre></td></tr></table></figure><h2 id="十七-实时数仓"><a class="markdownIt-Anchor" href="#十七-实时数仓"></a> 十七、实时数仓</h2><h3 id="1-ods-层"><a class="markdownIt-Anchor" href="#1-ods-层"></a> 1、ods 层</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">业务数据：mysql &#x3D;》 kafka，使用flink cdc， 底层debezium，</span><br><span class="line"></span><br><span class="line">同步过来的数据，自定义反序列化器，解析封装数据：</span><br><span class="line"></span><br><span class="line">几个topic？  日志1，业务1</span><br></pre></td></tr></table></figure><h3 id="2-dwd-层-dim-层"><a class="markdownIt-Anchor" href="#2-dwd-层-dim-层"></a> 2、dwd 层、dim 层</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dwd层：在kafka，存 日志数据、业务数据的事实表</span><br><span class="line"></span><br><span class="line">几个topic： 日志3，业务12</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dim层： 在HBase，存 业务数据的 维度表</span><br><span class="line"></span><br><span class="line">几张表：16张</span><br></pre></td></tr></table></figure><h3 id="日志数据"><a class="markdownIt-Anchor" href="#日志数据"></a> 日志数据：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1）识别新老用户：校正新老用户标识</span><br><span class="line"></span><br><span class="line">按照mid分组，首次访问时间不为空 且 时间早于当日 &#x3D;》 老访客</span><br><span class="line"></span><br><span class="line">否则，新访客 &#x3D;》 保存 首次访问时间为 今天</span><br><span class="line"></span><br><span class="line">2）数据拆分：  利用侧输出流对日志数据进行拆分（页面、启动、曝光），进入dwd层（kafka）</span><br></pre></td></tr></table></figure><h3 id="业务数据"><a class="markdownIt-Anchor" href="#业务数据"></a> 业务数据：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">1）动态分流：区分维度表、事实表，为了避免因表的变化而重启任务，在mysql存一张表来动态配置，</span><br><span class="line"></span><br><span class="line">方案一：使用Flink CDC获取mysql的配置表（获取变化数据，不需要重启任务）</span><br><span class="line"></span><br><span class="line">字段：来源表、操作类型、输出类型、输出表（Topic）、输出字段、主键字段、建表扩展</span><br><span class="line"></span><br><span class="line">联合主键： 源表名+操作类型</span><br><span class="line"></span><br><span class="line">              结合配置，事实表放主流写kafka、维度表放侧流写hbase</span><br><span class="line"></span><br><span class="line">方案二：在open（）方法里周期性查询mysql的配置表</span><br><span class="line"></span><br><span class="line">2）HBase查询优化：使用redis作旁路缓存 （ 后续查询时，查过的放在redis缓存，下次直接从redis查）</span><br><span class="line"></span><br><span class="line">设置redis过期时间24小时</span><br><span class="line"></span><br><span class="line">通过Phoenix JDBC的方式写HBase，如果是更新操作，删除Redis的缓存（保证一致性）</span><br></pre></td></tr></table></figure><h3 id="3-dwm"><a class="markdownIt-Anchor" href="#3-dwm"></a> 3、dwm</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">部分需求直接从DWD层到DWS层中间会有一定的计算量，而且这部分计算的结果很有可能被多个DWS层主题复用，所以部分DWD成会形成一层DWM，存在kafka</span><br><span class="line"></span><br><span class="line">1）哪些表： 访客uv、跳出明细、订单宽表、支付宽表</span><br><span class="line"></span><br><span class="line">2）拉宽表的时候，需要关联 维度数据（HBase）、事实表（kafka）</span><br><span class="line"></span><br><span class="line">事实表的关联 &#x3D;》双流join （interval join）</span><br><span class="line"></span><br><span class="line">与维表关联 &#x3D;》异步io（不需要阻塞式等待、单个并行可以连续发送多个请求）、redis旁路缓存</span><br><span class="line"></span><br><span class="line">缓存设过期时间24小时，避免冷数据常驻缓存浪费资源。</span><br><span class="line"></span><br><span class="line">维度数据发生变化，主动清除缓存（在dwd层动态分流写Hbase时，update类型，清楚redis数据）</span><br></pre></td></tr></table></figure><h3 id="4-dws-层主题宽表clickhouse"><a class="markdownIt-Anchor" href="#4-dws-层主题宽表clickhouse"></a> 4、dws 层（主题宽表，clickhouse）</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">轻度聚合、减少维度查询的次数</span><br><span class="line"></span><br><span class="line">访客主题宽表、商品主题宽表、地区主题宽表、关键词主题宽表</span><br><span class="line"></span><br><span class="line">确认维度和度量(事实数据)</span><br><span class="line"></span><br><span class="line">对 dwd明细数据、dwm数据 进行 合并、统一格式、开窗聚合（考虑时效性&#x3D;》轻度聚合&#x3D;》滚动窗口10s）</span><br><span class="line"></span><br><span class="line">写入 clickhouse &#x3D;》 选用 ReplacingMergeTree引擎 ，保证数据表的幂等性</span><br><span class="line"></span><br><span class="line">去重有延迟，怎么？  &#x3D;&#x3D;》 查询逻辑，加上去重逻辑</span><br></pre></td></tr></table></figure><h3 id="5-可视化大屏"><a class="markdownIt-Anchor" href="#5-可视化大屏"></a> 5、可视化大屏</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">百度的sugar</span><br><span class="line"></span><br><span class="line">sugar &#x3D;》 springboot数据接口服务 &#x3D;》 jdbc查询clickhouse</span><br></pre></td></tr></table></figure><h3 id="6-监控"><a class="markdownIt-Anchor" href="#6-监控"></a> 6、监控</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prometheus + Grafana</span><br></pre></td></tr></table></figure><h3 id="7-优化"><a class="markdownIt-Anchor" href="#7-优化"></a> 7、优化</h3><p>1）反压？</p><p>2）数据倾斜？</p><p>3）资源设置？</p><p>4）FlinkSQL =》   有</p><p>5）rocksdb、</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rocksdb状态一直增长，最终 超过TM分配的内存，导致 yarn的容器 被kill</span><br><span class="line"></span><br><span class="line">调整 ，</span><br></pre></td></tr></table></figure><h3 id="8-所有框架的监控"><a class="markdownIt-Anchor" href="#8-所有框架的监控"></a> 8、所有框架的监控</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">zabbix  监控机器、进程（所有框架 挂掉，）</span><br><span class="line"></span><br><span class="line">ganglia  监控 flume</span><br><span class="line"></span><br><span class="line">kafkaeagle 监控kafka</span><br><span class="line"></span><br><span class="line">prometheus 监控flink</span><br><span class="line"></span><br><span class="line">grafana 可视化展示、告警</span><br></pre></td></tr></table></figure><p>​</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="posts/0/"/>
      <url>posts/0/</url>
      
        <content type="html"><![CDATA[<p>服务器选型：</p><p>128G 内存，8T 机械，2T 固态，20 核 40 线程</p><p>用户行为数据：</p><p>日活 60 万，每人每日产生 100 条，也就是 60G 用户行为数据</p><p>ODS 层：6G</p><p>DWD 层：6G</p><p>DWS,DWT 层：30G</p><p>ADS 层忽略</p><p>总共：（6+6+30）_3  _180  / 0.7 = 33T</p><p>业务数据：</p><p>订单 6 万，每人每日产生 10 条，也就是 600M 业务数据</p><p>数仓五层存储大概是：600M*3 约等于 2G</p><p>总共：2 _3  _180 /0.7= 2T</p><p>Kafka 数据：</p><p>60G _2 _3  /0.7 =500G</p><p>总共：33T+2T+500G 约等于 36T</p><p>36T / 8T=5 台</p><p>内存：</p><p>5 台的话内存就是 128*5= 640G 的内存，</p><p>由于每台会有 100G 用于 yarn 那么 yarn 计算总共使用 500G==&gt;那么可以运算 64G 的数据</p><p>CPU：</p><p>40*5=200 线程</p><p>留转 G 复活</p><p>活跃：日活 60 万 周活 70 万 月活 120 万   总注册量 1000 万</p><p>GMV：60 万日活，大概就有 6 万个订单，6*50~100 也就是 300 万-600 万，最后 10%-20%就剩下：60 万~120 万</p><p>复购率：手巾牙膏 10%~20%，手机电脑 1%</p><p>转化率：5%70%下单–&gt;90%~95%支付</p><p>留存率：1 日 2 日 3 日留存率  10%~20%</p><p>如果活动的话，数据量增加多少？？</p><p>日活增加 50%，GMV 大概是 1.2~1.5 倍左右</p><p>有多少张表？？？</p><p>ODS 层：24 张</p><p>DWD 层：22 张 （23+5 -5-1-1+1）</p><p>DWS 层：5 张</p><p>DWT 层：5 张</p><p>ADS 层：30 张</p><p>总共大概是：80 张表</p><p>Kafka 的数据如何？？？</p><p>60 万日活，那么数据就是 60G</p><p>60G /3600/24=0.8M/s，平均每秒 800 条</p><p>*220M/s，也就是每秒数据大概是 2 千到 2 万条</p><p>Sqoop</p><p>60 万日活，6 万订单，每个订单 10 条，也就是 600M 的数据</p><p><img src="https://i.loli.net/2021/05/09/KE5CIWHL9scrzgO.png#alt=image-20210508235945943" alt="" /></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>博客搭建教程</title>
      <link href="posts/116b0f3c/"/>
      <url>posts/116b0f3c/</url>
      
        <content type="html"><![CDATA[<h1 id="安装与配置"><a class="markdownIt-Anchor" href="#安装与配置"></a> 安装与配置</h1><p>1，右击选择<strong>Git Bash</strong>，然后输入初始化 Hexo 博客，git 就会自动帮你下载 hexo 框架</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo init hexo</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/02/21/DawpShjGe2HxmMR.png#align=left&amp;display=inline&amp;height=260&amp;margin=%5Bobject%20Object%5D&amp;originHeight=260&amp;originWidth=735&amp;status=done&amp;style=none&amp;width=735" alt="" /></p><p>2，这个时候就会自动新建一个名为 hexo 的文件夹，并且 hexo 已经在该文件夹内完成初始化</p><p><img src="https://i.loli.net/2021/02/21/JYKaM9gy4wODjTS.png#align=left&amp;display=inline&amp;height=324&amp;margin=%5Bobject%20Object%5D&amp;originHeight=324&amp;originWidth=793&amp;status=done&amp;style=none&amp;width=793" alt="" /></p><p>3，hexo 的基本命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ hexo clean <span class="comment">#清除生成的博客静态文件</span></span><br><span class="line">$ hexo g <span class="comment">#生成博客静态文件=命令hexo generate</span></span><br><span class="line">$ hexo s <span class="comment">#启动博客的本地预览=命令hexo server</span></span><br><span class="line">$ hexo d <span class="comment">#推送博客到远程仓库=命令hexo deploy</span></span><br><span class="line">$ hexo new page xxx <span class="comment">#新建名为&quot;xxx&quot;的页面</span></span><br><span class="line">$ hexo new xxx <span class="comment">#新建名为&quot;xxx&quot;的文章</span></span><br><span class="line">$ hexo d -g <span class="comment">#生成静态博客并推送到Git远程仓库</span></span><br><span class="line">$ hexo s -g <span class="comment">#生成静态博客并启动本地预览</span></span><br></pre></td></tr></table></figure><p>4，直接输入（注意：不能在原来那个窗口输入，需要进入到 hexo 文件夹中打开 git bash 窗口输入），在执行 hexo g 的时候就会给我们生成一个 public 文件夹，这个文件夹就是我们之后部署到 Github 或者 Coding 等平台的文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ npm install <span class="comment">##需要安装 hexo 必须的依赖</span></span><br><span class="line">$ hexo clean &amp;&amp;  hexo g &amp;&amp; hexo s</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/02/21/DSlLXpujeHa9MEV.png#align=left&amp;display=inline&amp;height=314&amp;margin=%5Bobject%20Object%5D&amp;originHeight=314&amp;originWidth=786&amp;status=done&amp;style=none&amp;width=786" alt="" /></p><p>5，执行完之后，在浏览器输入：localhost:4000，就会显示如下：</p><p><img src="https://i.loli.net/2021/02/21/juAIM6tQTU9a2SL.png#align=left&amp;display=inline&amp;height=882&amp;margin=%5Bobject%20Object%5D&amp;originHeight=882&amp;originWidth=1554&amp;status=done&amp;style=none&amp;width=1554" alt="" /></p><p>6，下载主题，随便选一种下载：</p><p>Next 主题：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> -b dev https://github.com/iissnan/hexo-theme-next themes/next</span><br><span class="line">$ npm i hexo-renderer-swig <span class="comment">##hexo在5.0之后把swig给删除了需要自己手动安装</span></span><br></pre></td></tr></table></figure><p>或者 butterfly 主题：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> -b dev https://github.com/jerryc127/hexo-theme-butterfly.git themes/butterfly</span><br><span class="line">$ npm install hexo-renderer-pug hexo-renderer-stylus --save <span class="comment">##需要下载pug 以及 stylus 的渲染器</span></span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/02/21/ZpGN637oiCzHlvR.png#align=left&amp;display=inline&amp;height=180&amp;margin=%5Bobject%20Object%5D&amp;originHeight=180&amp;originWidth=722&amp;status=done&amp;style=none&amp;width=722" alt="" /></p><p>7，然后打开 hexo 目录下的**_config.yml**配置文件，找到第 101 行将 theme 参数修改为你所应用的主题，我这里使用 next 主题，修改之后保存。</p><p><strong><em>在 YAML 语法中，冒号后面必须要有一个空格。</em></strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># Extensions</span><br><span class="line">## Plugins: https:&#x2F;&#x2F;hexo.io&#x2F;plugins&#x2F;</span><br><span class="line">## Themes: https:&#x2F;&#x2F;hexo.io&#x2F;themes&#x2F;</span><br><span class="line">theme: next ##landscape</span><br></pre></td></tr></table></figure><p>8，再次执行，并在浏览器输入：localhost:4000，就可以发现和之前不一样了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo clean &amp;&amp;  hexo g &amp;&amp; hexo s</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/02/21/ElBUkVRyxYwN7J6.png#align=left&amp;display=inline&amp;height=865&amp;margin=%5Bobject%20Object%5D&amp;originHeight=865&amp;originWidth=1263&amp;status=done&amp;style=none&amp;width=1263" alt="" /></p><p>9，接下来就可以写文章了，在命令行输入，就会在 hexo/source/_posts/目录下生成.md 文件，在打开该文件就可以进行编写文章了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new  &quot;文件名&quot; ##创建文章</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/02/21/pziwtD4hZcsYUxf.png#align=left&amp;display=inline&amp;height=78&amp;margin=%5Bobject%20Object%5D&amp;originHeight=78&amp;originWidth=489&amp;status=done&amp;style=none&amp;width=489" alt="" /></p><p>10，编写文章，然后重新执行启动命令，打开网页</p><p><img src="https://i.loli.net/2021/02/21/nmMwiXyGY6CoINH.png#align=left&amp;display=inline&amp;height=961&amp;margin=%5Bobject%20Object%5D&amp;originHeight=961&amp;originWidth=1422&amp;status=done&amp;style=none&amp;width=1422" alt="" /></p><h1 id="部署"><a class="markdownIt-Anchor" href="#部署"></a> 部署</h1><p>1，如果没有账号，请点此前往 <a href="https://github.com/">GitHub</a> 注册一个 GitHub 账号。</p><p>2，新建一个公开仓库，仓库名格式为 <code>your_username.github.io</code> 例如你的 GitHub 用户名是 <code>Jrebes</code> ，那么你的仓库地址名称就应该是 <code>Jrebes.github.io</code></p><p><img src="https://i.loli.net/2021/02/21/mJxv2QsUeXDbwio.png#align=left&amp;display=inline&amp;height=236&amp;margin=%5Bobject%20Object%5D&amp;originHeight=236&amp;originWidth=255&amp;status=done&amp;style=none&amp;width=255" alt="" /></p><p><img src="https://i.loli.net/2021/02/21/Xnc4jPg6JorlTFd.png#align=left&amp;display=inline&amp;height=801&amp;margin=%5Bobject%20Object%5D&amp;originHeight=801&amp;originWidth=934&amp;status=done&amp;style=none&amp;width=934" alt="" /></p><p>3，复制该仓库的 HTTPS/SSH 地址，一般格式为 <code>https://github.com/your_username/your_reponame.git</code> 在下一步会用到。</p><p><img src="https://i.loli.net/2021/02/21/JpcaWQmSHfyOB9h.png#align=left&amp;display=inline&amp;height=362&amp;margin=%5Bobject%20Object%5D&amp;originHeight=362&amp;originWidth=448&amp;status=done&amp;style=none&amp;width=448" alt="" /></p><p>4，分别输入如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git config --global user.name <span class="string">&quot;username&quot;</span> <span class="comment"># username是你的Github用户名，注意大小写保持一致</span></span><br><span class="line">$ git config --global user.email <span class="string">&quot;your email address&quot;</span> <span class="comment"># your email address填写你的Github注册用的邮箱</span></span><br><span class="line">$ ssh-keygen -t rsa -C <span class="string">&quot;your email address&quot;</span>  <span class="comment"># 生成SSH公钥，your email address同上填</span></span><br></pre></td></tr></table></figure><p>5，完成上面步骤后默认生成的密钥在 C:\Users\用户名.ssh\ 目录下，以文本编辑器打开 id_rsa.pub 文件，复制里面的所有内容，在 Github 中配置</p><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1614558159452-2b266872-7739-4eef-acf1-6f851d203149.png#align=left&amp;display=inline&amp;height=236&amp;margin=%5Bobject%20Object%5D&amp;originHeight=236&amp;originWidth=255&amp;size=0&amp;status=done&amp;style=none&amp;width=255" alt="" /></p><p><img src="https://i.loli.net/2021/02/21/5XEeQOB4HGkcLSV.png#align=left&amp;display=inline&amp;height=637&amp;margin=%5Bobject%20Object%5D&amp;originHeight=637&amp;originWidth=1365&amp;status=done&amp;style=none&amp;width=1365" alt="" /></p><p>6，打开 Git Bash，输入下面命令，检查是否可以与 github 连接</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh -T git@github.com</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/02/21/bmjsIAtlPYUOu3G.png#align=left&amp;display=inline&amp;height=132&amp;margin=%5Bobject%20Object%5D&amp;originHeight=132&amp;originWidth=897&amp;status=done&amp;style=none&amp;width=897" alt="" /></p><p>7，打开 hexo 目录下的_config.yml 文件，修改最后一行的 deploy 配置信息：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="attr">repo:</span></span><br><span class="line">    <span class="attr">github:</span> <span class="string">https://github.com/username/username.github.io.git</span> <span class="comment"># Git仓库地址，username是Github用户名</span></span><br><span class="line">  <span class="attr">branch:</span> <span class="string">master</span> <span class="comment"># 部署到仓库的master(主)分支</span></span><br></pre></td></tr></table></figure><p>8，执行命令，第一次 push 的时候需要你输入你的 Github 的用户名和密码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-deployer-git --save <span class="comment">##安装hexo-deployer-git插件</span></span><br><span class="line">$ hexo clean &amp;&amp;  hexo g &amp;&amp; hexo d</span><br></pre></td></tr></table></figure><p>9，本来是需要自动去开启 Github Page 服务的，若没有则去 GitHub 中设置（在 setting 的 Options 中找到 Github Pages），好像最近更新之后就默认自动开启，所以在浏览器输入你的博客网站就可以了，博客网站如下：</p><p><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1614558568509-4f6f25cf-3f9e-480e-931e-8b9af2c5dd2a.png#align=left&amp;display=inline&amp;height=237&amp;margin=%5Bobject%20Object%5D&amp;originHeight=396&amp;originWidth=1220&amp;size=0&amp;status=done&amp;style=none&amp;width=730" alt="" /><br /><img src="https://cdn.nlark.com/yuque/0/2021/png/12672933/1614558578959-a9df021c-aa90-41f3-8eda-0dd77fc29852.png#align=left&amp;display=inline&amp;height=258&amp;margin=%5Bobject%20Object%5D&amp;originHeight=258&amp;originWidth=731&amp;size=0&amp;status=done&amp;style=none&amp;width=731" alt="" /></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://username.github.io/  ##username是Github用户名</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/02/21/wHsC5rfSPzMLKov.png#align=left&amp;display=inline&amp;height=920&amp;margin=%5Bobject%20Object%5D&amp;originHeight=920&amp;originWidth=1678&amp;status=done&amp;style=none&amp;width=1678" alt="" /></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>集群搭建</title>
      <link href="posts/56e0c1a5/"/>
      <url>posts/56e0c1a5/</url>
      
        <content type="html"><![CDATA[<h1 id="集群搭建"><a class="markdownIt-Anchor" href="#集群搭建"></a> 集群搭建</h1><h1 id="集群搭建-2"><a class="markdownIt-Anchor" href="#集群搭建-2"></a> <strong>集群搭建：</strong></h1><h3 id="1创建-atguigu-用户"><a class="markdownIt-Anchor" href="#1创建-atguigu-用户"></a> <strong>1，创建 atguigu 用户</strong></h3><p><code>useradd atguigu</code></p><p><code>passwd atguigu</code></p><h3 id="2设置-hostname"><a class="markdownIt-Anchor" href="#2设置-hostname"></a> <strong>2，设置 hostname</strong></h3><p><code>hostnamectl --static set-hostname hadoop102</code></p><h3 id="3关闭防火墙"><a class="markdownIt-Anchor" href="#3关闭防火墙"></a> <strong>3，关闭防火墙</strong></h3><p><code>systemctl stop firewalld</code></p><p><code>systemctl disable firewalld</code></p><h3 id="4设置-ip"><a class="markdownIt-Anchor" href="#4设置-ip"></a> <strong>4，设置 ip</strong></h3><p><code>vim /etc/sysconfig/network-scripts/ifcfg-ens33</code></p><p>修改：</p><p><code>BOOTPROTO=static</code></p><p><code>ONBOOT=yes</code></p><p>添加</p><p><code>IPADDR=192.168.1.102</code></p><p><code>GATEWAY=192.168.1.2</code></p><p><code>DNS1=192.168.1.2</code></p><p><code>DNS2=114.114.114.114</code></p><h3 id="5设置映射"><a class="markdownIt-Anchor" href="#5设置映射"></a> <strong>5，设置映射</strong></h3><p><code>vim /etc/hosts</code></p><p><code>192.168.1.102 hadoop102</code></p><p><code>192.168.1.103 hadoop103</code></p><p><code>192.168.1.104 hadoop104</code></p><h3 id="6创建文件夹并修改权限"><a class="markdownIt-Anchor" href="#6创建文件夹并修改权限"></a> <strong>6，创建文件夹，并修改权限</strong></h3><p><code>mkdir /opt/software</code></p><p><code>mkdir /opt/module</code></p><p><code>chown atguigu:atguigu /opt/software</code></p><p><code>chown atguigu:atguigu /opt/module</code></p><h3 id="7设置-sudo-权限为免密"><a class="markdownIt-Anchor" href="#7设置-sudo-权限为免密"></a> <strong>7，设置 sudo 权限为免密：</strong></h3><p><code>atguigu ALL=(ALL) NOPASSWD: ALL</code></p><h3 id="-复制虚拟机-"><a class="markdownIt-Anchor" href="#-复制虚拟机-"></a> -------复制虚拟机-------</h3><h3 id="8修改-ip-以及-hostname"><a class="markdownIt-Anchor" href="#8修改-ip-以及-hostname"></a> <strong>8，修改 ip 以及 hostname</strong></h3><p>修改 ip：</p><p><code>vim /etc/sysconfig/network-scripts/ifcfg-ens33</code></p><p><code>IPADDR=192.168.1.103</code></p><p><code>IPADDR=192.168.1.104</code></p><p>修改 hostname：</p><p><code>hostnamectl --static set-hostname hadoop103</code></p><p><code>hostnamectl --static set-hostname hadoop104</code></p><h3 id="9所有-root-以及-atguigu-都要设置-ssh-免密"><a class="markdownIt-Anchor" href="#9所有-root-以及-atguigu-都要设置-ssh-免密"></a> 9，<strong>所有 root 以及 atguigu 都要设置 ssh 免密</strong></h3><p><code>ssh-keygen</code></p><p>root 用户:</p><p><code>ssh-copy-id root@hadoop102</code></p><p>atguigu 用户：</p><p><code>ssh-copy-id atguigu@hadoop102</code></p><h3 id="10安装-vim"><a class="markdownIt-Anchor" href="#10安装-vim"></a> <strong>10，安装 vim</strong></h3><h3 id="11设置命令行界面设置为-3"><a class="markdownIt-Anchor" href="#11设置命令行界面设置为-3"></a> <strong>11，设置命令行界面（设置为 3）</strong></h3><p><code>systemctl get-default //获取当前的启动项</code></p><p><code>systemctl set-default multi-user.target</code></p><h3 id="12设置分发脚本"><a class="markdownIt-Anchor" href="#12设置分发脚本"></a> <strong>12，设置分发脚本</strong></h3><p>12.1，安装 rsync</p><p><code>sudo yum install -y rsync</code><br />12.2，xsync 脚本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">#! &#x2F;bin&#x2F;bash</span><br><span class="line">#1、判断是否传入待同步的文件&#x2F;目录</span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">echo &quot;必须传入一个文件&#x2F;目录&quot;</span><br><span class="line">exit</span><br><span class="line">fi</span><br><span class="line">#2、遍历待同步的多个文件&#x2F;目录</span><br><span class="line">for fileOrDir in $@</span><br><span class="line">do</span><br><span class="line">#3、判断当前待同步的文件&#x2F;目录是否存在</span><br><span class="line">#-d: 判断是否为目录</span><br><span class="line">#-f: 判断是否为文件</span><br><span class="line">#-e: 判断是否存在</span><br><span class="line">if [ -e $fileOrDir ]</span><br><span class="line">then</span><br><span class="line">#4、获取待同步的文件&#x2F;目录的父目录、文件名&#x2F;最后一个目录名</span><br><span class="line">pdir&#x3D;$(cd $(dirname $fileOrDir);pwd)</span><br><span class="line">fname&#x3D;$(basename $fileOrDir)</span><br><span class="line">for host in hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">echo &quot;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;$host&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&quot;</span><br><span class="line">#5、在其他机器上创建父目录</span><br><span class="line">ssh $host &quot;mkdir -p $pdir&quot;</span><br><span class="line">#6、同步文件&#x2F;目录</span><br><span class="line">rsync -av $pdir&#x2F;$fname $host:$pdir</span><br><span class="line">done</span><br><span class="line">fi</span><br><span class="line">done</span><br><span class="line"># cd &#x2F;opt&#x2F;module&#x2F;xx</span><br><span class="line">#xsync a.txt &#x2F;opt&#x2F;module&#x2F;xx&#x2F;b.txt</span><br></pre></td></tr></table></figure><p>-av 打印进度</p><p><strong>12.3，在 home 家目录下创建一个 bin 目录并编写脚本（平时用于分发 atguigu 账号下的文件，如：module 或者 software 文件夹下的）</strong></p><p><code>cd ~</code></p><p><code>mkdir bin</code></p><p><code>vim xsync</code></p><p><code>chmod +x xsync</code><br /><strong>12.4，将文件复制一份到 root 用户的/bin 目录下（平时用于分发 root 账号下的文件，如：/etc/profile.d/my_env.sh）</strong></p><p><code>sudo cp /home/atguigu/bin/xsync /bin/</code></p><h3 id="13安装-lrzsz用于传输文件"><a class="markdownIt-Anchor" href="#13安装-lrzsz用于传输文件"></a> <strong>13，安装 lrzsz，用于传输文件</strong></h3><p><code>sudo yum install lrzsz</code><br /><strong>使用 rz 命令上传文件，sz 文件名 命令下载文件</strong></p><h1 id="安装-jdk"><a class="markdownIt-Anchor" href="#安装-jdk"></a> <strong>安装 JDK</strong></h1><h3 id="1检查-linux-中是否存在-jdk如果存在就将其移除"><a class="markdownIt-Anchor" href="#1检查-linux-中是否存在-jdk如果存在就将其移除"></a> <strong>1，检查 Linux 中是否存在 jdk，如果存在就将其移除</strong></h3><p><code>sudo rpm -qa | grep -i java | xargs -n1sudo rpm -e --nodeps</code></p><h3 id="2将文件上传至-software-文件夹并解压至-module-文件夹并改名为-java"><a class="markdownIt-Anchor" href="#2将文件上传至-software-文件夹并解压至-module-文件夹并改名为-java"></a> <strong>2，将文件上传至 software 文件夹，并解压至 module 文件夹，并改名为 java</strong></h3><p><code>tar -zxvf /opt/software/jdk-8u212-linux-x64.tar.gz /opt/module/</code></p><p><code>mv /opt/module/jdk1.8.0_212 java</code></p><h3 id="3设置环境变量"><a class="markdownIt-Anchor" href="#3设置环境变量"></a> <strong>3，设置环境变量</strong></h3><p><code>export JAVA_HOME=/opt/module/java</code></p><p><code>export PATH=$PATH:$JAVA_HOME/bin</code></p><h3 id="4分发-java-文件"><a class="markdownIt-Anchor" href="#4分发-java-文件"></a> <strong>4，分发 java 文件</strong></h3><p><code>xsync /opt/module/java/</code></p><h3 id="5分发全局配置文件"><a class="markdownIt-Anchor" href="#5分发全局配置文件"></a> <strong>5，分发全局配置文件</strong></h3><p><code>sudo xsync /etc/profile.d/my_env.sh</code></p><p>注意：source 一下</p><p><code>source /etc/profile.d/my_env.sh</code></p><p>注意：不要在 root 用户连接的情况下，使用 sudo 去连接 atguigu 用户然后再进行操作 atguigu 这个用户</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
